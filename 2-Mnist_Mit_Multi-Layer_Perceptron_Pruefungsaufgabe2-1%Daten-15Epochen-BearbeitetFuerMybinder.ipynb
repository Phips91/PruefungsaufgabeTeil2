{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://akademie.datamics.com/kursliste/\">![title](bg_datamics_top.png)</a>\n\n<center><em>© Datamics</em></center><br><center><em>Besuche uns für mehr Informationen auf <a href='https://akademie.datamics.com/kursliste/'>www.akademie.datamics.com</a></em></center>\n\n# MNIST mit Multi-Layer Perceptron\n\nIn dieser Lektion werden wir ein Multi Layer Perceptron Modell erstellen und versuchen damit handgeschriebenen Zahlen zu klassifizieren. Das ist ein sehr verbreitetes Einsteigerproblem für Tensorflow.\n\nDenkt daran, dass eine einzige Lektion niemals ausreichen wird, um Deep Learning und/oder Tensorflow in seiner Komlexität abzudecken!\n\n## Die Daten laden\n\nWir werden die berühmten MNIST Daten über [handgeschriebenen Zahlen](http://yann.lecun.com/exdb/mnist/) verwenden.\n\nDie Bilder die wir verwenden werden sind schwarz-weiß Bilder der größe 28 x 28, d.h. 784 Pixel insgesamt. Unsere Features werden die Pixelwerte für jeden Pixel sein. Entweder ist der Pixel \"weiß\" (also eine 0 in den Daten) oder er hat einen Pixelwert.\n\nWir werden versuchen korrekt vorherzusagen, welche Nummer geschrieben steht. Dazu verwenden wir lediglich die Bilddaten in Form unseres Arrays. Diese Art von Problem (Image Recognition oder auf Deutsch: Bilderkennung) ist ein tolle Use Case für Deep Learning Methoden!\n\nDie Daten sind für Deep Learning das, was der Iris Datensatz für typische Machine Learning Algorithmen ist.","metadata":{}},{"cell_type":"markdown","source":"# Import von Bibliotheken in Python.\n\nIn diesem Code werden die Python-Bibliotheken TensorFlow, NumPy, Logging, Time, Matplotlib und Unittest importiert.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dieser Code importiert verschiedene Python-Bibliotheken, lädt den MNIST-Datensatz, normalisiert die Daten, teilt sie in Trainings- und Testdaten auf und konvertiert die Trainingsdaten in das gewünschte Datenformat für spätere Verwendung.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport logging\nimport time\nimport matplotlib.pyplot as plt\nimport struct\nimport unittest\nfrom functools import wraps\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom tensorflow.keras.datasets import mnist\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Pfad zu den heruntergeladenen MNIST-Daten\ntrain_data_path = 'train-images.idx3-ubyte'\ntest_data_path = 't10k-images.idx3-ubyte'\ntrain_labels_path = 'train-labels.idx1-ubyte'  # Hinzugefügt\ntest_labels_path = 't10k-labels.idx1-ubyte'  # Hinzugefügt\n\n# Laden der MNIST-Daten aus den lokal gespeicherten Dateien\ndef load_mnist_data(data_path):\n    with open(data_path, 'rb') as f:\n        magic, num_images, num_rows, num_cols = struct.unpack('>IIII', f.read(16))\n        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, num_rows, num_cols)\n    return images\n\n# Laden der MNIST-Label aus den lokal gespeicherten Dateien (Hinzugefügt)\ndef load_mnist_labels(labels_path):\n    with open(labels_path, 'rb') as f:\n        magic, num_labels = struct.unpack('>II', f.read(8))\n        labels = np.fromfile(f, dtype=np.uint8)\n    return labels\n\n# Laden Sie die MNIST-Trainings- und Testdaten\nx_train_mnist = load_mnist_data(train_data_path)\nx_test_mnist = load_mnist_data(test_data_path)\n# Laden Sie die MNIST-Label\ntrain_labels = load_mnist_labels(train_labels_path)\ntest_labels = load_mnist_labels(test_labels_path)\n\n# Normalisieren der Daten\nclass Normalize(object):\n    def normalize(self, X_train, X_test):\n        self.scaler = MinMaxScaler()\n        # Umformen in 2D-Arrays (Flatten)\n        X_train = X_train.reshape(X_train.shape[0], -1)\n        X_test = X_test.reshape(X_test.shape[0], -1)\n        X_train = self.scaler.fit_transform(X_train)\n        X_test = self.scaler.transform(X_test)\n        return (X_train, X_test)\n\n    def inverse(self, X_train, X_test):\n        X_train = self.scaler.inverse_transform(X_train)\n        X_test = self.scaler.inverse_transform(X_test)\n        return (X_train, X_test)\n\n# Aufteilen der Daten\ndef split(X, y, splitRatio):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n    return (X_train, y_train, X_test, y_test)\n\n# Annahme: Sie haben bereits die MNIST-Trainings- und Testdaten geladen und in x_train_mnist und x_test_mnist gespeichert.\n\n# Normalisieren der Daten\nnormalizer = Normalize()\nx_train, x_test = normalizer.normalize(x_train_mnist, x_test_mnist)\n\n# Aufteilen der Daten\nsplitRatio = 0.8  # Ändern Sie den Split-Verhältnis nach Bedarf\nx_train, y_train, x_test, y_test = split(x_train, train_labels, splitRatio)\n\n# Stellen Sie sicher, dass die Daten korrekt geladen wurden (ersetzen Sie y durch die entsprechenden Label-Daten)\nassert x_train.shape == (int(0.8 * len(x_train_mnist)), x_train_mnist.shape[1] * x_train_mnist.shape[2])\nassert x_test.shape == (int(0.2 * len(x_train_mnist)), x_train_mnist.shape[1] * x_train_mnist.shape[2])\nassert y_train.shape == (int(0.8 * len(x_train_mnist)),)\nassert y_test.shape == (int(0.2 * len(x_train_mnist)),)","metadata":{"trusted":true,"tags":[]},"execution_count":1,"outputs":[{"name":"stderr","text":"2023-10-29 23:29:06.273711: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-10-29 23:29:06.656951: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-10-29 23:29:06.661229: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-29 23:29:08.899343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definieren der my_logger und my_timer Funktion","metadata":{}},{"cell_type":"markdown","source":"def my_logger(orig_func):\n    logging.basicConfig(filename='{}.log'.format(orig_func.__name__), level=logging.INFO)\n\n    @wraps(orig_func)\n    def wrapper(*args, **kwargs):\n        logging.info(\n            'Ran with args: {}, and kwargs: {}'.format(args, kwargs))\n        return orig_func(*args, **kwargs)\n\n    return wrapper","metadata":{}},{"cell_type":"code","source":"def my_logger(orig_func):\n    logging.basicConfig(filename='{}.log'.format(orig_func.__name__), level=logging.INFO)\n    \n    @wraps(orig_func)\n    def wrapper(*args, **kwargs):\n        logging.info(\n            'Ran with args: {}, and kwargs: {}'.format(args, kwargs))\n        return orig_func(*args, **kwargs)\n\n    return wrapper","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"def my_timer(orig_func):\n    import time\n\n    @wraps(orig_func)\n    def wrapper(*args, **kwargs):\n        t1 = time.time()\n        result = orig_func(*args, **kwargs)\n        t2 = time.time() - t1\n        print('{} ran in: {} sec'.format(orig_func.__name__, t2))\n        return result\n\n    return wrapper","metadata":{}},{"cell_type":"code","source":"def my_timer(orig_func):\n    @wraps(orig_func)\n    def wrapper(*args, **kwargs):\n        t1 = time.time()\n        result = orig_func(*args, **kwargs)\n        t2 = time.time() - t1\n        print('{} ran in: {} sec'.format(orig_func.__name__, t2))\n        return result\n\n    return wrapper","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"In diesem Code wird eine Python-Klasse mit dem Namen \"TheAlgorithm\" definiert, die verschiedene Methoden wie den Konstruktor __init__, die Methode fit und die Methode predict enthält, wobei die Dekoratoren @my_logger und @my_timer verwendet werden, um die Ausführung dieser Methoden zu protokollieren und die Zeitmessung durchzuführen.","metadata":{}},{"cell_type":"code","source":"class TheAlgorithm(object):\n    @my_logger\n    @my_timer\n    def __init__(self, X_train, y_train, X_test, y_test):\n        self.X_train, self.y_train, self.X_test, self.y_test = X_train, y_train, X_test, y_test\n\n    @my_logger\n    @my_timer\n    def fit(self):\n        x_train, y_train, x_test, y_test = self.X_train, self.y_train, self.X_test, self.y_test\n\n        normalizer = Normalize()  # Use the correct class name here\n        x_train, x_test = normalizer.normalize(x_train, x_test)\n\n        train_samples = x_train.shape[0]\n\n        self.classifier = LogisticRegression(\n            C=50. / train_samples,\n            multi_class='multinomial',\n            penalty='l1',\n            solver='saga',\n            tol=0.1,\n            class_weight='balanced',\n        )\n\n        self.classifier.fit(x_train, y_train)\n        self.train_predictions = self.classifier.predict(x_train)\n        self.train_accuracy = np.mean(self.train_predictions.ravel() == y_train.ravel()) * 100\n        self.train_confusion_matrix = confusion_matrix(y_train, self.train_predictions)\n        return self.train_accuracy\n\n    def evaluate(self, X, y):\n        \"\"\"Evaluates the model on the given dataset.\n\n        Args:\n            X: The data to evaluate the model on.\n            y: The labels for the data.\n\n        Returns:\n            The accuracy of the model on the given dataset.\n        \"\"\"\n\n        # Make predictions on the data\n        predictions = self.predict(X)\n\n        # Calculate the accuracy\n        accuracy = np.mean(predictions == y)\n\n        return accuracy\n    \n    @my_logger\n    @my_timer\n    def predict(self):\n        x_test = self.X_test  # Test data doesn't need to be normalized again\n\n        self.test_predictions = self.classifier.predict(x_test)\n        self.test_accuracy = np.mean(self.test_predictions.ravel() == self.y_test.ravel()) * 100\n        self.test_confusion_matrix = confusion_matrix(self.y_test, self.test_predictions)\n        self.report = classification_report(self.y_test, self.test_predictions)\n        print(\"Classification Report for the classifier:\\n%s\\n\" % (self.report))\n\n        return self.test_accuracy\n","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Daten Format\n\nDie Daten sind im Vektor Format gespeichert, obwohl die Originaldaten eine 2-dimensionale Matrix waren, die angab, wie viele Pigmente sich an welcher Position befinden. Untersuchen wir das genauer:","metadata":{}},{"cell_type":"code","source":"type(mnist)","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"module"},"metadata":{}}]},{"cell_type":"code","source":"type(x_train)\ntype(y_train)","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"numpy.ndarray"},"metadata":{}}]},{"cell_type":"code","source":"x_train[2].shape","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(784,)"},"metadata":{}}]},{"cell_type":"code","source":"sample = x_train[2].reshape(28, 28)","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"plt.imshow(sample)","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f12c471cbe0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZ+klEQVR4nO3df0yV9/338dcR4fijcBgiHKjo0La6VWWZU0psnZ1EYLn9+iuLtl2iTW+NDpsp69qwtLVuS+hs0vXbhuk/naxJ1dbvXTU1nYvFgnEFF6nG+O1GhJtVjICr+cJBVET53H9496xHQXvwHN8cfD6SK5Fzrg/n3atX+uzFOV54nHNOAADcZcOsBwAA3JsIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMDHceoAb9fb26uzZs0pMTJTH47EeBwAQJuecOjs7lZmZqWHD+r/OGXQBOnv2rLKysqzHAADcoebmZo0bN67f5wddgBITEyVJj+rHGq5442kAAOG6qh4d1kfB/573J2oBKi8v12uvvabW1lbl5OTorbfe0qxZs2677qsfuw1XvIZ7CBAAxJz/f4fR272NEpUPIbz33nsqKSnRxo0b9dlnnyknJ0cFBQU6d+5cNF4OABCDohKg119/XatWrdLTTz+t7373u9q6datGjRqlP/7xj9F4OQBADIp4gK5cuaK6ujrl5+f/+0WGDVN+fr5qampu2r+7u1uBQCBkAwAMfREP0Jdffqlr164pPT095PH09HS1trbetH9ZWZl8Pl9w4xNwAHBvMP+LqKWlpero6Ahuzc3N1iMBAO6CiH8KLjU1VXFxcWprawt5vK2tTX6//6b9vV6vvF5vpMcAAAxyEb8CSkhI0IwZM1RZWRl8rLe3V5WVlcrLy4v0ywEAYlRU/h5QSUmJVqxYoR/84AeaNWuW3njjDXV1denpp5+OxssBAGJQVAK0bNky/etf/9LLL7+s1tZWfe9739P+/ftv+mACAODe5XHOOeshvi4QCMjn82muFnInBACIQVddj6q0Vx0dHUpKSup3P/NPwQEA7k0ECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAxHDrAQB8M4EnHwl7zaHXyqMwSeTMea447DVJO2qjMAkscAUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgZqTAENbjrlmPcEseZz0BLHEFBAAwQYAAACYiHqBXXnlFHo8nZJsyZUqkXwYAEOOi8h7Qww8/rI8//vjfLzKct5oAAKGiUobhw4fL7/dH41sDAIaIqLwHdOrUKWVmZmrixIl66qmndPr06X737e7uViAQCNkAAENfxAOUm5uriooK7d+/X1u2bFFTU5Mee+wxdXZ29rl/WVmZfD5fcMvKyor0SACAQSjiASoqKtJPfvITTZ8+XQUFBfroo4/U3t6u999/v8/9S0tL1dHREdyam5sjPRIAYBCK+qcDkpOT9dBDD6mhoaHP571er7xeb7THAAAMMlH/e0AXLlxQY2OjMjIyov1SAIAYEvEAPffcc6qurtY///lPffrpp1q8eLHi4uL0xBNPRPqlAAAxLOI/gjtz5oyeeOIJnT9/XmPHjtWjjz6q2tpajR07NtIvBQCIYREP0M6dOyP9LQFI2vTrt61HuKXvf/pM2GsmfvR52GsG9+1VEQ7uBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmIj6L6QDEBmPj7wc9poeF4VB+tEdCP8XS14LBKIwCWIFV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwd2wAQOn/vORAayqi/gcgCWugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFDDw3Zwvwl4T74mLwiR9O3P1UthrRjckRGESDGVcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKWDgam/4/+/X467dlTWS9JeuyWGvuf/VTwf0Wrh3cQUEADBBgAAAJsIO0KFDh7RgwQJlZmbK4/Foz549Ic875/Tyyy8rIyNDI0eOVH5+vk6dOhWpeQEAQ0TYAerq6lJOTo7Ky8v7fH7z5s168803tXXrVh05ckSjR49WQUGBLl++fMfDAgCGjrA/hFBUVKSioqI+n3PO6Y033tCLL76ohQsXSpLeeecdpaena8+ePVq+fPmdTQsAGDIi+h5QU1OTWltblZ+fH3zM5/MpNzdXNTU1fa7p7u5WIBAI2QAAQ19EA9Ta2ipJSk9PD3k8PT09+NyNysrK5PP5gltWVlYkRwIADFLmn4IrLS1VR0dHcGtubrYeCQBwF0Q0QH6/X5LU1tYW8nhbW1vwuRt5vV4lJSWFbACAoS+iAcrOzpbf71dlZWXwsUAgoCNHjigvLy+SLwUAiHFhfwruwoULamhoCH7d1NSk48ePKyUlRePHj9f69ev129/+Vg8++KCys7P10ksvKTMzU4sWLYrk3ACAGBd2gI4eParHH388+HVJSYkkacWKFaqoqNDzzz+vrq4urV69Wu3t7Xr00Ue1f/9+jRgxInJTAwBinsc556yH+LpAICCfz6e5WqjhnnjrcYDbiktPC3tNyu4rYa/ZNqHy9jvdYKA3I/3fXxSEveb87P8Z0Gth6LnqelSlvero6Ljl+/rmn4IDANybCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCLsX8cAINQ/Vz8Q9pr/Gv/GAF4pbgBrBuZ/ftb3bzC+zaqIz4GhjSsgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyMFhrCplWsGtG5KU0OEJwFuxhUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5ECXxOX7At7TffkS2GviffE3ZU1I06NCHuNJF0LBAa0DggHV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluRgp8zd83PxT2mv+eWx72mh4X9pKBuVuvAwwAV0AAABMECABgIuwAHTp0SAsWLFBmZqY8Ho/27NkT8vzKlSvl8XhCtsLCwkjNCwAYIsIOUFdXl3JyclRe3v/PvQsLC9XS0hLcduzYcUdDAgCGnrA/hFBUVKSioqJb7uP1euX3+wc8FABg6IvKe0BVVVVKS0vT5MmTtXbtWp0/f77ffbu7uxUIBEI2AMDQF/EAFRYW6p133lFlZaV+97vfqbq6WkVFRbp27Vqf+5eVlcnn8wW3rKysSI8EABiEIv73gJYvXx7887Rp0zR9+nRNmjRJVVVVmjdv3k37l5aWqqSkJPh1IBAgQgBwD4j6x7AnTpyo1NRUNTQ09Pm81+tVUlJSyAYAGPqiHqAzZ87o/PnzysjIiPZLAQBiSNg/grtw4ULI1UxTU5OOHz+ulJQUpaSkaNOmTVq6dKn8fr8aGxv1/PPP64EHHlBBQUFEBwcAxLawA3T06FE9/vjjwa+/ev9mxYoV2rJli06cOKE//elPam9vV2ZmpubPn6/f/OY38nq9kZsaABDzwg7Q3Llz5Vz/dzj8y1/+ckcDAZEwPGvcgNaV/fC/IjxJ5Jy5einsNQn8rQYMYtwLDgBgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYi/iu5gcHg803+Aa37j9FtEZ4kchZseT7sNfe/9WkUJgEigysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyPFoNeTPyPsNWWz/08UJrF1/6vcWBRDC1dAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJbkaKQe/gO2+HvabHXRvgq8UNcB2AcHEFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GakGPQGcmPRgd+M9O6YWrkm7DUP6rMoTALY4QoIAGCCAAEATIQVoLKyMs2cOVOJiYlKS0vTokWLVF9fH7LP5cuXVVxcrDFjxui+++7T0qVL1dbWFtGhAQCxL6wAVVdXq7i4WLW1tTpw4IB6eno0f/58dXV1BffZsGGDPvzwQ+3atUvV1dU6e/aslixZEvHBAQCxLawPIezfvz/k64qKCqWlpamurk5z5sxRR0eH3n77bW3fvl0/+tGPJEnbtm3Td77zHdXW1uqRRx6J3OQAgJh2R+8BdXR0SJJSUlIkSXV1derp6VF+fn5wnylTpmj8+PGqqanp83t0d3crEAiEbACAoW/AAert7dX69es1e/ZsTZ06VZLU2tqqhIQEJScnh+ybnp6u1tbWPr9PWVmZfD5fcMvKyhroSACAGDLgABUXF+vkyZPauXPnHQ1QWlqqjo6O4Nbc3HxH3w8AEBsG9BdR161bp3379unQoUMaN25c8HG/368rV66ovb095Cqora1Nfr+/z+/l9Xrl9XoHMgYAIIaFdQXknNO6deu0e/duHTx4UNnZ2SHPz5gxQ/Hx8aqsrAw+Vl9fr9OnTysvLy8yEwMAhoSwroCKi4u1fft27d27V4mJicH3dXw+n0aOHCmfz6dnnnlGJSUlSklJUVJSkp599lnl5eXxCTgAQIiwArRlyxZJ0ty5c0Me37Ztm1auXClJ+v3vf69hw4Zp6dKl6u7uVkFBgf7whz9EZFgAwNARVoCcc7fdZ8SIESovL1d5efmAhwKGOv+fE6xHAMxxLzgAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwMdx6ACDWzT+5POw13zrcHPaaq2GvAAY3roAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBSD3v+6f4b1CLc0Wv837DXcWBTgCggAYIQAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYCCtAZWVlmjlzphITE5WWlqZFixapvr4+ZJ+5c+fK4/GEbGvWrIno0ACA2BdWgKqrq1VcXKza2lodOHBAPT09mj9/vrq6ukL2W7VqlVpaWoLb5s2bIzo0ACD2hfUbUffv3x/ydUVFhdLS0lRXV6c5c+YEHx81apT8fn9kJgQADEl39B5QR0eHJCklJSXk8XfffVepqamaOnWqSktLdfHixX6/R3d3twKBQMgGABj6wroC+rre3l6tX79es2fP1tSpU4OPP/nkk5owYYIyMzN14sQJvfDCC6qvr9cHH3zQ5/cpKyvTpk2bBjoGACBGeZxzbiAL165dqz//+c86fPiwxo0b1+9+Bw8e1Lx589TQ0KBJkybd9Hx3d7e6u7uDXwcCAWVlZWmuFmq4J34gowEADF11ParSXnV0dCgpKanf/QZ0BbRu3Trt27dPhw4dumV8JCk3N1eS+g2Q1+uV1+sdyBgAgBgWVoCcc3r22We1e/duVVVVKTs7+7Zrjh8/LknKyMgY0IAAgKEprAAVFxdr+/bt2rt3rxITE9Xa2ipJ8vl8GjlypBobG7V9+3b9+Mc/1pgxY3TixAlt2LBBc+bM0fTp06PyDwAAiE1hvQfk8Xj6fHzbtm1auXKlmpub9dOf/lQnT55UV1eXsrKytHjxYr344ou3/Dng1wUCAfl8Pt4DAoAYFZX3gG7XqqysLFVXV4fzLQEA9yjuBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMDHceoAbOeckSVfVIznjYQAAYbuqHkn//u95fwZdgDo7OyVJh/WR8SQAgDvR2dkpn8/X7/Med7tE3WW9vb06e/asEhMT5fF4Qp4LBALKyspSc3OzkpKSjCa0x3G4juNwHcfhOo7DdYPhODjn1NnZqczMTA0b1v87PYPuCmjYsGEaN27cLfdJSkq6p0+wr3AcruM4XMdxuI7jcJ31cbjVlc9X+BACAMAEAQIAmIipAHm9Xm3cuFFer9d6FFMch+s4DtdxHK7jOFwXS8dh0H0IAQBwb4ipKyAAwNBBgAAAJggQAMAEAQIAmIiZAJWXl+vb3/62RowYodzcXP3tb3+zHumue+WVV+TxeEK2KVOmWI8VdYcOHdKCBQuUmZkpj8ejPXv2hDzvnNPLL7+sjIwMjRw5Uvn5+Tp16pTNsFF0u+OwcuXKm86PwsJCm2GjpKysTDNnzlRiYqLS0tK0aNEi1dfXh+xz+fJlFRcXa8yYMbrvvvu0dOlStbW1GU0cHd/kOMydO/em82HNmjVGE/ctJgL03nvvqaSkRBs3btRnn32mnJwcFRQU6Ny5c9aj3XUPP/ywWlpagtvhw4etR4q6rq4u5eTkqLy8vM/nN2/erDfffFNbt27VkSNHNHr0aBUUFOjy5ct3edLout1xkKTCwsKQ82PHjh13ccLoq66uVnFxsWpra3XgwAH19PRo/vz56urqCu6zYcMGffjhh9q1a5eqq6t19uxZLVmyxHDqyPsmx0GSVq1aFXI+bN682WjifrgYMGvWLFdcXBz8+tq1ay4zM9OVlZUZTnX3bdy40eXk5FiPYUqS2717d/Dr3t5e5/f73WuvvRZ8rL293Xm9Xrdjxw6DCe+OG4+Dc86tWLHCLVy40GQeK+fOnXOSXHV1tXPu+r/7+Ph4t2vXruA+f//7350kV1NTYzVm1N14HJxz7oc//KH7+c9/bjfUNzDor4CuXLmiuro65efnBx8bNmyY8vPzVVNTYziZjVOnTikzM1MTJ07UU089pdOnT1uPZKqpqUmtra0h54fP51Nubu49eX5UVVUpLS1NkydP1tq1a3X+/HnrkaKqo6NDkpSSkiJJqqurU09PT8j5MGXKFI0fP35Inw83HoevvPvuu0pNTdXUqVNVWlqqixcvWozXr0F3M9Ibffnll7p27ZrS09NDHk9PT9c//vEPo6ls5ObmqqKiQpMnT1ZLS4s2bdqkxx57TCdPnlRiYqL1eCZaW1slqc/z46vn7hWFhYVasmSJsrOz1djYqF/96lcqKipSTU2N4uLirMeLuN7eXq1fv16zZ8/W1KlTJV0/HxISEpScnByy71A+H/o6DpL05JNPasKECcrMzNSJEyf0wgsvqL6+Xh988IHhtKEGfYDwb0VFRcE/T58+Xbm5uZowYYLef/99PfPMM4aTYTBYvnx58M/Tpk3T9OnTNWnSJFVVVWnevHmGk0VHcXGxTp48eU+8D3or/R2H1atXB/88bdo0ZWRkaN68eWpsbNSkSZPu9ph9GvQ/gktNTVVcXNxNn2Jpa2uT3+83mmpwSE5O1kMPPaSGhgbrUcx8dQ5wftxs4sSJSk1NHZLnx7p167Rv3z598sknIb++xe/368qVK2pvbw/Zf6ieD/0dh77k5uZK0qA6HwZ9gBISEjRjxgxVVlYGH+vt7VVlZaXy8vIMJ7N34cIFNTY2KiMjw3oUM9nZ2fL7/SHnRyAQ0JEjR+758+PMmTM6f/78kDo/nHNat26ddu/erYMHDyo7Ozvk+RkzZig+Pj7kfKivr9fp06eH1Plwu+PQl+PHj0vS4DofrD8F8U3s3LnTeb1eV1FR4T7//HO3evVql5yc7FpbW61Hu6t+8YtfuKqqKtfU1OT++te/uvz8fJeamurOnTtnPVpUdXZ2umPHjrljx445Se711193x44dc1988YVzzrlXX33VJScnu71797oTJ064hQsXuuzsbHfp0iXjySPrVsehs7PTPffcc66mpsY1NTW5jz/+2H3/+993Dz74oLt8+bL16BGzdu1a5/P5XFVVlWtpaQluFy9eDO6zZs0aN378eHfw4EF39OhRl5eX5/Ly8gynjrzbHYeGhgb361//2h09etQ1NTW5vXv3uokTJ7o5c+YYTx4qJgLknHNvvfWWGz9+vEtISHCzZs1ytbW11iPddcuWLXMZGRkuISHB3X///W7ZsmWuoaHBeqyo++STT5ykm7YVK1Y4565/FPull15y6enpzuv1unnz5rn6+nrboaPgVsfh4sWLbv78+W7s2LEuPj7eTZgwwa1atWrI/U9aX//8kty2bduC+1y6dMn97Gc/c9/61rfcqFGj3OLFi11LS4vd0FFwu+Nw+vRpN2fOHJeSkuK8Xq974IEH3C9/+UvX0dFhO/gN+HUMAAATg/49IADA0ESAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmPh/49hITNVN2j8AAAAASUVORK5CYII="},"metadata":{}}]},{"cell_type":"markdown","source":"## Parameter\n\nWir werden 4 Parameter definieren müssen. Es ist wirklich (wirklich) schwer gute Parameterwerte für einen Datensatz zu bestimmen, mit dem man keine Erfahrung hat. Da dieser MNIST Datensatz allerdings so berühmt ist haben wir schon einige Ausgangswerte. Die Parameter sind:\n\n* Learning Rate - Wie schnell die Kostenfunktion angepasst wird\n* Traing Epochs - Wie viele Trainingszyklen durchlaufen werden sollen\n* Batch Size - Größe der \"Batches\" an Traingsdaten","metadata":{}},{"cell_type":"code","source":"# Parameter\nlearning_rate = 0.001\ntraining_epochs = 2\nbatch_size = 100","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Netzwerk Parameter\n\nHier haben wir Parameter welche unser Neuronales Netz direkt definieren. Diese werden entsprechend der betrachteten Daten angepasst und hängen auch davon ab, welche Art von Netz man nutzt. Es sind bis zu diesem Punkt erst einmal nur Zahlen, die wir später verwenden, um unser Netz zu definieren:","metadata":{}},{"cell_type":"code","source":"# Netzwerk Parameter\nn_hidden_1 = 256\nn_hidden_2 = 256\nn_input = x_train.shape[1]\nn_classes = 10\nn_samples = len(x_train)","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Tensorflow Graph Input","metadata":{}},{"cell_type":"code","source":"x = tf.keras.Input(shape=(n_input,), dtype=tf.float32)\ny = tf.keras.Input(shape=(n_classes,), dtype=tf.float32)","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## MultiLayer Modell\n\nEs ist Zeit unser Modell zu erstellen. Wiederholen wir deshalb kurz, was wir erstellen wollen:\n\nZuerst erhalten wir einen *Input* in Form eines Datenarrays und schicken diesen an die erste *Hidden Layer*. Dann wird den Daten ein  *Weight* zwischen den Schichten zugewiesen (welches zuerst ein zufälliger Wert ist). Anschließend wird es an einen *Node* geschicht und unterläuft eine *Activation Function* (zusammen mit einem Bias, wie in der Neural Network Lektion erwähnt). Dann geht es weiter zur nächsten *Layer* und immer so weiter, bis zur finalen *Output Layer*. In unserem Fall werden wir nur 2 *Hidden Layers* verwenden. Je mehr wir davon verwenden, desto länger braucht das Modell (aber er hat mehr Möglichkeiten um die Genauigkeit zu erhöhen).\n\nSobald die transformierte Daten die *Output Layer* erreicht haben müssen wir sie auswerten. Hier verwenden wir eine *Loss Function* (auch Cost Function genannt). Diese berechnet, wie sehr wir vom gewünschten Ergebnis entfernt sind. In diesem Fall: Wie viele der Klassen wir richtig zugeteilt haben.\n\nDann wenden wir eine Optimierungsfunktion an, um die *Costs* (bzw. den Error) zu minimieren. Dies geschiet durch die Anpassung der *Weights* entlang des Netzes. Wir verwenden in unserem Beispiel den [Adam Optimizer](https://arxiv.org/pdf/1412.6980v8.pdf), welcher eine (im Vergleich zu anderen) sehr neue Entwicklung ist.\n\nWir können anpassen, wie schnell diese Optimierung angewendet wird, indem wir unseren *Learning Rate* Parameter anpassen. Je geringer die Rate, desto höher die Möglichkeiten für Anpassungen. Dies erzeugt allerdings die Kosten einer erhöhten Wartezeit. Ab einem bestimmten Punkt lohnt es sich nicht mehr, die Learning Rate weiter zu senken.\n\nJetzt können wir unser Modell erstellen. Wir beginnen mit 2 Hidden Layers, welche die []() Activation Function verwenden. Dies ist eine einfache Umformungsfunktion, die entweder x oder 0 zurückgibt. Für unsere finale Output Layer verwenden wir eine lineare Activation mit Matrixmultiplikation:","metadata":{}},{"cell_type":"markdown","source":"In diesem Code wird eine Funktion namens \"multilayer_perceptron\" definiert, die ein mehrschichtiges neuronales Netzwerk mit ReLU-Aktivierungsfunktionen für die Hidden Layers und linearer Aktivierungsfunktion für die Output Layer erstellt und die Ausgabe des Netzwerks zurückgibt, wobei die Funktionen `my_logger` und `my_timer` als Dekoratoren verwendet werden, um die Ausführung der Funktion zu protokollieren und die Zeitmessung durchzuführen.","metadata":{}},{"cell_type":"code","source":"@my_logger\n@my_timer\ndef multilayer_perceptron(x, weights, biases):\n    '''\n    x : Platzhalter für den Dateninput\n    weights: Dictionary der Weights\n    biases: Dictionary Der Biases\n    '''\n    \n    # Erste Hidden layer mit RELU Activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.relu(layer_1)\n    \n    # Zweite Hidden layer mit RELU Activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.relu(layer_2)\n    \n    # Letzte Output layer mit linearer Activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n    return out_layer","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Weights und Bias\n\nDamit unser Tensorflow Modell funktioniert müssen wir zwei Dictionaries anlegen, die unsere Weights und Biases enthalten. Wir können das `tf.variable` Objekt verwenden. Dies ist anders als eine Konstante, da Tensorflow's Graph Objekt alle Zustände der Variablen wahrnimmt. Eine Variable ist ein anpassbares Tensor, der zwischen Tensorflow's Graph von interagierenden Operationen lebt. Er kann durch die Berechnung verwendet und verändert werden. Wir werden die Modell Parameter generell als Variablen verwenden. Aus der Dokumentation können wir entnehmen:\n\n    A variable maintains state in the graph across calls to `run()`. You add a variable to the graph by constructing an instance of the class `Variable`.\n\n    The `Variable()` constructor requires an initial value for the variable, which can be a `Tensor` of any type and shape. The initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.\n    \nWir werden Tensorflow's eingebaute `random_normal` Methode verwenden, um zufällige Werte für unsere Weights und Biases zu erstellen.","metadata":{}},{"cell_type":"code","source":"# Gewichtsinitialisierung\nweights = {\n    'h1': tf.Variable(tf.random.normal([n_input, n_hidden_1], dtype=tf.float32)),\n    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2], dtype=tf.float32)),\n    'out': tf.Variable(tf.random.normal([n_hidden_2, n_classes], dtype=tf.float32))\n}","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Bias-Initialisierung\nbiases = {\n    'b1': tf.Variable(tf.random.normal([n_hidden_1], dtype=tf.float32)),\n    'b2': tf.Variable(tf.random.normal([n_hidden_2], dtype=tf.float32)),\n    'out': tf.Variable(tf.random.normal([n_classes], dtype=tf.float32))\n}","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Model erstellen\npred = multilayer_perceptron(x, weights, biases)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(784, 256) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\nWARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(256,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\nWARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(256, 256) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\nWARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(256,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\nWARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_2), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(256, 10) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\nWARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(10,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\nmultilayer_perceptron ran in: 0.07732009887695312 sec\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Cost und Optimierungs-Funktion\n\nWir verwenden Tensorflow's eingebaute Funktionen für diesesn Teil. Weitere Details bietet die Dokumentation:","metadata":{}},{"cell_type":"code","source":"# Cost und Optimierungsfunktion definieren\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits=pred))","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Initialisierung der Variablen\n\nWir initialisieren nun alle tf.Variable Objekte die wir zuvor erstellt haben. Das wird das erste sein, dass wir ausführen, wenn wir unser Modell trainieren.","metadata":{}},{"cell_type":"markdown","source":"## Das Modell trainieren\n\n### next_batch()\n\nBevor wir beginnen möchte ich eine weitere nützliche Funktion in unserem MNIST Datenobjekt abdecken, die `next_batch` heißt. Diese gibt ein Tupel in der Form (X,y) mit einem X Array der Daten und einem y Array der Klasse. Zum Beispiel:","metadata":{}},{"cell_type":"code","source":"# Erstelle ein TensorFlow-Dataset aus den Trainingsdaten\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n\n# Mische die Daten und teile sie in Batches auf\nbatch_size = 1\ntrain_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n\n# Erstelle einen Iterator für das Dataset\ntrain_iterator = iter(train_dataset)\n\n# Greife auf ein Batch von Daten zu\nXsamp, ysamp = next(train_iterator)","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"2023-10-29 23:29:15.314982: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [48000]\n\t [[{{node Placeholder/_1}}]]\n2023-10-29 23:29:15.315731: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [48000]\n\t [[{{node Placeholder/_1}}]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Wandele Xsamp in ein Numpy-Array um\nXsamp_numpy = Xsamp.numpy()\n\n# Zeige das Bild mit imshow\nplt.imshow(Xsamp_numpy.reshape(28, 28))","metadata":{"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f12244e47c0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"print(ysamp)","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tf.Tensor([7], shape=(1,), dtype=uint8)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ihre Decorators\n\ndef my_logger(orig_func):\n    logging.basicConfig(filename='{}.log'.format(orig_func.__name__), level=logging.INFO)\n\n    @wraps(orig_func)\n    def wrapper(*args, **kwargs):\n        logging.info(\n            'Ran with args: {}, and kwargs: {}'.format(args, kwargs))\n        return orig_func(*args, **kwargs)\n\n    return wrapper\n\ndef my_timer(orig_func):\n    @wraps(orig_func)\n    def wrapper(*args, **kwargs):\n        t1 = time.time()\n        result = orig_func(*args, **kwargs)\n        t2 = time.time() - t1\n        print('{} ran in: {} sec'.format(orig_func.__name__, t2))\n        return result\n\n    return wrapper\n","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Die Session ausführen\n\nJetzt ist es Zeit unsere Session auszuführen! Achte darauf wie wir zwei Schleifen verwenden. Die äußere, die die Epochs durchläuft, und die innere, die die Batches für jede Epoch des Trainings ausführt.\n\n## Wichtig, hier wurden aus zeitlichen Gründen mit 1% der Daten gearbeitet. Der Code ist mit jeder Prozentzahl reproduzierbar.\n\nEs wurde aus zeitlicher und übersichtlicher Sicht eine View sowie eine Abfrage des gewünschten Prozentsatzes eingebaut.","metadata":{"tags":[]}},{"cell_type":"code","source":"# from tensorflow import keras\n\n# Eingabe vom Benutzer: Prozentsatz der Daten, die verarbeitet werden sollen\npercentage_to_process = float(input(\"Geben Sie den Prozentsatz der Daten ein, der verarbeitet werden soll (0-100): \"))\n\n# Berechnen Sie die Anzahl der Datensätze, die verarbeitet werden sollen\nn_samples_to_process = int(n_samples * (percentage_to_process / 100))\n\n# Training Epochs\nfor epoch in range(training_epochs):\n    avg_cost = 0.0\n\n    total_batch = int(n_samples_to_process / batch_size)\n\n    for i in range(total_batch):\n\n        # Den nächsten Batch an Trainingsdaten und -labels nehmen\n\n        batch_x = tf.cast(x_train[i * batch_size: (i + 1) * batch_size], tf.float32)\n        batch_y = tf.cast(y_train[i * batch_size: (i + 1) * batch_size], tf.float32)\n\n        # Führen Sie die Optimierung und Cost-Berechnung durch\n\n        with tf.GradientTape() as tape:\n\n            pred = multilayer_perceptron(batch_x, weights, biases)\n\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=batch_y, logits=pred))\n\n        gradients = tape.gradient(loss, list(weights.values()) + list(biases.values()))\n        optimizer.apply_gradients(zip(gradients, list(weights.values()) + list(biases.values())))\n\n        avg_cost += loss / total_batch\n\n    print(\"Epoch: {} Cost={:.4f}\".format(epoch + 1, avg_cost))\n\nprint(\"Modellierung ist beendet: {} Epochs of Training\".format(training_epochs))\n","metadata":{"trusted":true},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdin","text":"Geben Sie den Prozentsatz der Daten ein, der verarbeitet werden soll (0-100):  1\n"},{"name":"stdout","text":"multilayer_perceptron ran in: 0.007580280303955078 sec\nmultilayer_perceptron ran in: 0.0015866756439208984 sec\nmultilayer_perceptron ran in: 0.0015070438385009766 sec\nmultilayer_perceptron ran in: 0.0016067028045654297 sec\nmultilayer_perceptron ran in: 0.001565694808959961 sec\nmultilayer_perceptron ran in: 0.0015959739685058594 sec\nmultilayer_perceptron ran in: 0.0018885135650634766 sec\nmultilayer_perceptron ran in: 0.0015482902526855469 sec\nmultilayer_perceptron ran in: 0.0016939640045166016 sec\nmultilayer_perceptron ran in: 0.0015714168548583984 sec\nmultilayer_perceptron ran in: 0.001684427261352539 sec\nmultilayer_perceptron ran in: 0.0016222000122070312 sec\nmultilayer_perceptron ran in: 0.0017216205596923828 sec\nmultilayer_perceptron ran in: 0.0016353130340576172 sec\nmultilayer_perceptron ran in: 0.0015475749969482422 sec\nmultilayer_perceptron ran in: 0.0015578269958496094 sec\nmultilayer_perceptron ran in: 0.0015196800231933594 sec\nmultilayer_perceptron ran in: 0.0013599395751953125 sec\nmultilayer_perceptron ran in: 0.0014586448669433594 sec\nmultilayer_perceptron ran in: 0.0014874935150146484 sec\nmultilayer_perceptron ran in: 0.0014820098876953125 sec\nmultilayer_perceptron ran in: 0.0015070438385009766 sec\nmultilayer_perceptron ran in: 0.0015559196472167969 sec\nmultilayer_perceptron ran in: 0.0014743804931640625 sec\nmultilayer_perceptron ran in: 0.0015151500701904297 sec\nmultilayer_perceptron ran in: 0.001535177230834961 sec\nmultilayer_perceptron ran in: 0.001453399658203125 sec\nmultilayer_perceptron ran in: 0.0016644001007080078 sec\nmultilayer_perceptron ran in: 0.0015277862548828125 sec\nmultilayer_perceptron ran in: 0.0015978813171386719 sec\nmultilayer_perceptron ran in: 0.0015668869018554688 sec\nmultilayer_perceptron ran in: 0.0019674301147460938 sec\nmultilayer_perceptron ran in: 0.0024869441986083984 sec\nmultilayer_perceptron ran in: 0.0015652179718017578 sec\nmultilayer_perceptron ran in: 0.0015099048614501953 sec\nmultilayer_perceptron ran in: 0.001779794692993164 sec\nmultilayer_perceptron ran in: 0.0017681121826171875 sec\nmultilayer_perceptron ran in: 0.0017254352569580078 sec\nmultilayer_perceptron ran in: 0.0015494823455810547 sec\nmultilayer_perceptron ran in: 0.0015132427215576172 sec\nmultilayer_perceptron ran in: 0.0015037059783935547 sec\nmultilayer_perceptron ran in: 0.0028336048126220703 sec\nmultilayer_perceptron ran in: 0.0016283988952636719 sec\nmultilayer_perceptron ran in: 0.0015964508056640625 sec\nmultilayer_perceptron ran in: 0.0015821456909179688 sec\nmultilayer_perceptron ran in: 0.0015289783477783203 sec\nmultilayer_perceptron ran in: 0.0015079975128173828 sec\nmultilayer_perceptron ran in: 0.0015430450439453125 sec\nmultilayer_perceptron ran in: 0.0015556812286376953 sec\nmultilayer_perceptron ran in: 0.0016570091247558594 sec\nmultilayer_perceptron ran in: 0.0015017986297607422 sec\nmultilayer_perceptron ran in: 0.0016334056854248047 sec\nmultilayer_perceptron ran in: 0.0015976428985595703 sec\nmultilayer_perceptron ran in: 0.0015480518341064453 sec\nmultilayer_perceptron ran in: 0.0015895366668701172 sec\nmultilayer_perceptron ran in: 0.0015490055084228516 sec\nmultilayer_perceptron ran in: 0.0016434192657470703 sec\nmultilayer_perceptron ran in: 0.0015265941619873047 sec\nmultilayer_perceptron ran in: 0.001505136489868164 sec\nmultilayer_perceptron ran in: 0.0014905929565429688 sec\nmultilayer_perceptron ran in: 0.0015106201171875 sec\nmultilayer_perceptron ran in: 0.0014653205871582031 sec\nmultilayer_perceptron ran in: 0.0015816688537597656 sec\nmultilayer_perceptron ran in: 0.0015492439270019531 sec\nmultilayer_perceptron ran in: 0.0015120506286621094 sec\nmultilayer_perceptron ran in: 0.0014793872833251953 sec\nmultilayer_perceptron ran in: 0.00164794921875 sec\nmultilayer_perceptron ran in: 0.0015206336975097656 sec\nmultilayer_perceptron ran in: 0.001661539077758789 sec\nmultilayer_perceptron ran in: 0.0015883445739746094 sec\nmultilayer_perceptron ran in: 0.0018277168273925781 sec\nmultilayer_perceptron ran in: 0.0015447139739990234 sec\nmultilayer_perceptron ran in: 0.0015485286712646484 sec\nmultilayer_perceptron ran in: 0.0015964508056640625 sec\nmultilayer_perceptron ran in: 0.0015556812286376953 sec\nmultilayer_perceptron ran in: 0.0016193389892578125 sec\nmultilayer_perceptron ran in: 0.0016455650329589844 sec\nmultilayer_perceptron ran in: 0.0016219615936279297 sec\nmultilayer_perceptron ran in: 0.0016243457794189453 sec\nmultilayer_perceptron ran in: 0.0015933513641357422 sec\nmultilayer_perceptron ran in: 0.0015149116516113281 sec\nmultilayer_perceptron ran in: 0.0015878677368164062 sec\nmultilayer_perceptron ran in: 0.001512765884399414 sec\nmultilayer_perceptron ran in: 0.0015208721160888672 sec\nmultilayer_perceptron ran in: 0.001489877700805664 sec\nmultilayer_perceptron ran in: 0.0016045570373535156 sec\nmultilayer_perceptron ran in: 0.0014996528625488281 sec\nmultilayer_perceptron ran in: 0.0015692710876464844 sec\nmultilayer_perceptron ran in: 0.001528024673461914 sec\nmultilayer_perceptron ran in: 0.0017080307006835938 sec\nmultilayer_perceptron ran in: 0.0017993450164794922 sec\nmultilayer_perceptron ran in: 0.002134561538696289 sec\nmultilayer_perceptron ran in: 0.0025959014892578125 sec\nmultilayer_perceptron ran in: 0.0023322105407714844 sec\nmultilayer_perceptron ran in: 0.00211334228515625 sec\nmultilayer_perceptron ran in: 0.001628875732421875 sec\nmultilayer_perceptron ran in: 0.001744985580444336 sec\nmultilayer_perceptron ran in: 0.0015749931335449219 sec\nmultilayer_perceptron ran in: 0.002009153366088867 sec\nmultilayer_perceptron ran in: 0.0015323162078857422 sec\nmultilayer_perceptron ran in: 0.0016355514526367188 sec\nmultilayer_perceptron ran in: 0.0015838146209716797 sec\nmultilayer_perceptron ran in: 0.0015423297882080078 sec\nmultilayer_perceptron ran in: 0.0016510486602783203 sec\nmultilayer_perceptron ran in: 0.0016016960144042969 sec\nmultilayer_perceptron ran in: 0.0019488334655761719 sec\nmultilayer_perceptron ran in: 0.0015251636505126953 sec\nmultilayer_perceptron ran in: 0.0015697479248046875 sec\nmultilayer_perceptron ran in: 0.0016870498657226562 sec\nmultilayer_perceptron ran in: 0.0015995502471923828 sec\nmultilayer_perceptron ran in: 0.0017261505126953125 sec\nmultilayer_perceptron ran in: 0.0014791488647460938 sec\nmultilayer_perceptron ran in: 0.0022461414337158203 sec\nmultilayer_perceptron ran in: 0.0015606880187988281 sec\nmultilayer_perceptron ran in: 0.00272369384765625 sec\nmultilayer_perceptron ran in: 0.0015377998352050781 sec\nmultilayer_perceptron ran in: 0.0015397071838378906 sec\nmultilayer_perceptron ran in: 0.0015072822570800781 sec\nmultilayer_perceptron ran in: 0.0014767646789550781 sec\nmultilayer_perceptron ran in: 0.0016207695007324219 sec\nmultilayer_perceptron ran in: 0.0014925003051757812 sec\nmultilayer_perceptron ran in: 0.0016410350799560547 sec\nmultilayer_perceptron ran in: 0.0016641616821289062 sec\nmultilayer_perceptron ran in: 0.0016453266143798828 sec\nmultilayer_perceptron ran in: 0.0015256404876708984 sec\nmultilayer_perceptron ran in: 0.001489877700805664 sec\nmultilayer_perceptron ran in: 0.0017864704132080078 sec\nmultilayer_perceptron ran in: 0.0015645027160644531 sec\nmultilayer_perceptron ran in: 0.0015869140625 sec\nmultilayer_perceptron ran in: 0.001598358154296875 sec\nmultilayer_perceptron ran in: 0.0018017292022705078 sec\nmultilayer_perceptron ran in: 0.0015447139739990234 sec\nmultilayer_perceptron ran in: 0.002026796340942383 sec\nmultilayer_perceptron ran in: 0.001413583755493164 sec\nmultilayer_perceptron ran in: 0.0023436546325683594 sec\nmultilayer_perceptron ran in: 0.0013740062713623047 sec\nmultilayer_perceptron ran in: 0.0015377998352050781 sec\nmultilayer_perceptron ran in: 0.0016472339630126953 sec\nmultilayer_perceptron ran in: 0.001575469970703125 sec\nmultilayer_perceptron ran in: 0.0015139579772949219 sec\nmultilayer_perceptron ran in: 0.001562356948852539 sec\nmultilayer_perceptron ran in: 0.0015375614166259766 sec\nmultilayer_perceptron ran in: 0.0015995502471923828 sec\nmultilayer_perceptron ran in: 0.0019469261169433594 sec\nmultilayer_perceptron ran in: 0.001486063003540039 sec\nmultilayer_perceptron ran in: 0.0016634464263916016 sec\nmultilayer_perceptron ran in: 0.0015430450439453125 sec\nmultilayer_perceptron ran in: 0.0016067028045654297 sec\nmultilayer_perceptron ran in: 0.0015342235565185547 sec\nmultilayer_perceptron ran in: 0.0028657913208007812 sec\nmultilayer_perceptron ran in: 0.0015804767608642578 sec\nmultilayer_perceptron ran in: 0.0015447139739990234 sec\nmultilayer_perceptron ran in: 0.001833200454711914 sec\nmultilayer_perceptron ran in: 0.001516103744506836 sec\nmultilayer_perceptron ran in: 0.0015304088592529297 sec\nmultilayer_perceptron ran in: 0.001546621322631836 sec\nmultilayer_perceptron ran in: 0.0016574859619140625 sec\nmultilayer_perceptron ran in: 0.0014507770538330078 sec\nmultilayer_perceptron ran in: 0.0015344619750976562 sec\nmultilayer_perceptron ran in: 0.001565694808959961 sec\nmultilayer_perceptron ran in: 0.0016248226165771484 sec\nmultilayer_perceptron ran in: 0.0164492130279541 sec\nmultilayer_perceptron ran in: 0.0015213489532470703 sec\nmultilayer_perceptron ran in: 0.001516580581665039 sec\nmultilayer_perceptron ran in: 0.0014772415161132812 sec\nmultilayer_perceptron ran in: 0.0015985965728759766 sec\nmultilayer_perceptron ran in: 0.0016148090362548828 sec\nmultilayer_perceptron ran in: 0.0015540122985839844 sec\nmultilayer_perceptron ran in: 0.0016722679138183594 sec\nmultilayer_perceptron ran in: 0.0016222000122070312 sec\nmultilayer_perceptron ran in: 0.001537322998046875 sec\nmultilayer_perceptron ran in: 0.001604318618774414 sec\nmultilayer_perceptron ran in: 0.0018825531005859375 sec\nmultilayer_perceptron ran in: 0.001967906951904297 sec\nmultilayer_perceptron ran in: 0.0021665096282958984 sec\nmultilayer_perceptron ran in: 0.0018360614776611328 sec\nmultilayer_perceptron ran in: 0.001956939697265625 sec\nmultilayer_perceptron ran in: 0.0019290447235107422 sec\nmultilayer_perceptron ran in: 0.0018777847290039062 sec\nmultilayer_perceptron ran in: 0.0021102428436279297 sec\nmultilayer_perceptron ran in: 0.0017895698547363281 sec\nmultilayer_perceptron ran in: 0.0017516613006591797 sec\nmultilayer_perceptron ran in: 0.0019233226776123047 sec\nmultilayer_perceptron ran in: 0.0017528533935546875 sec\nmultilayer_perceptron ran in: 0.0015959739685058594 sec\nmultilayer_perceptron ran in: 0.0015425682067871094 sec\nmultilayer_perceptron ran in: 0.0016214847564697266 sec\nmultilayer_perceptron ran in: 0.001615762710571289 sec\nmultilayer_perceptron ran in: 0.0020208358764648438 sec\nmultilayer_perceptron ran in: 0.0015914440155029297 sec\nmultilayer_perceptron ran in: 0.0016436576843261719 sec\nmultilayer_perceptron ran in: 0.001729726791381836 sec\nmultilayer_perceptron ran in: 0.001611471176147461 sec\nmultilayer_perceptron ran in: 0.0017740726470947266 sec\nmultilayer_perceptron ran in: 0.0016622543334960938 sec\nmultilayer_perceptron ran in: 0.0017042160034179688 sec\nmultilayer_perceptron ran in: 0.0017237663269042969 sec\nmultilayer_perceptron ran in: 0.0017731189727783203 sec\nmultilayer_perceptron ran in: 0.0017158985137939453 sec\nmultilayer_perceptron ran in: 0.0016512870788574219 sec\nmultilayer_perceptron ran in: 0.0016374588012695312 sec\nmultilayer_perceptron ran in: 0.0016930103302001953 sec\nmultilayer_perceptron ran in: 0.0018277168273925781 sec\nmultilayer_perceptron ran in: 0.0018603801727294922 sec\nmultilayer_perceptron ran in: 0.0016720294952392578 sec\nmultilayer_perceptron ran in: 0.019139528274536133 sec\nmultilayer_perceptron ran in: 0.0016489028930664062 sec\nmultilayer_perceptron ran in: 0.001596212387084961 sec\nmultilayer_perceptron ran in: 0.0016338825225830078 sec\nmultilayer_perceptron ran in: 0.0016810894012451172 sec\nmultilayer_perceptron ran in: 0.002428293228149414 sec\nmultilayer_perceptron ran in: 0.0015537738800048828 sec\nmultilayer_perceptron ran in: 0.001529693603515625 sec\nmultilayer_perceptron ran in: 0.0015347003936767578 sec\nmultilayer_perceptron ran in: 0.0018699169158935547 sec\nmultilayer_perceptron ran in: 0.0015783309936523438 sec\nmultilayer_perceptron ran in: 0.001645803451538086 sec\nmultilayer_perceptron ran in: 0.0015499591827392578 sec\nmultilayer_perceptron ran in: 0.001644134521484375 sec\nmultilayer_perceptron ran in: 0.0016803741455078125 sec\nmultilayer_perceptron ran in: 0.0017461776733398438 sec\nmultilayer_perceptron ran in: 0.0016031265258789062 sec\nmultilayer_perceptron ran in: 0.0015780925750732422 sec\nmultilayer_perceptron ran in: 0.001638174057006836 sec\nmultilayer_perceptron ran in: 0.0015485286712646484 sec\nmultilayer_perceptron ran in: 0.0016417503356933594 sec\nmultilayer_perceptron ran in: 0.0015883445739746094 sec\nmultilayer_perceptron ran in: 0.0016672611236572266 sec\nmultilayer_perceptron ran in: 0.0017774105072021484 sec\nmultilayer_perceptron ran in: 0.0016605854034423828 sec\nmultilayer_perceptron ran in: 0.0016255378723144531 sec\nmultilayer_perceptron ran in: 0.0015823841094970703 sec\nmultilayer_perceptron ran in: 0.0017070770263671875 sec\nmultilayer_perceptron ran in: 0.0016632080078125 sec\nmultilayer_perceptron ran in: 0.0021507740020751953 sec\nmultilayer_perceptron ran in: 0.0016961097717285156 sec\nmultilayer_perceptron ran in: 0.0015556812286376953 sec\nmultilayer_perceptron ran in: 0.001558065414428711 sec\nmultilayer_perceptron ran in: 0.0015749931335449219 sec\nmultilayer_perceptron ran in: 0.0017466545104980469 sec\nmultilayer_perceptron ran in: 0.001556396484375 sec\nmultilayer_perceptron ran in: 0.0016529560089111328 sec\nmultilayer_perceptron ran in: 0.0016639232635498047 sec\nmultilayer_perceptron ran in: 0.0016460418701171875 sec\nmultilayer_perceptron ran in: 0.001653432846069336 sec\nmultilayer_perceptron ran in: 0.0069391727447509766 sec\nmultilayer_perceptron ran in: 0.0016782283782958984 sec\nmultilayer_perceptron ran in: 0.0016341209411621094 sec\nmultilayer_perceptron ran in: 0.0016562938690185547 sec\nmultilayer_perceptron ran in: 0.0017561912536621094 sec\nmultilayer_perceptron ran in: 0.0016155242919921875 sec\nmultilayer_perceptron ran in: 0.001680612564086914 sec\nmultilayer_perceptron ran in: 0.0019412040710449219 sec\nmultilayer_perceptron ran in: 0.0016710758209228516 sec\nmultilayer_perceptron ran in: 0.0016605854034423828 sec\nmultilayer_perceptron ran in: 0.0017402172088623047 sec\nmultilayer_perceptron ran in: 0.0021359920501708984 sec\nmultilayer_perceptron ran in: 0.002371072769165039 sec\nmultilayer_perceptron ran in: 0.0015616416931152344 sec\nmultilayer_perceptron ran in: 0.0017642974853515625 sec\nmultilayer_perceptron ran in: 0.0017178058624267578 sec\nmultilayer_perceptron ran in: 0.0016391277313232422 sec\nmultilayer_perceptron ran in: 0.00167083740234375 sec\nmultilayer_perceptron ran in: 0.0017523765563964844 sec\nmultilayer_perceptron ran in: 0.0016672611236572266 sec\nmultilayer_perceptron ran in: 0.0016293525695800781 sec\nmultilayer_perceptron ran in: 0.0016491413116455078 sec\nmultilayer_perceptron ran in: 0.0016677379608154297 sec\nmultilayer_perceptron ran in: 0.0016257762908935547 sec\nmultilayer_perceptron ran in: 0.0015614032745361328 sec\nmultilayer_perceptron ran in: 0.0017063617706298828 sec\nmultilayer_perceptron ran in: 0.0015683174133300781 sec\nmultilayer_perceptron ran in: 0.0017445087432861328 sec\nmultilayer_perceptron ran in: 0.0016040802001953125 sec\nmultilayer_perceptron ran in: 0.0015532970428466797 sec\nmultilayer_perceptron ran in: 0.0015869140625 sec\nmultilayer_perceptron ran in: 0.0019516944885253906 sec\nmultilayer_perceptron ran in: 0.024451732635498047 sec\nmultilayer_perceptron ran in: 0.0034258365631103516 sec\nmultilayer_perceptron ran in: 0.0015590190887451172 sec\nmultilayer_perceptron ran in: 0.0015878677368164062 sec\nmultilayer_perceptron ran in: 0.0017676353454589844 sec\nmultilayer_perceptron ran in: 0.0017518997192382812 sec\nmultilayer_perceptron ran in: 0.0017597675323486328 sec\nmultilayer_perceptron ran in: 0.0015652179718017578 sec\nmultilayer_perceptron ran in: 0.0016777515411376953 sec\nmultilayer_perceptron ran in: 0.0017719268798828125 sec\nmultilayer_perceptron ran in: 0.0017824172973632812 sec\nmultilayer_perceptron ran in: 0.0016410350799560547 sec\nmultilayer_perceptron ran in: 0.001955747604370117 sec\nmultilayer_perceptron ran in: 0.0015795230865478516 sec\nmultilayer_perceptron ran in: 0.0015492439270019531 sec\nmultilayer_perceptron ran in: 0.0015437602996826172 sec\nmultilayer_perceptron ran in: 0.0015187263488769531 sec\nmultilayer_perceptron ran in: 0.0014433860778808594 sec\nmultilayer_perceptron ran in: 0.0015454292297363281 sec\nmultilayer_perceptron ran in: 0.0014262199401855469 sec\nmultilayer_perceptron ran in: 0.0014262199401855469 sec\nmultilayer_perceptron ran in: 0.0015823841094970703 sec\nmultilayer_perceptron ran in: 0.0018229484558105469 sec\nmultilayer_perceptron ran in: 0.0015561580657958984 sec\nmultilayer_perceptron ran in: 0.0015859603881835938 sec\nmultilayer_perceptron ran in: 0.0016510486602783203 sec\nmultilayer_perceptron ran in: 0.0016300678253173828 sec\nmultilayer_perceptron ran in: 0.0014035701751708984 sec\nmultilayer_perceptron ran in: 0.0016632080078125 sec\nmultilayer_perceptron ran in: 0.0018901824951171875 sec\nmultilayer_perceptron ran in: 0.0016112327575683594 sec\nmultilayer_perceptron ran in: 0.0016286373138427734 sec\nmultilayer_perceptron ran in: 0.0015134811401367188 sec\nmultilayer_perceptron ran in: 0.0020914077758789062 sec\nmultilayer_perceptron ran in: 0.0016295909881591797 sec\nmultilayer_perceptron ran in: 0.0016047954559326172 sec\nmultilayer_perceptron ran in: 0.0025548934936523438 sec\nmultilayer_perceptron ran in: 0.01182246208190918 sec\nmultilayer_perceptron ran in: 0.001544952392578125 sec\nmultilayer_perceptron ran in: 0.014978408813476562 sec\nmultilayer_perceptron ran in: 0.0016596317291259766 sec\nmultilayer_perceptron ran in: 0.00156402587890625 sec\nmultilayer_perceptron ran in: 0.0018656253814697266 sec\nmultilayer_perceptron ran in: 0.001584768295288086 sec\nmultilayer_perceptron ran in: 0.0024919509887695312 sec\nmultilayer_perceptron ran in: 0.0022127628326416016 sec\nmultilayer_perceptron ran in: 0.0019855499267578125 sec\nmultilayer_perceptron ran in: 0.0017881393432617188 sec\nmultilayer_perceptron ran in: 0.0020182132720947266 sec\nmultilayer_perceptron ran in: 0.002910614013671875 sec\nmultilayer_perceptron ran in: 0.0017359256744384766 sec\nmultilayer_perceptron ran in: 0.002425670623779297 sec\nmultilayer_perceptron ran in: 0.002506256103515625 sec\nmultilayer_perceptron ran in: 0.0018205642700195312 sec\nmultilayer_perceptron ran in: 0.001997232437133789 sec\nmultilayer_perceptron ran in: 0.001611471176147461 sec\nmultilayer_perceptron ran in: 0.0028295516967773438 sec\nmultilayer_perceptron ran in: 0.0015747547149658203 sec\nmultilayer_perceptron ran in: 0.0019249916076660156 sec\nmultilayer_perceptron ran in: 0.0014271736145019531 sec\nmultilayer_perceptron ran in: 0.0014781951904296875 sec\nmultilayer_perceptron ran in: 0.0016436576843261719 sec\nmultilayer_perceptron ran in: 0.0014483928680419922 sec\nmultilayer_perceptron ran in: 0.0015611648559570312 sec\nmultilayer_perceptron ran in: 0.0015387535095214844 sec\nmultilayer_perceptron ran in: 0.002264261245727539 sec\nmultilayer_perceptron ran in: 0.0028498172760009766 sec\nmultilayer_perceptron ran in: 0.0026481151580810547 sec\nmultilayer_perceptron ran in: 0.0015091896057128906 sec\nmultilayer_perceptron ran in: 0.0015175342559814453 sec\nmultilayer_perceptron ran in: 0.0015366077423095703 sec\nmultilayer_perceptron ran in: 0.0015447139739990234 sec\nmultilayer_perceptron ran in: 0.0016510486602783203 sec\nmultilayer_perceptron ran in: 0.0015146732330322266 sec\nmultilayer_perceptron ran in: 0.0016260147094726562 sec\nmultilayer_perceptron ran in: 0.0015444755554199219 sec\nmultilayer_perceptron ran in: 0.0014431476593017578 sec\nmultilayer_perceptron ran in: 0.0015115737915039062 sec\nmultilayer_perceptron ran in: 0.0015759468078613281 sec\nmultilayer_perceptron ran in: 0.0022678375244140625 sec\nmultilayer_perceptron ran in: 0.0015816688537597656 sec\nmultilayer_perceptron ran in: 0.0014729499816894531 sec\nmultilayer_perceptron ran in: 0.0027637481689453125 sec\nmultilayer_perceptron ran in: 0.001773834228515625 sec\nmultilayer_perceptron ran in: 0.0016167163848876953 sec\nmultilayer_perceptron ran in: 0.0014297962188720703 sec\nmultilayer_perceptron ran in: 0.001451253890991211 sec\nmultilayer_perceptron ran in: 0.0014925003051757812 sec\nmultilayer_perceptron ran in: 0.00167083740234375 sec\nmultilayer_perceptron ran in: 0.001562356948852539 sec\nmultilayer_perceptron ran in: 0.0018513202667236328 sec\nmultilayer_perceptron ran in: 0.0016567707061767578 sec\nmultilayer_perceptron ran in: 0.0015635490417480469 sec\nmultilayer_perceptron ran in: 0.0015978813171386719 sec\nmultilayer_perceptron ran in: 0.0016281604766845703 sec\nmultilayer_perceptron ran in: 0.0017495155334472656 sec\nmultilayer_perceptron ran in: 0.0017249584197998047 sec\nmultilayer_perceptron ran in: 0.0016443729400634766 sec\nmultilayer_perceptron ran in: 0.0016887187957763672 sec\nmultilayer_perceptron ran in: 0.0015347003936767578 sec\nmultilayer_perceptron ran in: 0.0017292499542236328 sec\nmultilayer_perceptron ran in: 0.0015895366668701172 sec\nmultilayer_perceptron ran in: 0.0026509761810302734 sec\nmultilayer_perceptron ran in: 0.0016303062438964844 sec\nmultilayer_perceptron ran in: 0.001645803451538086 sec\nmultilayer_perceptron ran in: 0.0016782283782958984 sec\nmultilayer_perceptron ran in: 0.0021178722381591797 sec\nmultilayer_perceptron ran in: 0.001622915267944336 sec\nmultilayer_perceptron ran in: 0.0016777515411376953 sec\nmultilayer_perceptron ran in: 0.0016093254089355469 sec\nmultilayer_perceptron ran in: 0.0017900466918945312 sec\nmultilayer_perceptron ran in: 0.0015091896057128906 sec\nmultilayer_perceptron ran in: 0.0016651153564453125 sec\nmultilayer_perceptron ran in: 0.0016086101531982422 sec\nmultilayer_perceptron ran in: 0.0015451908111572266 sec\nmultilayer_perceptron ran in: 0.0017116069793701172 sec\nmultilayer_perceptron ran in: 0.0015151500701904297 sec\nmultilayer_perceptron ran in: 0.002132415771484375 sec\nmultilayer_perceptron ran in: 0.0015425682067871094 sec\nmultilayer_perceptron ran in: 0.0014462471008300781 sec\nmultilayer_perceptron ran in: 0.0018661022186279297 sec\nmultilayer_perceptron ran in: 0.002062559127807617 sec\nmultilayer_perceptron ran in: 0.0019578933715820312 sec\nmultilayer_perceptron ran in: 0.001918792724609375 sec\nmultilayer_perceptron ran in: 0.0019121170043945312 sec\nmultilayer_perceptron ran in: 0.0018987655639648438 sec\nmultilayer_perceptron ran in: 0.002994537353515625 sec\nmultilayer_perceptron ran in: 0.0018639564514160156 sec\nmultilayer_perceptron ran in: 0.002631664276123047 sec\nmultilayer_perceptron ran in: 0.001997709274291992 sec\nmultilayer_perceptron ran in: 0.0018889904022216797 sec\nmultilayer_perceptron ran in: 0.0018951892852783203 sec\nmultilayer_perceptron ran in: 0.0018024444580078125 sec\nmultilayer_perceptron ran in: 0.0017256736755371094 sec\nmultilayer_perceptron ran in: 0.0017862319946289062 sec\nmultilayer_perceptron ran in: 0.0019152164459228516 sec\nmultilayer_perceptron ran in: 0.0018162727355957031 sec\nmultilayer_perceptron ran in: 0.0018677711486816406 sec\nmultilayer_perceptron ran in: 0.0018761157989501953 sec\nmultilayer_perceptron ran in: 0.0019254684448242188 sec\nmultilayer_perceptron ran in: 0.001924276351928711 sec\nmultilayer_perceptron ran in: 0.0023131370544433594 sec\nmultilayer_perceptron ran in: 0.0017690658569335938 sec\nmultilayer_perceptron ran in: 0.0016095638275146484 sec\nmultilayer_perceptron ran in: 0.0017666816711425781 sec\nmultilayer_perceptron ran in: 0.0025975704193115234 sec\nmultilayer_perceptron ran in: 0.001535177230834961 sec\nmultilayer_perceptron ran in: 0.0015621185302734375 sec\nmultilayer_perceptron ran in: 0.0017261505126953125 sec\nmultilayer_perceptron ran in: 0.0016925334930419922 sec\nmultilayer_perceptron ran in: 0.001875162124633789 sec\nmultilayer_perceptron ran in: 0.0017764568328857422 sec\nmultilayer_perceptron ran in: 0.0019402503967285156 sec\nmultilayer_perceptron ran in: 0.0017859935760498047 sec\nmultilayer_perceptron ran in: 0.0018520355224609375 sec\nmultilayer_perceptron ran in: 0.0015561580657958984 sec\nmultilayer_perceptron ran in: 0.0017490386962890625 sec\nmultilayer_perceptron ran in: 0.0016880035400390625 sec\nmultilayer_perceptron ran in: 0.0019097328186035156 sec\nmultilayer_perceptron ran in: 0.0016498565673828125 sec\nmultilayer_perceptron ran in: 0.0018472671508789062 sec\nmultilayer_perceptron ran in: 0.0018703937530517578 sec\nmultilayer_perceptron ran in: 0.0017178058624267578 sec\nmultilayer_perceptron ran in: 0.0017905235290527344 sec\nmultilayer_perceptron ran in: 0.0018842220306396484 sec\nmultilayer_perceptron ran in: 0.0018982887268066406 sec\nmultilayer_perceptron ran in: 0.0018720626831054688 sec\nmultilayer_perceptron ran in: 0.0019178390502929688 sec\nmultilayer_perceptron ran in: 0.001893758773803711 sec\nmultilayer_perceptron ran in: 0.0019164085388183594 sec\nmultilayer_perceptron ran in: 0.0019195079803466797 sec\nmultilayer_perceptron ran in: 0.001895904541015625 sec\nmultilayer_perceptron ran in: 0.0018246173858642578 sec\nmultilayer_perceptron ran in: 0.002016305923461914 sec\nmultilayer_perceptron ran in: 0.0016582012176513672 sec\nmultilayer_perceptron ran in: 0.0019791126251220703 sec\nmultilayer_perceptron ran in: 0.002321958541870117 sec\nmultilayer_perceptron ran in: 0.0018870830535888672 sec\nmultilayer_perceptron ran in: 0.001924753189086914 sec\nmultilayer_perceptron ran in: 0.002392292022705078 sec\nmultilayer_perceptron ran in: 0.0018749237060546875 sec\nmultilayer_perceptron ran in: 0.0024635791778564453 sec\nmultilayer_perceptron ran in: 0.0018680095672607422 sec\nmultilayer_perceptron ran in: 0.0020787715911865234 sec\nmultilayer_perceptron ran in: 0.0017402172088623047 sec\nmultilayer_perceptron ran in: 0.0018699169158935547 sec\nmultilayer_perceptron ran in: 0.0018711090087890625 sec\nmultilayer_perceptron ran in: 0.0019276142120361328 sec\nmultilayer_perceptron ran in: 0.0016984939575195312 sec\nmultilayer_perceptron ran in: 0.0020284652709960938 sec\nmultilayer_perceptron ran in: 0.0019257068634033203 sec\nmultilayer_perceptron ran in: 0.0019845962524414062 sec\nmultilayer_perceptron ran in: 0.0019571781158447266 sec\nmultilayer_perceptron ran in: 0.002028226852416992 sec\nmultilayer_perceptron ran in: 0.0018961429595947266 sec\nmultilayer_perceptron ran in: 0.0020720958709716797 sec\nmultilayer_perceptron ran in: 0.0019826889038085938 sec\nmultilayer_perceptron ran in: 0.0019452571868896484 sec\nmultilayer_perceptron ran in: 0.001817464828491211 sec\nmultilayer_perceptron ran in: 0.0018973350524902344 sec\nmultilayer_perceptron ran in: 0.019414186477661133 sec\nmultilayer_perceptron ran in: 0.0019092559814453125 sec\nmultilayer_perceptron ran in: 0.0018639564514160156 sec\nEpoch: 1 Cost=523808.5312\nmultilayer_perceptron ran in: 0.0021729469299316406 sec\nmultilayer_perceptron ran in: 0.00196075439453125 sec\nmultilayer_perceptron ran in: 0.0015764236450195312 sec\nmultilayer_perceptron ran in: 0.0017883777618408203 sec\nmultilayer_perceptron ran in: 0.0016937255859375 sec\nmultilayer_perceptron ran in: 0.0016520023345947266 sec\nmultilayer_perceptron ran in: 0.0015361309051513672 sec\nmultilayer_perceptron ran in: 0.0017735958099365234 sec\nmultilayer_perceptron ran in: 0.001580953598022461 sec\nmultilayer_perceptron ran in: 0.0016472339630126953 sec\nmultilayer_perceptron ran in: 0.0017998218536376953 sec\nmultilayer_perceptron ran in: 0.001634836196899414 sec\nmultilayer_perceptron ran in: 0.00162506103515625 sec\nmultilayer_perceptron ran in: 0.0015921592712402344 sec\nmultilayer_perceptron ran in: 0.00164031982421875 sec\nmultilayer_perceptron ran in: 0.001630544662475586 sec\nmultilayer_perceptron ran in: 0.0015871524810791016 sec\nmultilayer_perceptron ran in: 0.0016765594482421875 sec\nmultilayer_perceptron ran in: 0.001543283462524414 sec\nmultilayer_perceptron ran in: 0.0015323162078857422 sec\nmultilayer_perceptron ran in: 0.0015766620635986328 sec\nmultilayer_perceptron ran in: 0.001766204833984375 sec\nmultilayer_perceptron ran in: 0.0015366077423095703 sec\nmultilayer_perceptron ran in: 0.0015633106231689453 sec\nmultilayer_perceptron ran in: 0.001529693603515625 sec\nmultilayer_perceptron ran in: 0.0016596317291259766 sec\nmultilayer_perceptron ran in: 0.001619577407836914 sec\nmultilayer_perceptron ran in: 0.0013544559478759766 sec\nmultilayer_perceptron ran in: 0.0014426708221435547 sec\nmultilayer_perceptron ran in: 0.0044324398040771484 sec\nmultilayer_perceptron ran in: 0.0015935897827148438 sec\nmultilayer_perceptron ran in: 0.0015327930450439453 sec\nmultilayer_perceptron ran in: 0.0015361309051513672 sec\nmultilayer_perceptron ran in: 0.0026557445526123047 sec\nmultilayer_perceptron ran in: 0.001760244369506836 sec\nmultilayer_perceptron ran in: 0.0015363693237304688 sec\nmultilayer_perceptron ran in: 0.001688241958618164 sec\nmultilayer_perceptron ran in: 0.0014009475708007812 sec\nmultilayer_perceptron ran in: 0.0015032291412353516 sec\nmultilayer_perceptron ran in: 0.0016245841979980469 sec\nmultilayer_perceptron ran in: 0.0014235973358154297 sec\nmultilayer_perceptron ran in: 0.0014948844909667969 sec\nmultilayer_perceptron ran in: 0.0016148090362548828 sec\nmultilayer_perceptron ran in: 0.001603841781616211 sec\nmultilayer_perceptron ran in: 0.001642465591430664 sec\nmultilayer_perceptron ran in: 0.0015778541564941406 sec\nmultilayer_perceptron ran in: 0.001626729965209961 sec\nmultilayer_perceptron ran in: 0.0016391277313232422 sec\nmultilayer_perceptron ran in: 0.0016412734985351562 sec\nmultilayer_perceptron ran in: 0.0017125606536865234 sec\nmultilayer_perceptron ran in: 0.001434326171875 sec\nmultilayer_perceptron ran in: 0.0016713142395019531 sec\nmultilayer_perceptron ran in: 0.0018644332885742188 sec\nmultilayer_perceptron ran in: 0.0016405582427978516 sec\nmultilayer_perceptron ran in: 0.0018153190612792969 sec\nmultilayer_perceptron ran in: 0.001940011978149414 sec\nmultilayer_perceptron ran in: 0.0017538070678710938 sec\nmultilayer_perceptron ran in: 0.0017311573028564453 sec\nmultilayer_perceptron ran in: 0.0017850399017333984 sec\nmultilayer_perceptron ran in: 0.001562356948852539 sec\nmultilayer_perceptron ran in: 0.0015900135040283203 sec\nmultilayer_perceptron ran in: 0.0016088485717773438 sec\nmultilayer_perceptron ran in: 0.010774374008178711 sec\nmultilayer_perceptron ran in: 0.0014657974243164062 sec\nmultilayer_perceptron ran in: 0.0016198158264160156 sec\nmultilayer_perceptron ran in: 0.0015900135040283203 sec\nmultilayer_perceptron ran in: 0.004221677780151367 sec\nmultilayer_perceptron ran in: 0.001726388931274414 sec\nmultilayer_perceptron ran in: 0.0022029876708984375 sec\nmultilayer_perceptron ran in: 0.00167083740234375 sec\nmultilayer_perceptron ran in: 0.0015954971313476562 sec\nmultilayer_perceptron ran in: 0.0015826225280761719 sec\nmultilayer_perceptron ran in: 0.0014295578002929688 sec\nmultilayer_perceptron ran in: 0.001680135726928711 sec\nmultilayer_perceptron ran in: 0.0016319751739501953 sec\nmultilayer_perceptron ran in: 0.002054452896118164 sec\nmultilayer_perceptron ran in: 0.0021309852600097656 sec\nmultilayer_perceptron ran in: 0.001583099365234375 sec\nmultilayer_perceptron ran in: 0.0021605491638183594 sec\nmultilayer_perceptron ran in: 0.0016422271728515625 sec\nmultilayer_perceptron ran in: 0.0022499561309814453 sec\nmultilayer_perceptron ran in: 0.0015325546264648438 sec\nmultilayer_perceptron ran in: 0.0014379024505615234 sec\nmultilayer_perceptron ran in: 0.0014057159423828125 sec\nmultilayer_perceptron ran in: 0.001529693603515625 sec\nmultilayer_perceptron ran in: 0.0016942024230957031 sec\nmultilayer_perceptron ran in: 0.0015757083892822266 sec\nmultilayer_perceptron ran in: 0.001692056655883789 sec\nmultilayer_perceptron ran in: 0.0015685558319091797 sec\nmultilayer_perceptron ran in: 0.0022542476654052734 sec\nmultilayer_perceptron ran in: 0.0017995834350585938 sec\nmultilayer_perceptron ran in: 0.011100292205810547 sec\nmultilayer_perceptron ran in: 0.0017430782318115234 sec\nmultilayer_perceptron ran in: 0.0015828609466552734 sec\nmultilayer_perceptron ran in: 0.0016529560089111328 sec\nmultilayer_perceptron ran in: 0.0016455650329589844 sec\nmultilayer_perceptron ran in: 0.0016331672668457031 sec\nmultilayer_perceptron ran in: 0.0017521381378173828 sec\nmultilayer_perceptron ran in: 0.0017254352569580078 sec\nmultilayer_perceptron ran in: 0.0016164779663085938 sec\nmultilayer_perceptron ran in: 0.0020461082458496094 sec\nmultilayer_perceptron ran in: 0.0016758441925048828 sec\nmultilayer_perceptron ran in: 0.0022497177124023438 sec\nmultilayer_perceptron ran in: 0.0016515254974365234 sec\nmultilayer_perceptron ran in: 0.0022382736206054688 sec\nmultilayer_perceptron ran in: 0.002168893814086914 sec\nmultilayer_perceptron ran in: 0.001651763916015625 sec\nmultilayer_perceptron ran in: 0.0016894340515136719 sec\nmultilayer_perceptron ran in: 0.0016336441040039062 sec\nmultilayer_perceptron ran in: 0.0015549659729003906 sec\nmultilayer_perceptron ran in: 0.0016388893127441406 sec\nmultilayer_perceptron ran in: 0.0016636848449707031 sec\nmultilayer_perceptron ran in: 0.002113819122314453 sec\nmultilayer_perceptron ran in: 0.0016660690307617188 sec\nmultilayer_perceptron ran in: 0.01473546028137207 sec\nmultilayer_perceptron ran in: 0.001562356948852539 sec\nmultilayer_perceptron ran in: 0.0016727447509765625 sec\nmultilayer_perceptron ran in: 0.0015358924865722656 sec\nmultilayer_perceptron ran in: 0.0018537044525146484 sec\nmultilayer_perceptron ran in: 0.0017261505126953125 sec\nmultilayer_perceptron ran in: 0.0016741752624511719 sec\nmultilayer_perceptron ran in: 0.0017290115356445312 sec\nmultilayer_perceptron ran in: 0.0015702247619628906 sec\nmultilayer_perceptron ran in: 0.001589059829711914 sec\nmultilayer_perceptron ran in: 0.0016391277313232422 sec\nmultilayer_perceptron ran in: 0.001592874526977539 sec\nmultilayer_perceptron ran in: 0.0016601085662841797 sec\nmultilayer_perceptron ran in: 0.0016942024230957031 sec\nmultilayer_perceptron ran in: 0.0028192996978759766 sec\nmultilayer_perceptron ran in: 0.0018587112426757812 sec\nmultilayer_perceptron ran in: 0.0018622875213623047 sec\nmultilayer_perceptron ran in: 0.0018603801727294922 sec\nmultilayer_perceptron ran in: 0.0020742416381835938 sec\nmultilayer_perceptron ran in: 0.001989126205444336 sec\nmultilayer_perceptron ran in: 0.0018320083618164062 sec\nmultilayer_perceptron ran in: 0.001840829849243164 sec\nmultilayer_perceptron ran in: 0.0018622875213623047 sec\nmultilayer_perceptron ran in: 0.002215862274169922 sec\nmultilayer_perceptron ran in: 0.001986265182495117 sec\nmultilayer_perceptron ran in: 0.0018870830535888672 sec\nmultilayer_perceptron ran in: 0.0018520355224609375 sec\nmultilayer_perceptron ran in: 0.0018379688262939453 sec\nmultilayer_perceptron ran in: 0.0018460750579833984 sec\nmultilayer_perceptron ran in: 0.0019884109497070312 sec\nmultilayer_perceptron ran in: 0.001978158950805664 sec\nmultilayer_perceptron ran in: 0.0020513534545898438 sec\nmultilayer_perceptron ran in: 0.004002094268798828 sec\nmultilayer_perceptron ran in: 0.0016047954559326172 sec\nmultilayer_perceptron ran in: 0.001615762710571289 sec\nmultilayer_perceptron ran in: 0.0016050338745117188 sec\nmultilayer_perceptron ran in: 0.001584768295288086 sec\nmultilayer_perceptron ran in: 0.001615285873413086 sec\nmultilayer_perceptron ran in: 0.001626729965209961 sec\nmultilayer_perceptron ran in: 0.0016858577728271484 sec\nmultilayer_perceptron ran in: 0.0016384124755859375 sec\nmultilayer_perceptron ran in: 0.0015382766723632812 sec\nmultilayer_perceptron ran in: 0.0015172958374023438 sec\nmultilayer_perceptron ran in: 0.0020134449005126953 sec\nmultilayer_perceptron ran in: 0.0016632080078125 sec\nmultilayer_perceptron ran in: 0.0017554759979248047 sec\nmultilayer_perceptron ran in: 0.0015642642974853516 sec\nmultilayer_perceptron ran in: 0.0015912055969238281 sec\nmultilayer_perceptron ran in: 0.0016334056854248047 sec\nmultilayer_perceptron ran in: 0.002040863037109375 sec\nmultilayer_perceptron ran in: 0.0016627311706542969 sec\nmultilayer_perceptron ran in: 0.001691579818725586 sec\nmultilayer_perceptron ran in: 0.0017895698547363281 sec\nmultilayer_perceptron ran in: 0.0016584396362304688 sec\nmultilayer_perceptron ran in: 0.001615762710571289 sec\nmultilayer_perceptron ran in: 0.0016331672668457031 sec\nmultilayer_perceptron ran in: 0.0015189647674560547 sec\nmultilayer_perceptron ran in: 0.0016820430755615234 sec\nmultilayer_perceptron ran in: 0.0016810894012451172 sec\nmultilayer_perceptron ran in: 0.0015113353729248047 sec\nmultilayer_perceptron ran in: 0.0017015933990478516 sec\nmultilayer_perceptron ran in: 0.0016071796417236328 sec\nmultilayer_perceptron ran in: 0.001539468765258789 sec\nmultilayer_perceptron ran in: 0.001508951187133789 sec\nmultilayer_perceptron ran in: 0.0021309852600097656 sec\nmultilayer_perceptron ran in: 0.001615762710571289 sec\nmultilayer_perceptron ran in: 0.0020465850830078125 sec\nmultilayer_perceptron ran in: 0.0015573501586914062 sec\nmultilayer_perceptron ran in: 0.0032231807708740234 sec\nmultilayer_perceptron ran in: 0.0015323162078857422 sec\nmultilayer_perceptron ran in: 0.0015535354614257812 sec\nmultilayer_perceptron ran in: 0.001538991928100586 sec\nmultilayer_perceptron ran in: 0.0015544891357421875 sec\nmultilayer_perceptron ran in: 0.0015482902526855469 sec\nmultilayer_perceptron ran in: 0.0014998912811279297 sec\nmultilayer_perceptron ran in: 0.0015501976013183594 sec\nmultilayer_perceptron ran in: 0.0016934871673583984 sec\nmultilayer_perceptron ran in: 0.001705169677734375 sec\nmultilayer_perceptron ran in: 0.0016665458679199219 sec\nmultilayer_perceptron ran in: 0.0027599334716796875 sec\nmultilayer_perceptron ran in: 0.0018663406372070312 sec\nmultilayer_perceptron ran in: 0.0016541481018066406 sec\nmultilayer_perceptron ran in: 0.0015819072723388672 sec\nmultilayer_perceptron ran in: 0.0016605854034423828 sec\nmultilayer_perceptron ran in: 0.0018684864044189453 sec\nmultilayer_perceptron ran in: 0.0016779899597167969 sec\nmultilayer_perceptron ran in: 0.001621246337890625 sec\nmultilayer_perceptron ran in: 0.0015635490417480469 sec\nmultilayer_perceptron ran in: 0.0015101432800292969 sec\nmultilayer_perceptron ran in: 0.0014984607696533203 sec\nmultilayer_perceptron ran in: 0.0015575885772705078 sec\nmultilayer_perceptron ran in: 0.0016491413116455078 sec\nmultilayer_perceptron ran in: 0.0015017986297607422 sec\nmultilayer_perceptron ran in: 0.0015285015106201172 sec\nmultilayer_perceptron ran in: 0.00152587890625 sec\nmultilayer_perceptron ran in: 0.0015387535095214844 sec\nmultilayer_perceptron ran in: 0.0015804767608642578 sec\nmultilayer_perceptron ran in: 0.001497030258178711 sec\nmultilayer_perceptron ran in: 0.0015380382537841797 sec\nmultilayer_perceptron ran in: 0.002184152603149414 sec\nmultilayer_perceptron ran in: 0.0017287731170654297 sec\nmultilayer_perceptron ran in: 0.0015201568603515625 sec\nmultilayer_perceptron ran in: 0.0015408992767333984 sec\nmultilayer_perceptron ran in: 0.0015892982482910156 sec\nmultilayer_perceptron ran in: 0.0015664100646972656 sec\nmultilayer_perceptron ran in: 0.0015003681182861328 sec\nmultilayer_perceptron ran in: 0.002310037612915039 sec\nmultilayer_perceptron ran in: 0.0018548965454101562 sec\nmultilayer_perceptron ran in: 0.0015327930450439453 sec\nmultilayer_perceptron ran in: 0.001481771469116211 sec\nmultilayer_perceptron ran in: 0.0015282630920410156 sec\nmultilayer_perceptron ran in: 0.0015137195587158203 sec\nmultilayer_perceptron ran in: 0.0015096664428710938 sec\nmultilayer_perceptron ran in: 0.0014891624450683594 sec\nmultilayer_perceptron ran in: 0.0015516281127929688 sec\nmultilayer_perceptron ran in: 0.0016908645629882812 sec\nmultilayer_perceptron ran in: 0.0021970272064208984 sec\nmultilayer_perceptron ran in: 0.002403736114501953 sec\nmultilayer_perceptron ran in: 0.0016493797302246094 sec\nmultilayer_perceptron ran in: 0.0016949176788330078 sec\nmultilayer_perceptron ran in: 0.0017213821411132812 sec\nmultilayer_perceptron ran in: 0.001992464065551758 sec\nmultilayer_perceptron ran in: 0.0015277862548828125 sec\nmultilayer_perceptron ran in: 0.0015189647674560547 sec\nmultilayer_perceptron ran in: 0.0014865398406982422 sec\nmultilayer_perceptron ran in: 0.0017910003662109375 sec\nmultilayer_perceptron ran in: 0.0014698505401611328 sec\nmultilayer_perceptron ran in: 0.0014772415161132812 sec\nmultilayer_perceptron ran in: 0.0015408992767333984 sec\nmultilayer_perceptron ran in: 0.0017440319061279297 sec\nmultilayer_perceptron ran in: 0.002080678939819336 sec\nmultilayer_perceptron ran in: 0.0014979839324951172 sec\nmultilayer_perceptron ran in: 0.0014808177947998047 sec\nmultilayer_perceptron ran in: 0.0014948844909667969 sec\nmultilayer_perceptron ran in: 0.0016505718231201172 sec\nmultilayer_perceptron ran in: 0.001434326171875 sec\nmultilayer_perceptron ran in: 0.00159454345703125 sec\nmultilayer_perceptron ran in: 0.0014748573303222656 sec\nmultilayer_perceptron ran in: 0.002471446990966797 sec\nmultilayer_perceptron ran in: 0.0025861263275146484 sec\nmultilayer_perceptron ran in: 0.0017299652099609375 sec\nmultilayer_perceptron ran in: 0.0013530254364013672 sec\nmultilayer_perceptron ran in: 0.001506805419921875 sec\nmultilayer_perceptron ran in: 0.0014176368713378906 sec\nmultilayer_perceptron ran in: 0.0017600059509277344 sec\nmultilayer_perceptron ran in: 0.0015552043914794922 sec\nmultilayer_perceptron ran in: 0.0019104480743408203 sec\nmultilayer_perceptron ran in: 0.0014951229095458984 sec\nmultilayer_perceptron ran in: 0.0014836788177490234 sec\nmultilayer_perceptron ran in: 0.001556396484375 sec\nmultilayer_perceptron ran in: 0.0015187263488769531 sec\nmultilayer_perceptron ran in: 0.0025157928466796875 sec\nmultilayer_perceptron ran in: 0.0016443729400634766 sec\nmultilayer_perceptron ran in: 0.0016808509826660156 sec\nmultilayer_perceptron ran in: 0.0016734600067138672 sec\nmultilayer_perceptron ran in: 0.0018973350524902344 sec\nmultilayer_perceptron ran in: 0.002678394317626953 sec\nmultilayer_perceptron ran in: 0.002492666244506836 sec\nmultilayer_perceptron ran in: 0.002264261245727539 sec\nmultilayer_perceptron ran in: 0.002545595169067383 sec\nmultilayer_perceptron ran in: 0.002306699752807617 sec\nmultilayer_perceptron ran in: 0.0022339820861816406 sec\nmultilayer_perceptron ran in: 0.0015430450439453125 sec\nmultilayer_perceptron ran in: 0.0015833377838134766 sec\nmultilayer_perceptron ran in: 0.0014917850494384766 sec\nmultilayer_perceptron ran in: 0.0015552043914794922 sec\nmultilayer_perceptron ran in: 0.0015888214111328125 sec\nmultilayer_perceptron ran in: 0.0016188621520996094 sec\nmultilayer_perceptron ran in: 0.0016219615936279297 sec\nmultilayer_perceptron ran in: 0.0014808177947998047 sec\nmultilayer_perceptron ran in: 0.0015218257904052734 sec\nmultilayer_perceptron ran in: 0.0014431476593017578 sec\nmultilayer_perceptron ran in: 0.0014965534210205078 sec\nmultilayer_perceptron ran in: 0.0015919208526611328 sec\nmultilayer_perceptron ran in: 0.0027878284454345703 sec\nmultilayer_perceptron ran in: 0.0014870166778564453 sec\nmultilayer_perceptron ran in: 0.0014874935150146484 sec\nmultilayer_perceptron ran in: 0.003116130828857422 sec\nmultilayer_perceptron ran in: 0.0014929771423339844 sec\nmultilayer_perceptron ran in: 0.0015363693237304688 sec\nmultilayer_perceptron ran in: 0.0014660358428955078 sec\nmultilayer_perceptron ran in: 0.001470804214477539 sec\nmultilayer_perceptron ran in: 0.001478433609008789 sec\nmultilayer_perceptron ran in: 0.0017588138580322266 sec\nmultilayer_perceptron ran in: 0.0014941692352294922 sec\nmultilayer_perceptron ran in: 0.0015270709991455078 sec\nmultilayer_perceptron ran in: 0.0030341148376464844 sec\nmultilayer_perceptron ran in: 0.0015780925750732422 sec\nmultilayer_perceptron ran in: 0.0015690326690673828 sec\nmultilayer_perceptron ran in: 0.0015594959259033203 sec\nmultilayer_perceptron ran in: 0.0015287399291992188 sec\nmultilayer_perceptron ran in: 0.001850128173828125 sec\nmultilayer_perceptron ran in: 0.0016574859619140625 sec\nmultilayer_perceptron ran in: 0.0016949176788330078 sec\nmultilayer_perceptron ran in: 0.0016751289367675781 sec\nmultilayer_perceptron ran in: 0.0015645027160644531 sec\nmultilayer_perceptron ran in: 0.0017845630645751953 sec\nmultilayer_perceptron ran in: 0.001619100570678711 sec\nmultilayer_perceptron ran in: 0.001569509506225586 sec\nmultilayer_perceptron ran in: 0.001588582992553711 sec\nmultilayer_perceptron ran in: 0.0015282630920410156 sec\nmultilayer_perceptron ran in: 0.0031423568725585938 sec\nmultilayer_perceptron ran in: 0.0015935897827148438 sec\nmultilayer_perceptron ran in: 0.0021736621856689453 sec\nmultilayer_perceptron ran in: 0.0015957355499267578 sec\nmultilayer_perceptron ran in: 0.0015985965728759766 sec\nmultilayer_perceptron ran in: 0.0015554428100585938 sec\nmultilayer_perceptron ran in: 0.0016095638275146484 sec\nmultilayer_perceptron ran in: 0.0015747547149658203 sec\nmultilayer_perceptron ran in: 0.0016028881072998047 sec\nmultilayer_perceptron ran in: 0.0015156269073486328 sec\nmultilayer_perceptron ran in: 0.0014836788177490234 sec\nmultilayer_perceptron ran in: 0.001695871353149414 sec\nmultilayer_perceptron ran in: 0.001512765884399414 sec\nmultilayer_perceptron ran in: 0.0015854835510253906 sec\nmultilayer_perceptron ran in: 0.00160980224609375 sec\nmultilayer_perceptron ran in: 0.0016705989837646484 sec\nmultilayer_perceptron ran in: 0.0015156269073486328 sec\nmultilayer_perceptron ran in: 0.0020134449005126953 sec\nmultilayer_perceptron ran in: 0.0016093254089355469 sec\nmultilayer_perceptron ran in: 0.0015072822570800781 sec\nmultilayer_perceptron ran in: 0.0015981197357177734 sec\nmultilayer_perceptron ran in: 0.001669168472290039 sec\nmultilayer_perceptron ran in: 0.0016715526580810547 sec\nmultilayer_perceptron ran in: 0.0016262531280517578 sec\nmultilayer_perceptron ran in: 0.0015704631805419922 sec\nmultilayer_perceptron ran in: 0.002616405487060547 sec\nmultilayer_perceptron ran in: 0.0015709400177001953 sec\nmultilayer_perceptron ran in: 0.0017933845520019531 sec\nmultilayer_perceptron ran in: 0.0017840862274169922 sec\nmultilayer_perceptron ran in: 0.0015513896942138672 sec\nmultilayer_perceptron ran in: 0.0018322467803955078 sec\nmultilayer_perceptron ran in: 0.002376079559326172 sec\nmultilayer_perceptron ran in: 0.0025424957275390625 sec\nmultilayer_perceptron ran in: 0.002117156982421875 sec\nmultilayer_perceptron ran in: 0.00159454345703125 sec\nmultilayer_perceptron ran in: 0.0021462440490722656 sec\nmultilayer_perceptron ran in: 0.0015780925750732422 sec\nmultilayer_perceptron ran in: 0.0021462440490722656 sec\nmultilayer_perceptron ran in: 0.002121448516845703 sec\nmultilayer_perceptron ran in: 0.002008676528930664 sec\nmultilayer_perceptron ran in: 0.0015590190887451172 sec\nmultilayer_perceptron ran in: 0.0018310546875 sec\nmultilayer_perceptron ran in: 0.0015316009521484375 sec\nmultilayer_perceptron ran in: 0.002088785171508789 sec\nmultilayer_perceptron ran in: 0.0016906261444091797 sec\nmultilayer_perceptron ran in: 0.0022017955780029297 sec\nmultilayer_perceptron ran in: 0.002443552017211914 sec\nmultilayer_perceptron ran in: 0.01139688491821289 sec\nmultilayer_perceptron ran in: 0.0016376972198486328 sec\nmultilayer_perceptron ran in: 0.002105712890625 sec\nmultilayer_perceptron ran in: 0.0016436576843261719 sec\nmultilayer_perceptron ran in: 0.0017142295837402344 sec\nmultilayer_perceptron ran in: 0.0016868114471435547 sec\nmultilayer_perceptron ran in: 0.0018181800842285156 sec\nmultilayer_perceptron ran in: 0.0017368793487548828 sec\nmultilayer_perceptron ran in: 0.0017631053924560547 sec\nmultilayer_perceptron ran in: 0.0018088817596435547 sec\nmultilayer_perceptron ran in: 0.001703023910522461 sec\nmultilayer_perceptron ran in: 0.00191497802734375 sec\nmultilayer_perceptron ran in: 0.0018491744995117188 sec\nmultilayer_perceptron ran in: 0.0017137527465820312 sec\nmultilayer_perceptron ran in: 0.002103567123413086 sec\nmultilayer_perceptron ran in: 0.0016546249389648438 sec\nmultilayer_perceptron ran in: 0.0016956329345703125 sec\nmultilayer_perceptron ran in: 0.0016627311706542969 sec\nmultilayer_perceptron ran in: 0.002076387405395508 sec\nmultilayer_perceptron ran in: 0.0015516281127929688 sec\nmultilayer_perceptron ran in: 0.0015769004821777344 sec\nmultilayer_perceptron ran in: 0.0016107559204101562 sec\nmultilayer_perceptron ran in: 0.0021409988403320312 sec\nmultilayer_perceptron ran in: 0.0016279220581054688 sec\nmultilayer_perceptron ran in: 0.0016379356384277344 sec\nmultilayer_perceptron ran in: 0.0015611648559570312 sec\nmultilayer_perceptron ran in: 0.0015752315521240234 sec\nmultilayer_perceptron ran in: 0.0020029544830322266 sec\nmultilayer_perceptron ran in: 0.0018210411071777344 sec\nmultilayer_perceptron ran in: 0.0015070438385009766 sec\nmultilayer_perceptron ran in: 0.0015914440155029297 sec\nmultilayer_perceptron ran in: 0.023989200592041016 sec\nmultilayer_perceptron ran in: 0.0016105175018310547 sec\nmultilayer_perceptron ran in: 0.0014996528625488281 sec\nmultilayer_perceptron ran in: 0.0015463829040527344 sec\nmultilayer_perceptron ran in: 0.0015871524810791016 sec\nmultilayer_perceptron ran in: 0.0015861988067626953 sec\nmultilayer_perceptron ran in: 0.0015664100646972656 sec\nmultilayer_perceptron ran in: 0.0016019344329833984 sec\nmultilayer_perceptron ran in: 0.0015425682067871094 sec\nmultilayer_perceptron ran in: 0.0021729469299316406 sec\nmultilayer_perceptron ran in: 0.0015273094177246094 sec\nmultilayer_perceptron ran in: 0.0015192031860351562 sec\nmultilayer_perceptron ran in: 0.001567840576171875 sec\nmultilayer_perceptron ran in: 0.0014858245849609375 sec\nmultilayer_perceptron ran in: 0.0014946460723876953 sec\nmultilayer_perceptron ran in: 0.0015099048614501953 sec\nmultilayer_perceptron ran in: 0.0016183853149414062 sec\nmultilayer_perceptron ran in: 0.0014896392822265625 sec\nmultilayer_perceptron ran in: 0.0014910697937011719 sec\nmultilayer_perceptron ran in: 0.0014750957489013672 sec\nmultilayer_perceptron ran in: 0.0017244815826416016 sec\nmultilayer_perceptron ran in: 0.0014815330505371094 sec\nmultilayer_perceptron ran in: 0.001508474349975586 sec\nmultilayer_perceptron ran in: 0.0015625953674316406 sec\nmultilayer_perceptron ran in: 0.0015420913696289062 sec\nmultilayer_perceptron ran in: 0.0014371871948242188 sec\nmultilayer_perceptron ran in: 0.0015337467193603516 sec\nmultilayer_perceptron ran in: 0.0015726089477539062 sec\nmultilayer_perceptron ran in: 0.0017163753509521484 sec\nmultilayer_perceptron ran in: 0.0016875267028808594 sec\nmultilayer_perceptron ran in: 0.0015840530395507812 sec\nmultilayer_perceptron ran in: 0.0015685558319091797 sec\nmultilayer_perceptron ran in: 0.0015742778778076172 sec\nmultilayer_perceptron ran in: 0.0015187263488769531 sec\nmultilayer_perceptron ran in: 0.0016102790832519531 sec\nmultilayer_perceptron ran in: 0.0017838478088378906 sec\nmultilayer_perceptron ran in: 0.0015952587127685547 sec\nmultilayer_perceptron ran in: 0.0014879703521728516 sec\nmultilayer_perceptron ran in: 0.0018873214721679688 sec\nmultilayer_perceptron ran in: 0.0014495849609375 sec\nmultilayer_perceptron ran in: 0.0014963150024414062 sec\nmultilayer_perceptron ran in: 0.001535177230834961 sec\nmultilayer_perceptron ran in: 0.001585245132446289 sec\nmultilayer_perceptron ran in: 0.0015006065368652344 sec\nmultilayer_perceptron ran in: 0.0015058517456054688 sec\nmultilayer_perceptron ran in: 0.0016415119171142578 sec\nmultilayer_perceptron ran in: 0.001573801040649414 sec\nmultilayer_perceptron ran in: 0.0015289783477783203 sec\nmultilayer_perceptron ran in: 0.0015447139739990234 sec\nmultilayer_perceptron ran in: 0.0016262531280517578 sec\nmultilayer_perceptron ran in: 0.0015709400177001953 sec\nmultilayer_perceptron ran in: 0.0016002655029296875 sec\nmultilayer_perceptron ran in: 0.001505136489868164 sec\nmultilayer_perceptron ran in: 0.0016062259674072266 sec\nmultilayer_perceptron ran in: 0.0015385150909423828 sec\nmultilayer_perceptron ran in: 0.0015888214111328125 sec\nmultilayer_perceptron ran in: 0.0015573501586914062 sec\nmultilayer_perceptron ran in: 0.0015497207641601562 sec\nmultilayer_perceptron ran in: 0.0015025138854980469 sec\nmultilayer_perceptron ran in: 0.0015056133270263672 sec\nmultilayer_perceptron ran in: 0.0017230510711669922 sec\nmultilayer_perceptron ran in: 0.0015139579772949219 sec\nmultilayer_perceptron ran in: 0.0015497207641601562 sec\nmultilayer_perceptron ran in: 0.0016453266143798828 sec\nmultilayer_perceptron ran in: 0.0016024112701416016 sec\nmultilayer_perceptron ran in: 0.0027599334716796875 sec\nmultilayer_perceptron ran in: 0.0020546913146972656 sec\nmultilayer_perceptron ran in: 0.0016415119171142578 sec\nmultilayer_perceptron ran in: 0.0017511844635009766 sec\nmultilayer_perceptron ran in: 0.0016319751739501953 sec\nmultilayer_perceptron ran in: 0.001874685287475586 sec\nmultilayer_perceptron ran in: 0.0022330284118652344 sec\nmultilayer_perceptron ran in: 0.0017211437225341797 sec\nmultilayer_perceptron ran in: 0.0014815330505371094 sec\nmultilayer_perceptron ran in: 0.0017173290252685547 sec\nmultilayer_perceptron ran in: 0.0015933513641357422 sec\nmultilayer_perceptron ran in: 0.0018169879913330078 sec\nmultilayer_perceptron ran in: 0.0017194747924804688 sec\nmultilayer_perceptron ran in: 0.0016243457794189453 sec\nmultilayer_perceptron ran in: 0.0016222000122070312 sec\nmultilayer_perceptron ran in: 0.0015361309051513672 sec\nmultilayer_perceptron ran in: 0.001714468002319336 sec\nmultilayer_perceptron ran in: 0.0015940666198730469 sec\nmultilayer_perceptron ran in: 0.0017023086547851562 sec\nmultilayer_perceptron ran in: 0.0015053749084472656 sec\nmultilayer_perceptron ran in: 0.0017995834350585938 sec\nmultilayer_perceptron ran in: 0.0016732215881347656 sec\nEpoch: 2 Cost=5177687.0000\nModellierung ist beendet: 2 Epochs of Training\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Modell Auswertung\n\nTensorflow bietet einige eingebaute Funktionen, die uns bei der Auswertung helfen. Dazu gehören `tf.equal` und `tf.reduce_mean`.\n\n\n### tf.equal\n\nDies ist im Grunde genommen nur eine Kontrolle, ob die Vorhersagen mit den Labels übereinstimmen. Da wir in unserem Fall wissen, dass die Labels eine 1 in einem Array von Nullen sind, können wir `argmax()` verwenden, um die Position zu vergleichen. Denke daran, dass y immer noch der Platzhalter ist, den wir anfangs erstellt haben. Wir werden eine Reihe an Operationen durchführen, um einen Tensor zu erhalten, in den wir die Testdaten einlesen können, um es auszuwerten.","metadata":{}},{"cell_type":"code","source":"# Teste das Modell\ncorrect_predictions = tf.math.reduce_all(tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)))","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(correct_predictions)","metadata":{"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.bool, name=None), name='tf.math.reduce_all/All:0', description=\"created by layer 'tf.math.reduce_all'\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Um numerische Werte für unsere Vorhersagen zu erhalten müssen wir `tf.cast` verwenden, um den Tensor mit Booleans zurückzuführen in einen Tensor mit Floats. Dann können wir den Durchschnitt nehmen.","metadata":{}},{"cell_type":"code","source":"correct_predictions = tf.cast(correct_predictions, \"float\")","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(correct_predictions)","metadata":{"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.cast/Cast:0', description=\"created by layer 'tf.cast'\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Jetzt können wir `tf.reduce_mean` verwenden, um den Durchschnitt der Elemente im Tensor zu erhalten:","metadata":{}},{"cell_type":"code","source":"accuracy = tf.reduce_mean(correct_predictions)","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"type(accuracy)","metadata":{"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"keras.engine.keras_tensor.KerasTensor"},"metadata":{}}]},{"cell_type":"markdown","source":"Das wirkt evtl. etwas merkwürdig, aber diese Genauigkeit ist immer noch ein Tensor Objekt. Denke daran, dass wir immer noch die tatsächlichen Testdaten übergeben müssen. Jetzt können wir die MNIST Testlabels und Bilder aufrufen und die Genauigkeit auswerten!","metadata":{}},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"array([7, 3, 8, ..., 9, 7, 2], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"x_test","metadata":{"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Die `eval()` Methode erlaubt es uns direkt in der Session den Tensor auszuwerten ohne `tf.sess():mm` aufrufen zu müssen","metadata":{}},{"cell_type":"code","source":"# Überprüfen, ob eine GPU verfügbar ist\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    print(\"GPU gefunden. Aktiviere GPU-Unterstützung.\")\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\nelse:\n    print(\"Keine GPU gefunden. Verwende die CPU.\")\n\n# Erstellen des Modells\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(n_input,)),  \n    tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.relu),\n    tf.keras.layers.Dense(n_hidden_2, activation=tf.nn.relu),\n    tf.keras.layers.Dense(n_classes, activation=None)\n])\n\n# Kompilieren des Modells mit der Genauigkeitsmetrik\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n)\n\n# Trainieren des Modells auf den Trainingsdaten in Batches mit einer erhöhten Batch-Größe\nx_train_flattened = x_train.reshape(-1, n_input)\nbatch_size = 128  # Erhöhen Sie die Batch-Größe, um die GPU besser auszulasten\nmodel.fit(x_train_flattened, y_train, batch_size=batch_size, epochs=training_epochs)\n\n# Evaluieren der Genauigkeit auf den Testdaten (nur am Ende)\nx_test_flattened = x_test.reshape(-1, n_input)\ntest_loss, test_accuracy = model.evaluate(x_test_flattened, y_test)\n\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Keine GPU gefunden. Verwende die CPU.\nEpoch 1/2\n375/375 [==============================] - 11s 27ms/step - loss: 0.2934 - sparse_categorical_accuracy: 0.9156\nEpoch 2/2\n375/375 [==============================] - 11s 29ms/step - loss: 0.1136 - sparse_categorical_accuracy: 0.9666\n375/375 [==============================] - 3s 8ms/step - loss: 0.1063 - sparse_categorical_accuracy: 0.9676\nTest Loss: 0.10626482218503952\nTest Accuracy: 0.9675833582878113\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialisieren des Unittest","metadata":{}},{"cell_type":"markdown","source":"Hier startet der Unittest, zu anfang werden die Datenausgelesen und anschließend die Train Accuracy und Test Accurracy ausgegeben.","metadata":{}},{"cell_type":"code","source":"# Verwenden Sie bereits aufgeteilte Daten\nsplitRatio = 0.8  # Das ursprüngliche Split-Verhältnis\nnum_samples = len(x_train)\n\nX = x_train[:int(splitRatio * num_samples)]\ny = train_labels[:int(splitRatio * num_samples)]\n\n# Keine Notwendigkeit für die erneute Zufallstestgröße, da Sie bereits eine festgelegte Größe verwendet haben\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n\nnp.random.seed(31337)\n\nta = TheAlgorithm(X_train, y_train, X_test, y_test)\n\ntrain_accuracy = ta.fit()\n\nprint()\n\nprint('Train Accuracy:', train_accuracy, '\\n')\n\nprint(\"Train confusion matrix:\\n%s\\n\" % ta.train_confusion_matrix)\n\ntest_accuracy = ta.predict()\n\nprint()\n\nprint('Test Accuracy:', test_accuracy, '\\n')\n\nprint(\"Test confusion matrix:\\n%s\\n\" % ta.test_confusion_matrix)","metadata":{"tags":[],"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"__init__ ran in: 4.5299530029296875e-06 sec\nfit ran in: 12.161783695220947 sec\n\nTrain Accuracy: 9.7265625 \n\nTrain confusion matrix:\n[[   0    0    0    0    0    0    0    0 2984    0]\n [   0    0    0    0    0    0    0    0 3436    0]\n [   0    0    0    0    0    0    0    0 3036    0]\n [   0    0    0    0    0    0    0    0 3160    0]\n [   0    0    0    0    0    0    0    0 3041    0]\n [   0    0    0    0    0    0    0    0 2765    0]\n [   0    0    0    0    0    0    0    0 3052    0]\n [   0    0    0    0    0    0    0    0 3172    0]\n [   0    0    0    0    0    0    0    0 2988    0]\n [   0    0    0    0    0    0    0    0 3086    0]]\n\nClassification Report for the classifier:\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       776\n           1       0.00      0.00      0.00       928\n           2       0.00      0.00      0.00       751\n           3       0.00      0.00      0.00       779\n           4       0.00      0.00      0.00       710\n           5       0.00      0.00      0.00       688\n           6       0.00      0.00      0.00       761\n           7       0.00      0.00      0.00       794\n           8       0.09      1.00      0.17       716\n           9       0.00      0.00      0.00       777\n\n    accuracy                           0.09      7680\n   macro avg       0.01      0.10      0.02      7680\nweighted avg       0.01      0.09      0.02      7680\n\n\npredict ran in: 0.33014488220214844 sec\n\nTest Accuracy: 9.322916666666666 \n\nTest confusion matrix:\n[[  0   0   0   0   0   0   0   0 776   0]\n [  0   0   0   0   0   0   0   0 928   0]\n [  0   0   0   0   0   0   0   0 751   0]\n [  0   0   0   0   0   0   0   0 779   0]\n [  0   0   0   0   0   0   0   0 710   0]\n [  0   0   0   0   0   0   0   0 688   0]\n [  0   0   0   0   0   0   0   0 761   0]\n [  0   0   0   0   0   0   0   0 794   0]\n [  0   0   0   0   0   0   0   0 716   0]\n [  0   0   0   0   0   0   0   0 777   0]]\n\n","output_type":"stream"},{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/srv/conda/envs/notebook/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/srv/conda/envs/notebook/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Durchführung der Unittests ausgeführt","metadata":{}},{"cell_type":"markdown","source":"Test des Ausfalls der Laufzeit von test_fit mit 0,01% der representativen Zeit","metadata":{}},{"cell_type":"code","source":"class TestInput(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        print('setUp')\n        # Verwende die Werte und Objekte aus Code 34\n        X, y = download()\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        # Setzen Sie die erwarteten Genauigkeiten basierend auf Ihren tatsächlichen Daten und Modellparametern\n        self.train_accuracy = train_accuracy  # Beispielwert, aktualisieren mit tatsächlichen Trainingsgenauigkeitswert\n        self.test_accuracy = test_accuracy  # Beispielwert, aktualisieren mit tatsächlichen Testgenauigkeitswert\n        # Ersetzen der ta_train_accuracy und ta_test_accuracy durch die tatsächlichen Genauigkeitswerte\n        self.train_confusion_matrix = ta.train_confusion_matrix\n        self.test_confusion_matrix = ta.test_confusion_matrix\n\n    def tearDown(self):\n        pass\n\n    def test_fit(self):\n        np.random.seed(31337)\n        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n        self.assertEqual(self.ta.fit(), self.train_accuracy)\n        self.assertTrue(np.array_equal(self.ta.train_confusion_matrix, self.train_confusion_matrix))\n\n    def test_predict(self):\n        np.random.seed(31337)\n        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n        self.ta.fit()\n        self.assertEqual(self.ta.predict(), self.test_accuracy)\n        self.assertTrue(np.array_equal(self.ta.test_confusion_matrix, self.test_confusion_matrix))\n        \n    def test_runtime_fit(self):\n        np.random.seed(31337)\n        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n\n        # Messen der Laufzeit der fit-Funktion\n        start_time = time.time()\n        self.ta.fit()\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n\n        # Legen Sie den Grenzwert fest, z.B. 120% der repräsentativen Laufzeit\n        representative_runtime = elapsed_time  # Aktualisieren Sie diesen Wert entsprechend\n        max_allowed_runtime = 0.0001 * representative_runtime\n\n        # Überprüfen, ob die Laufzeit innerhalb des zulässigen Bereichs liegt\n        self.assertLessEqual(elapsed_time, max_allowed_runtime)\n\nif __name__ == '__main__':\n    unittest.main(argv=['first-arg-is-ignored'], exit=False)","metadata":{"tags":[],"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"EEE\n======================================================================\nERROR: test_fit (__main__.TestInput)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1844/407630555.py\", line 13, in setUp\n    X, y = download()\nNameError: name 'download' is not defined\n\n======================================================================\nERROR: test_predict (__main__.TestInput)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1844/407630555.py\", line 13, in setUp\n    X, y = download()\nNameError: name 'download' is not defined\n\n======================================================================\nERROR: test_runtime_fit (__main__.TestInput)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1844/407630555.py\", line 13, in setUp\n    X, y = download()\nNameError: name 'download' is not defined\n\n----------------------------------------------------------------------\nRan 3 tests in 0.057s\n\nFAILED (errors=3)\n","output_type":"stream"},{"name":"stdout","text":"setUp\nsetUp\nsetUp\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Test des Durchlaufs der Laufzeit von test_fit mit 120% der representativen Zeit","metadata":{}},{"cell_type":"code","source":"class TestInput(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        pass\n\n    @classmethod\n    def tearDownClass(cls):\n        pass\n\n    def setUp(self):\n        print('setUp')\n        # Verwende die Werte und Objekte aus Code 34\n        X, y = download()\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        # Setzen Sie die erwarteten Genauigkeiten basierend auf Ihren tatsächlichen Daten und Modellparametern\n        self.train_accuracy = train_accuracy  # Beispielwert, aktualisieren mit tatsächlichen Trainingsgenauigkeitswert\n        self.test_accuracy = test_accuracy  # Beispielwert, aktualisieren mit tatsächlichen Testgenauigkeitswert\n        # Ersetzen der ta_train_accuracy und ta_test_accuracy durch die tatsächlichen Genauigkeitswerte\n        self.train_confusion_matrix = ta.train_confusion_matrix\n        self.test_confusion_matrix = ta.test_confusion_matrix\n\n    def tearDown(self):\n        pass\n\n    def test_fit(self):\n        np.random.seed(31337)\n        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n        self.assertEqual(self.ta.fit(), self.train_accuracy)\n        self.assertTrue(np.array_equal(self.ta.train_confusion_matrix, self.train_confusion_matrix))\n\n    def test_predict(self):\n        np.random.seed(31337)\n        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n        self.ta.fit()\n        self.assertEqual(self.ta.predict(), self.test_accuracy)\n        self.assertTrue(np.array_equal(self.ta.test_confusion_matrix, self.test_confusion_matrix))\n        \n    def test_runtime_fit(self):\n        np.random.seed(31337)\n        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n\n        # Messen der Laufzeit der fit-Funktion\n        start_time = time.time()\n        self.ta.fit()\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n\n        # Legen Sie den Grenzwert fest, z.B. 120% der repräsentativen Laufzeit\n        representative_runtime = elapsed_time  # Aktualisieren Sie diesen Wert entsprechend\n        max_allowed_runtime = 1.2 * representative_runtime\n\n        # Überprüfen, ob die Laufzeit innerhalb des zulässigen Bereichs liegt\n        self.assertLessEqual(elapsed_time, max_allowed_runtime)\n\nif __name__ == '__main__':\n    unittest.main(argv=['first-arg-is-ignored'], exit=False)","metadata":{"tags":[],"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"EEE\n======================================================================\nERROR: test_fit (__main__.TestInput)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1844/3136829347.py\", line 13, in setUp\n    X, y = download()\nNameError: name 'download' is not defined\n\n======================================================================\nERROR: test_predict (__main__.TestInput)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1844/3136829347.py\", line 13, in setUp\n    X, y = download()\nNameError: name 'download' is not defined\n\n======================================================================\nERROR: test_runtime_fit (__main__.TestInput)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1844/3136829347.py\", line 13, in setUp\n    X, y = download()\nNameError: name 'download' is not defined\n\n----------------------------------------------------------------------\nRan 3 tests in 0.063s\n\nFAILED (errors=3)\n","output_type":"stream"},{"name":"stdout","text":"setUp\nsetUp\nsetUp\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n# Gut gemacht!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}