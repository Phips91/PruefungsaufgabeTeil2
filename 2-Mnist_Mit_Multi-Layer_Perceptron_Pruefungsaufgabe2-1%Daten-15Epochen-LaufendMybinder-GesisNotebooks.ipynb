{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://akademie.datamics.com/kursliste/\">![title](bg_datamics_top.png)</a>\n",
    "\n",
    "<center><em>© Datamics</em></center><br><center><em>Besuche uns für mehr Informationen auf <a href='https://akademie.datamics.com/kursliste/'>www.akademie.datamics.com</a></em></center>\n",
    "\n",
    "# MNIST mit Multi-Layer Perceptron\n",
    "\n",
    "In dieser Lektion werden wir ein Multi Layer Perceptron Modell erstellen und versuchen damit handgeschriebenen Zahlen zu klassifizieren. Das ist ein sehr verbreitetes Einsteigerproblem für Tensorflow.\n",
    "\n",
    "Denkt daran, dass eine einzige Lektion niemals ausreichen wird, um Deep Learning und/oder Tensorflow in seiner Komlexität abzudecken!\n",
    "\n",
    "## Die Daten laden\n",
    "\n",
    "Wir werden die berühmten MNIST Daten über [handgeschriebenen Zahlen](http://yann.lecun.com/exdb/mnist/) verwenden.\n",
    "\n",
    "Die Bilder die wir verwenden werden sind schwarz-weiß Bilder der größe 28 x 28, d.h. 784 Pixel insgesamt. Unsere Features werden die Pixelwerte für jeden Pixel sein. Entweder ist der Pixel \"weiß\" (also eine 0 in den Daten) oder er hat einen Pixelwert.\n",
    "\n",
    "Wir werden versuchen korrekt vorherzusagen, welche Nummer geschrieben steht. Dazu verwenden wir lediglich die Bilddaten in Form unseres Arrays. Diese Art von Problem (Image Recognition oder auf Deutsch: Bilderkennung) ist ein tolle Use Case für Deep Learning Methoden!\n",
    "\n",
    "Die Daten sind für Deep Learning das, was der Iris Datensatz für typische Machine Learning Algorithmen ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import von Bibliotheken in Python.\n",
    "\n",
    "In diesem Code werden die Python-Bibliotheken TensorFlow, NumPy, Logging, Time, Matplotlib und Unittest importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 15:46:14.099791: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-31 15:46:14.250226: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-31 15:46:14.252375: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 15:46:16.507255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import unittest\n",
    "import psutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der Teildateien\n",
    "num_parts = 3\n",
    "\n",
    "# Wiederherstellen der ursprünglichen Datei\n",
    "output_file = \"train-images.idx3-ubyte\"\n",
    "with open(output_file, 'wb') as restored_file:\n",
    "    for i in range(num_parts):\n",
    "        part_file = f\"part_{i+1}.zip\"\n",
    "        with zipfile.ZipFile(part_file, 'r') as part_zip:\n",
    "            data = part_zip.read(\"data\")\n",
    "            restored_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warte 15 Sekunden, bevor der nächste Codeabschnitt ausgeführt wird\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "\n",
    "def max_ram_usage():\n",
    "    max_memory = 0\n",
    "\n",
    "    for interval in range(1, 11):\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        max_memory = max(max_memory, memory_info.used)\n",
    "        time.sleep(1)\n",
    "\n",
    "    return max_memory / (1024 * 1024)  # In MB umrechnen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Code importiert verschiedene Python-Bibliotheken, lädt den MNIST-Datensatz, normalisiert die Daten, teilt sie in Trainings- und Testdaten auf und konvertiert die Trainingsdaten in das gewünschte Datenformat für spätere Verwendung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Pfad zu den heruntergeladenen MNIST-Daten\n",
    "train_data_path = 'train-images.idx3-ubyte'\n",
    "test_data_path = 't10k-images.idx3-ubyte'\n",
    "train_labels_path = 'train-labels.idx1-ubyte'  # Hinzugefügt\n",
    "test_labels_path = 't10k-labels.idx1-ubyte'  # Hinzugefügt\n",
    "\n",
    "# Laden der MNIST-Daten aus den lokal gespeicherten Dateien\n",
    "def load_mnist_data(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        magic, num_images, num_rows, num_cols = struct.unpack('>IIII', f.read(16))\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, num_rows, num_cols)\n",
    "    return images\n",
    "\n",
    "# Laden der MNIST-Label aus den lokal gespeicherten Dateien (Hinzugefügt)\n",
    "def load_mnist_labels(labels_path):\n",
    "    with open(labels_path, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "# Laden Sie die MNIST-Trainings- und Testdaten\n",
    "x_train_mnist = load_mnist_data(train_data_path)\n",
    "x_test_mnist = load_mnist_data(test_data_path)\n",
    "# Laden Sie die MNIST-Label\n",
    "train_labels = load_mnist_labels(train_labels_path)\n",
    "test_labels = load_mnist_labels(test_labels_path)\n",
    "\n",
    "# Normalisieren der Daten\n",
    "class Normalize(object):\n",
    "    def normalize(self, X_train, X_test):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        # Umformen in 2D-Arrays (Flatten)\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        return (X_train, X_test)\n",
    "\n",
    "    def inverse(self, X_train, X_test):\n",
    "        X_train = self.scaler.inverse_transform(X_train)\n",
    "        X_test = self.scaler.inverse_transform(X_test)\n",
    "        return (X_train, X_test)\n",
    "\n",
    "# Aufteilen der Daten\n",
    "def split(X, y, splitRatio):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Annahme: Sie haben bereits die MNIST-Trainings- und Testdaten geladen und in x_train_mnist und x_test_mnist gespeichert.\n",
    "\n",
    "# Normalisieren der Daten\n",
    "normalizer = Normalize()\n",
    "x_train, x_test = normalizer.normalize(x_train_mnist, x_test_mnist)\n",
    "\n",
    "# Aufteilen der Daten\n",
    "splitRatio = 0.8  # Ändern Sie den Split-Verhältnis nach Bedarf\n",
    "x_train, y_train, x_test, y_test = split(x_train, train_labels, splitRatio)\n",
    "\n",
    "# Stellen Sie sicher, dass die Daten korrekt geladen wurden (ersetzen Sie y durch die entsprechenden Label-Daten)\n",
    "assert x_train.shape == (int(0.8 * len(x_train_mnist)), x_train_mnist.shape[1] * x_train_mnist.shape[2])\n",
    "assert x_test.shape == (int(0.2 * len(x_train_mnist)), x_train_mnist.shape[1] * x_train_mnist.shape[2])\n",
    "assert y_train.shape == (int(0.8 * len(x_train_mnist)),)\n",
    "assert y_test.shape == (int(0.2 * len(x_train_mnist)),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren der my_logger und my_timer Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_logger(orig_func):\n",
    "    logging.basicConfig(filename='{}.log'.format(orig_func.__name__), level=logging.INFO)\n",
    "\n",
    "    @wraps(orig_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logging.info(\n",
    "            'Ran with args: {}, and kwargs: {}'.format(args, kwargs))\n",
    "        return orig_func(*args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_timer(orig_func):\n",
    "    import time\n",
    "\n",
    "    @wraps(orig_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = orig_func(*args, **kwargs)\n",
    "        t2 = time.time() - t1\n",
    "        print('{} ran in: {} sec'.format(orig_func.__name__, t2))\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Code wird eine Python-Klasse mit dem Namen \"TheAlgorithm\" definiert, die verschiedene Methoden wie den Konstruktor __init__, die Methode fit und die Methode predict enthält, wobei die Dekoratoren @my_logger und @my_timer verwendet werden, um die Ausführung dieser Methoden zu protokollieren und die Zeitmessung durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheAlgorithm(object):\n",
    "    @my_logger\n",
    "    @my_timer\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = X_train, y_train, X_test, y_test\n",
    "\n",
    "    @my_logger\n",
    "    @my_timer\n",
    "    def fit(self):\n",
    "        x_train, y_train, x_test, y_test = self.X_train, self.y_train, self.X_test, self.y_test\n",
    "\n",
    "        normalizer = Normalize()  # Use the correct class name here\n",
    "        x_train, x_test = normalizer.normalize(x_train, x_test)\n",
    "\n",
    "        train_samples = x_train.shape[0]\n",
    "\n",
    "        self.classifier = LogisticRegression(\n",
    "            C=50. / train_samples,\n",
    "            multi_class='multinomial',\n",
    "            penalty='l1',\n",
    "            solver='saga',\n",
    "            tol=0.1,\n",
    "            class_weight='balanced',\n",
    "        )\n",
    "\n",
    "        self.classifier.fit(x_train, y_train)\n",
    "        self.train_predictions = self.classifier.predict(x_train)\n",
    "        self.train_accuracy = np.mean(self.train_predictions.ravel() == y_train.ravel()) * 100\n",
    "        self.train_confusion_matrix = confusion_matrix(y_train, self.train_predictions)\n",
    "        return self.train_accuracy\n",
    "\n",
    "    @my_logger\n",
    "    @my_timer\n",
    "    def predict(self):\n",
    "        x_test = self.X_test  # Test data doesn't need to be normalized again\n",
    "\n",
    "        self.test_predictions = self.classifier.predict(x_test)\n",
    "        self.test_accuracy = np.mean(self.test_predictions.ravel() == self.y_test.ravel()) * 100\n",
    "        self.test_confusion_matrix = confusion_matrix(self.y_test, self.test_predictions)\n",
    "        self.report = classification_report(self.y_test, self.test_predictions)\n",
    "        print(\"Classification Report for the classifier:\\n%s\\n\" % (self.report))\n",
    "\n",
    "        return self.test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Format\n",
    "\n",
    "Die Daten sind im Vektor Format gespeichert, obwohl die Originaldaten eine 2-dimensionale Matrix waren, die angab, wie viele Pigmente sich an welcher Position befinden. Untersuchen wir das genauer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = x_train[2].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa501828100>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter\n",
    "\n",
    "Wir werden 4 Parameter definieren müssen. Es ist wirklich (wirklich) schwer gute Parameterwerte für einen Datensatz zu bestimmen, mit dem man keine Erfahrung hat. Da dieser MNIST Datensatz allerdings so berühmt ist haben wir schon einige Ausgangswerte. Die Parameter sind:\n",
    "\n",
    "* Learning Rate - Wie schnell die Kostenfunktion angepasst wird\n",
    "* Traing Epochs - Wie viele Trainingszyklen durchlaufen werden sollen\n",
    "* Batch Size - Größe der \"Batches\" an Traingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netzwerk Parameter\n",
    "\n",
    "Hier haben wir Parameter welche unser Neuronales Netz direkt definieren. Diese werden entsprechend der betrachteten Daten angepasst und hängen auch davon ab, welche Art von Netz man nutzt. Es sind bis zu diesem Punkt erst einmal nur Zahlen, die wir später verwenden, um unser Netz zu definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netzwerk Parameter\n",
    "n_hidden_1 = 256  # 1. Layer: Anzahl an Features\n",
    "n_hidden_2 = 256  # 2. Layer: Anzahl an Features\n",
    "n_input = 784     # MNIST Daten Input (img shape: 28*28)\n",
    "n_classes = 10    # MNIST Klassen (0-9 Zahlen)\n",
    "n_samples = len(x_train)  # Anzahl der Trainingsbeispiele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Graph Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.Input(shape=(n_input,), dtype=tf.float32)\n",
    "y = tf.keras.Input(shape=(n_classes,), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Modell\n",
    "\n",
    "Es ist Zeit unser Modell zu erstellen. Wiederholen wir deshalb kurz, was wir erstellen wollen:\n",
    "\n",
    "Zuerst erhalten wir einen *Input* in Form eines Datenarrays und schicken diesen an die erste *Hidden Layer*. Dann wird den Daten ein  *Weight* zwischen den Schichten zugewiesen (welches zuerst ein zufälliger Wert ist). Anschließend wird es an einen *Node* geschicht und unterläuft eine *Activation Function* (zusammen mit einem Bias, wie in der Neural Network Lektion erwähnt). Dann geht es weiter zur nächsten *Layer* und immer so weiter, bis zur finalen *Output Layer*. In unserem Fall werden wir nur 2 *Hidden Layers* verwenden. Je mehr wir davon verwenden, desto länger braucht das Modell (aber er hat mehr Möglichkeiten um die Genauigkeit zu erhöhen).\n",
    "\n",
    "Sobald die transformierte Daten die *Output Layer* erreicht haben müssen wir sie auswerten. Hier verwenden wir eine *Loss Function* (auch Cost Function genannt). Diese berechnet, wie sehr wir vom gewünschten Ergebnis entfernt sind. In diesem Fall: Wie viele der Klassen wir richtig zugeteilt haben.\n",
    "\n",
    "Dann wenden wir eine Optimierungsfunktion an, um die *Costs* (bzw. den Error) zu minimieren. Dies geschiet durch die Anpassung der *Weights* entlang des Netzes. Wir verwenden in unserem Beispiel den [Adam Optimizer](https://arxiv.org/pdf/1412.6980v8.pdf), welcher eine (im Vergleich zu anderen) sehr neue Entwicklung ist.\n",
    "\n",
    "Wir können anpassen, wie schnell diese Optimierung angewendet wird, indem wir unseren *Learning Rate* Parameter anpassen. Je geringer die Rate, desto höher die Möglichkeiten für Anpassungen. Dies erzeugt allerdings die Kosten einer erhöhten Wartezeit. Ab einem bestimmten Punkt lohnt es sich nicht mehr, die Learning Rate weiter zu senken.\n",
    "\n",
    "Jetzt können wir unser Modell erstellen. Wir beginnen mit 2 Hidden Layers, welche die []() Activation Function verwenden. Dies ist eine einfache Umformungsfunktion, die entweder x oder 0 zurückgibt. Für unsere finale Output Layer verwenden wir eine lineare Activation mit Matrixmultiplikation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Code wird eine Funktion namens \"multilayer_perceptron\" definiert, die ein mehrschichtiges neuronales Netzwerk mit ReLU-Aktivierungsfunktionen für die Hidden Layers und linearer Aktivierungsfunktion für die Output Layer erstellt und die Ausgabe des Netzwerks zurückgibt, wobei die Funktionen `my_logger` und `my_timer` als Dekoratoren verwendet werden, um die Ausführung der Funktion zu protokollieren und die Zeitmessung durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@my_logger\n",
    "@my_timer\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    '''\n",
    "    x : Platzhalter für den Dateninput\n",
    "    weights: Dictionary der Weights\n",
    "    biases: Dictionary Der Biases\n",
    "    '''\n",
    "    \n",
    "    # Erste Hidden layer mit RELU Activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Zweite Hidden layer mit RELU Activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Letzte Output layer mit linearer Activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights und Bias\n",
    "\n",
    "Damit unser Tensorflow Modell funktioniert müssen wir zwei Dictionaries anlegen, die unsere Weights und Biases enthalten. Wir können das `tf.variable` Objekt verwenden. Dies ist anders als eine Konstante, da Tensorflow's Graph Objekt alle Zustände der Variablen wahrnimmt. Eine Variable ist ein anpassbares Tensor, der zwischen Tensorflow's Graph von interagierenden Operationen lebt. Er kann durch die Berechnung verwendet und verändert werden. Wir werden die Modell Parameter generell als Variablen verwenden. Aus der Dokumentation können wir entnehmen:\n",
    "\n",
    "    A variable maintains state in the graph across calls to `run()`. You add a variable to the graph by constructing an instance of the class `Variable`.\n",
    "\n",
    "    The `Variable()` constructor requires an initial value for the variable, which can be a `Tensor` of any type and shape. The initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.\n",
    "    \n",
    "Wir werden Tensorflow's eingebaute `random_normal` Methode verwenden, um zufällige Werte für unsere Weights und Biases zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gewichtsinitialisierung\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.initializers.RandomNormal()(shape=[n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.initializers.RandomNormal()(shape=[n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.initializers.RandomNormal()(shape=[n_hidden_2, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-Initialisierung\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(784, 256) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(256,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(256, 256) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.add_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(256,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_2), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(256, 10) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(10,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "multilayer_perceptron ran in: 0.10695409774780273 sec\n"
     ]
    }
   ],
   "source": [
    "# Model erstellen\n",
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost und Optimierungs-Funktion\n",
    "\n",
    "Wir verwenden Tensorflow's eingebaute Funktionen für diesesn Teil. Weitere Details bietet die Dokumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost und Optimierungsfunktion definieren\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisierung der Variablen\n",
    "\n",
    "Wir initialisieren nun alle tf.Variable Objekte die wir zuvor erstellt haben. Das wird das erste sein, dass wir ausführen, wenn wir unser Modell trainieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Modell trainieren\n",
    "\n",
    "### next_batch()\n",
    "\n",
    "Bevor wir beginnen möchte ich eine weitere nützliche Funktion in unserem MNIST Datenobjekt abdecken, die `next_batch` heißt. Diese gibt ein Tupel in der Form (X,y) mit einem X Array der Daten und einem y Array der Klasse. Zum Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 15:46:41.612248: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [48000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-10-31 15:46:41.612881: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [48000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "# Erstelle ein TensorFlow-Dataset aus den Trainingsdaten\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "# Mische die Daten und teile sie in Batches auf\n",
    "batch_size = 1\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "# Erstelle einen Iterator für das Dataset\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "# Greife auf ein Batch von Daten zu\n",
    "Xsamp, ysamp = next(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa4f85fc430>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAazElEQVR4nO3df3DV9b3n8dchJEfQ5NAQk5NIoAEVrEDaUkizKMWSIYl7HUC2A/6YBdcLKw1OgVrddBSk7UxanGu9Wgo7sy2pM4LKjsDqtXQ1mLDWhA4Iw7KtWcLEEi4k1MzlnBAgBPLZP1hPPZCA38M5eeeE52PmO2PO+X7yffvtqU++nJNvfM45JwAA+tkQ6wEAADcmAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwMtR7gcj09PTp+/LjS09Pl8/msxwEAeOScU0dHh/Ly8jRkSN/XOQMuQMePH1d+fr71GACA69TS0qJRo0b1+fyAC1B6erok6R7dr6FKNZ4GAODVBXXrQ70b+e95XxIWoPXr1+uFF15Qa2urCgsL9corr2jatGnXXPf5X7sNVaqG+ggQACSd/3+H0Wu9jZKQDyG88cYbWrVqldasWaOPP/5YhYWFKi0t1cmTJxNxOABAEkpIgF588UUtWbJEjz32mL72ta9p48aNGj58uH77298m4nAAgCQU9wCdP39e+/btU0lJyd8PMmSISkpKVF9ff8X+XV1dCofDURsAYPCLe4A+++wzXbx4UTk5OVGP5+TkqLW19Yr9q6qqFAgEIhufgAOAG4P5D6JWVlYqFApFtpaWFuuRAAD9IO6fgsvKylJKSora2tqiHm9ra1MwGLxif7/fL7/fH+8xAAADXNyvgNLS0jRlyhTV1NREHuvp6VFNTY2Ki4vjfTgAQJJKyM8BrVq1SosWLdK3vvUtTZs2TS+99JI6Ozv12GOPJeJwAIAklJAALViwQH/729+0evVqtba26utf/7p27tx5xQcTAAA3Lp9zzlkP8UXhcFiBQEAzNYc7IQBAErrgulWrHQqFQsrIyOhzP/NPwQEAbkwECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiaHWAwAYHA7/qsj7mnkbPK8p+U9LPa9J+8Nez2uQeFwBAQBMECAAgIm4B+j555+Xz+eL2iZMmBDvwwAAklxC3gO6++679f777//9IEN5qwkAEC0hZRg6dKiCwWAivjUAYJBIyHtAhw8fVl5ensaOHatHHnlER48e7XPfrq4uhcPhqA0AMPjFPUBFRUWqrq7Wzp07tWHDBjU3N+vee+9VR0dHr/tXVVUpEAhEtvz8/HiPBAAYgOIeoPLycn3ve9/T5MmTVVpaqnfffVenTp3Sm2++2ev+lZWVCoVCka2lpSXeIwEABqCEfzpgxIgRuvPOO9XU1NTr836/X36/P9FjAAAGmIT/HNDp06d15MgR5ebmJvpQAIAkEvcAPfXUU6qrq9Onn36qjz76SPPmzVNKSooeeuiheB8KAJDE4v5XcMeOHdNDDz2k9vZ23XrrrbrnnnvU0NCgW2+9Nd6HAgAksbgH6PXXX4/3twQwSPXIeV7zr4+d97xmXN1NntdIUs+5czGtw5fDveAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMJ/4V0AJJP+z8We16zsey/JWCSK43Nbve8xl3sScAkuF5cAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEd8MGkoRvqPf/u7b/x6kxHeuPa1/2vGZIDH+e/ZczAe/HWX6z5zUXu//V8xokHldAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJbkYKJAnf1273vOajn/4qxqP1z59Nn/ofj3peM+7PDQmYBBa4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAzUiBJND38FesRrmrl8X/nec34F496XnPB8woMVFwBAQBMECAAgAnPAdq9e7ceeOAB5eXlyefzafv27VHPO+e0evVq5ebmatiwYSopKdHhw4fjNS8AYJDwHKDOzk4VFhZq/fr1vT6/bt06vfzyy9q4caP27Nmjm2++WaWlpTp37tx1DwsAGDw8fwihvLxc5eXlvT7nnNNLL72kZ599VnPmzJEkvfrqq8rJydH27du1cOHC65sWADBoxPU9oObmZrW2tqqkpCTyWCAQUFFRkerr63td09XVpXA4HLUBAAa/uAaotbVVkpSTkxP1eE5OTuS5y1VVVSkQCES2/Pz8eI4EABigzD8FV1lZqVAoFNlaWlqsRwIA9IO4BigYDEqS2traoh5va2uLPHc5v9+vjIyMqA0AMPjFNUAFBQUKBoOqqamJPBYOh7Vnzx4VFxfH81AAgCTn+VNwp0+fVlNTU+Tr5uZmHThwQJmZmRo9erRWrFihn/3sZ7rjjjtUUFCg5557Tnl5eZo7d2485wYAJDnPAdq7d6/uu+++yNerVq2SJC1atEjV1dV6+umn1dnZqaVLl+rUqVO65557tHPnTt10003xmxoAkPR8zjlnPcQXhcNhBQIBzdQcDfWlWo8DJETXv5/qec2WDb/0vCYrZZjnNbGaueL7ntfcsnVPAiaBtQuuW7XaoVAodNX39c0/BQcAuDERIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAhOdfxwAgWud/KPK85nf/9E+e12SnDPe8pkex3ex+actMz2vSt+/3vGZA3Yof/Y4rIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABDcjBb4gJWuk5zX3r671vGb00GGe16T4vP95scdd9LxGkj7557s9r8nobojpWLhxcQUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgZqTAF1wYn+95zY9G/s8ETHKli67H85rx7y+J6Vjjtx3wvMb7dLjRcQUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgZqQYnIakxLTs3yrPeD+UfDEdy6tftN/lec0diz6O6VjcWBT9gSsgAIAJAgQAMOE5QLt379YDDzygvLw8+Xw+bd++Per5xYsXy+fzRW1lZWXxmhcAMEh4DlBnZ6cKCwu1fv36PvcpKyvTiRMnItuWLVuua0gAwODj+UMI5eXlKi8vv+o+fr9fwWAw5qEAAINfQt4Dqq2tVXZ2tsaPH69ly5apvb29z327uroUDoejNgDA4Bf3AJWVlenVV19VTU2NfvGLX6iurk7l5eW6ePFir/tXVVUpEAhEtvz8/HiPBAAYgOL+c0ALFy6M/POkSZM0efJkjRs3TrW1tZo1a9YV+1dWVmrVqlWRr8PhMBECgBtAwj+GPXbsWGVlZampqanX5/1+vzIyMqI2AMDgl/AAHTt2TO3t7crNzU30oQAAScTzX8GdPn066mqmublZBw4cUGZmpjIzM7V27VrNnz9fwWBQR44c0dNPP63bb79dpaWlcR0cAJDcPAdo7969uu+++yJff/7+zaJFi7RhwwYdPHhQv/vd73Tq1Cnl5eVp9uzZ+ulPfyq/3x+/qQEASc9zgGbOnCnnXJ/P/+EPf7iugYB4SAnE9l7iR9/w/kPTsdy481/OBDyv+V+PfiOGI30Swxqgf3AvOACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJiI+6/kBgaCs9PGxbjy/bjO0ZeVuxdee6fL3HlwbwImAexwBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBmpBjweu79huc1v/6vL8d4NH+M67wZ/6uznte4BMwBWOIKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1IMeC1lAzzvOb21P65qagk3f/JXM9rUj897nnNRc8rgIGNKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQ3I0W/GjrqNs9r/suC/56ASXr3f7vPe17jezbT85qL/9bieQ0w2HAFBAAwQYAAACY8BaiqqkpTp05Venq6srOzNXfuXDU2Nkbtc+7cOVVUVGjkyJG65ZZbNH/+fLW1tcV1aABA8vMUoLq6OlVUVKihoUHvvfeeuru7NXv2bHV2dkb2Wblypd5++21t3bpVdXV1On78uB588MG4Dw4ASG6ePoSwc+fOqK+rq6uVnZ2tffv2acaMGQqFQvrNb36jzZs367vf/a4kadOmTbrrrrvU0NCgb3/72/GbHACQ1K7rPaBQKCRJysy89Cmgffv2qbu7WyUlJZF9JkyYoNGjR6u+vr7X79HV1aVwOBy1AQAGv5gD1NPToxUrVmj69OmaOHGiJKm1tVVpaWkaMWJE1L45OTlqbW3t9ftUVVUpEAhEtvz8/FhHAgAkkZgDVFFRoUOHDun111+/rgEqKysVCoUiW0sLPx8BADeCmH4Qdfny5XrnnXe0e/dujRo1KvJ4MBjU+fPnderUqairoLa2NgWDwV6/l9/vl9/vj2UMAEAS83QF5JzT8uXLtW3bNu3atUsFBQVRz0+ZMkWpqamqqamJPNbY2KijR4+quLg4PhMDAAYFT1dAFRUV2rx5s3bs2KH09PTI+zqBQEDDhg1TIBDQ448/rlWrVikzM1MZGRl68sknVVxczCfgAABRPAVow4YNkqSZM2dGPb5p0yYtXrxYkvTLX/5SQ4YM0fz589XV1aXS0lL9+te/jsuwAIDBw+ecc9ZDfFE4HFYgENBMzdFQX6r1OIizv745yfOa/z29Ov6D9OGu2n/0vGbcI/sTMAmQvC64btVqh0KhkDIyMvrcj3vBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwERMvxEVkKSUrJGe1/x48u8TMMmVjl04G9M67mwN9B+ugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFDHz3XKz5zUPpbclYJIr/cPe/xzTulH6P3GeBEBfuAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwM1IMeL8Jjfa8ZswzZ2I61sWYVgGIBVdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJbkaKmF349KjnNf9w25QETNKb5n46DoBYcQUEADBBgAAAJjwFqKqqSlOnTlV6erqys7M1d+5cNTY2Ru0zc+ZM+Xy+qO2JJ56I69AAgOTnKUB1dXWqqKhQQ0OD3nvvPXV3d2v27Nnq7OyM2m/JkiU6ceJEZFu3bl1chwYAJD9PH0LYuXNn1NfV1dXKzs7Wvn37NGPGjMjjw4cPVzAYjM+EAIBB6breAwqFQpKkzMzMqMdfe+01ZWVlaeLEiaqsrNSZM33/euSuri6Fw+GoDQAw+MX8Meyenh6tWLFC06dP18SJEyOPP/zwwxozZozy8vJ08OBBPfPMM2psbNRbb73V6/epqqrS2rVrYx0DAJCkfM45F8vCZcuW6fe//70+/PBDjRo1qs/9du3apVmzZqmpqUnjxo274vmuri51dXVFvg6Hw8rPz9dMzdFQX2osowEADF1w3arVDoVCIWVkZPS5X0xXQMuXL9c777yj3bt3XzU+klRUVCRJfQbI7/fL7/fHMgYAIIl5CpBzTk8++aS2bdum2tpaFRQUXHPNgQMHJEm5ubkxDQgAGJw8BaiiokKbN2/Wjh07lJ6ertbWVklSIBDQsGHDdOTIEW3evFn333+/Ro4cqYMHD2rlypWaMWOGJk+enJB/AQBAcvL0HpDP5+v18U2bNmnx4sVqaWnRo48+qkOHDqmzs1P5+fmaN2+enn322av+PeAXhcNhBQIB3gMCgCSVkPeArtWq/Px81dXVefmWAIAbFPeCAwCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGGo9wOWcc5KkC+qWnPEwAADPLqhb0t//e96XARegjo4OSdKHetd4EgDA9ejo6FAgEOjzeZ+7VqL6WU9Pj44fP6709HT5fL6o58LhsPLz89XS0qKMjAyjCe1xHi7hPFzCebiE83DJQDgPzjl1dHQoLy9PQ4b0/U7PgLsCGjJkiEaNGnXVfTIyMm7oF9jnOA+XcB4u4Txcwnm4xPo8XO3K53N8CAEAYIIAAQBMJFWA/H6/1qxZI7/fbz2KKc7DJZyHSzgPl3AeLkmm8zDgPoQAALgxJNUVEABg8CBAAAATBAgAYIIAAQBMJE2A1q9fr69+9au66aabVFRUpD/96U/WI/W7559/Xj6fL2qbMGGC9VgJt3v3bj3wwAPKy8uTz+fT9u3bo553zmn16tXKzc3VsGHDVFJSosOHD9sMm0DXOg+LFy++4vVRVlZmM2yCVFVVaerUqUpPT1d2drbmzp2rxsbGqH3OnTuniooKjRw5Urfccovmz5+vtrY2o4kT48uch5kzZ17xenjiiSeMJu5dUgTojTfe0KpVq7RmzRp9/PHHKiwsVGlpqU6ePGk9Wr+7++67deLEicj24YcfWo+UcJ2dnSosLNT69et7fX7dunV6+eWXtXHjRu3Zs0c333yzSktLde7cuX6eNLGudR4kqaysLOr1sWXLln6cMPHq6upUUVGhhoYGvffee+ru7tbs2bPV2dkZ2WflypV6++23tXXrVtXV1en48eN68MEHDaeOvy9zHiRpyZIlUa+HdevWGU3cB5cEpk2b5ioqKiJfX7x40eXl5bmqqirDqfrfmjVrXGFhofUYpiS5bdu2Rb7u6elxwWDQvfDCC5HHTp065fx+v9uyZYvBhP3j8vPgnHOLFi1yc+bMMZnHysmTJ50kV1dX55y79L99amqq27p1a2Sfv/zlL06Sq6+vtxoz4S4/D845953vfMf94Ac/sBvqSxjwV0Dnz5/Xvn37VFJSEnlsyJAhKikpUX19veFkNg4fPqy8vDyNHTtWjzzyiI4ePWo9kqnm5ma1trZGvT4CgYCKiopuyNdHbW2tsrOzNX78eC1btkzt7e3WIyVUKBSSJGVmZkqS9u3bp+7u7qjXw4QJEzR69OhB/Xq4/Dx87rXXXlNWVpYmTpyoyspKnTlzxmK8Pg24m5Fe7rPPPtPFixeVk5MT9XhOTo4++eQTo6lsFBUVqbq6WuPHj9eJEye0du1a3XvvvTp06JDS09OtxzPR2toqSb2+Pj5/7kZRVlamBx98UAUFBTpy5Ih+/OMfq7y8XPX19UpJSbEeL+56enq0YsUKTZ8+XRMnTpR06fWQlpamESNGRO07mF8PvZ0HSXr44Yc1ZswY5eXl6eDBg3rmmWfU2Niot956y3DaaAM+QPi78vLyyD9PnjxZRUVFGjNmjN588009/vjjhpNhIFi4cGHknydNmqTJkydr3Lhxqq2t1axZswwnS4yKigodOnTohngf9Gr6Og9Lly6N/POkSZOUm5urWbNm6ciRIxo3blx/j9mrAf9XcFlZWUpJSbniUyxtbW0KBoNGUw0MI0aM0J133qmmpibrUcx8/hrg9XGlsWPHKisra1C+PpYvX6533nlHH3zwQdSvbwkGgzp//rxOnToVtf9gfT30dR56U1RUJEkD6vUw4AOUlpamKVOmqKamJvJYT0+PampqVFxcbDiZvdOnT+vIkSPKzc21HsVMQUGBgsFg1OsjHA5rz549N/zr49ixY2pvbx9Urw/nnJYvX65t27Zp165dKigoiHp+ypQpSk1NjXo9NDY26ujRo4Pq9XCt89CbAwcOSNLAej1Yfwriy3j99ded3+931dXV7s9//rNbunSpGzFihGttbbUerV/98Ic/dLW1ta65udn98Y9/dCUlJS4rK8udPHnSerSE6ujocPv373f79+93ktyLL77o9u/f7/76178655z7+c9/7kaMGOF27NjhDh486ObMmeMKCgrc2bNnjSePr6udh46ODvfUU0+5+vp619zc7N5//333zW9+091xxx3u3Llz1qPHzbJly1wgEHC1tbXuxIkTke3MmTORfZ544gk3evRot2vXLrd3715XXFzsiouLDaeOv2udh6amJveTn/zE7d271zU3N7sdO3a4sWPHuhkzZhhPHi0pAuScc6+88oobPXq0S0tLc9OmTXMNDQ3WI/W7BQsWuNzcXJeWluZuu+02t2DBAtfU1GQ9VsJ98MEHTtIV26JFi5xzlz6K/dxzz7mcnBzn9/vdrFmzXGNjo+3QCXC183DmzBk3e/Zsd+utt7rU1FQ3ZswYt2TJkkH3h7Te/v0luU2bNkX2OXv2rPv+97/vvvKVr7jhw4e7efPmuRMnTtgNnQDXOg9Hjx51M2bMcJmZmc7v97vbb7/d/ehHP3KhUMh28Mvw6xgAACYG/HtAAIDBiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw8f8A4fRrKGOTQR4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wandele Xsamp in ein Numpy-Array um\n",
    "Xsamp_numpy = Xsamp.numpy()\n",
    "\n",
    "# Zeige das Bild mit imshow\n",
    "plt.imshow(Xsamp_numpy.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1], shape=(1,), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "print(ysamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_logger(orig_func):\n",
    "    logging.basicConfig(filename='{}.log'.format(orig_func.__name__), level=logging.INFO)\n",
    "\n",
    "    @wraps(orig_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logging.info(\n",
    "            'Ran with args: {}, and kwargs: {}'.format(args, kwargs))\n",
    "        return orig_func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def my_timer(orig_func):\n",
    "\n",
    "    @wraps(orig_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = orig_func(*args, **kwargs)\n",
    "        t2 = time.time() - t1\n",
    "        print('{} ran in: {} sec'.format(orig_func.__name__, t2))\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Session ausführen\n",
    "\n",
    "Jetzt ist es Zeit unsere Session auszuführen! Achte darauf wie wir zwei Schleifen verwenden. Die äußere, die die Epochs durchläuft, und die innere, die die Batches für jede Epoch des Trainings ausführt.\n",
    "\n",
    "## Wichtig, hier wurden aus zeitlichen Gründen mit 1% der Daten gearbeitet. Der Code ist mit jeder Prozentzahl reproduzierbar.\n",
    "\n",
    "Es wurde aus zeitlicher und übersichtlicher Sicht eine View sowie eine Abfrage des gewünschten Prozentsatzes eingebaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Geben Sie den Prozentsatz der Daten ein, der verarbeitet werden soll (0-100):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multilayer_perceptron ran in: 0.005301713943481445 sec\n",
      "multilayer_perceptron ran in: 0.0016682147979736328 sec\n",
      "multilayer_perceptron ran in: 0.0016384124755859375 sec\n",
      "multilayer_perceptron ran in: 0.0016994476318359375 sec\n",
      "multilayer_perceptron ran in: 0.0030298233032226562 sec\n",
      "multilayer_perceptron ran in: 0.0016460418701171875 sec\n",
      "multilayer_perceptron ran in: 0.0016505718231201172 sec\n",
      "multilayer_perceptron ran in: 0.001615762710571289 sec\n",
      "multilayer_perceptron ran in: 0.0016710758209228516 sec\n",
      "multilayer_perceptron ran in: 0.0019681453704833984 sec\n",
      "multilayer_perceptron ran in: 0.0016293525695800781 sec\n",
      "multilayer_perceptron ran in: 0.001615285873413086 sec\n",
      "multilayer_perceptron ran in: 0.006432294845581055 sec\n",
      "multilayer_perceptron ran in: 0.0016121864318847656 sec\n",
      "multilayer_perceptron ran in: 0.0016164779663085938 sec\n",
      "multilayer_perceptron ran in: 0.0019981861114501953 sec\n",
      "multilayer_perceptron ran in: 0.0015866756439208984 sec\n",
      "multilayer_perceptron ran in: 0.0015785694122314453 sec\n",
      "multilayer_perceptron ran in: 0.0016274452209472656 sec\n",
      "multilayer_perceptron ran in: 0.0016388893127441406 sec\n",
      "multilayer_perceptron ran in: 0.0022134780883789062 sec\n",
      "multilayer_perceptron ran in: 0.0015785694122314453 sec\n",
      "multilayer_perceptron ran in: 0.0015795230865478516 sec\n",
      "multilayer_perceptron ran in: 0.0017554759979248047 sec\n",
      "multilayer_perceptron ran in: 0.0016236305236816406 sec\n",
      "multilayer_perceptron ran in: 0.0016329288482666016 sec\n",
      "multilayer_perceptron ran in: 0.0015821456909179688 sec\n",
      "multilayer_perceptron ran in: 0.001605987548828125 sec\n",
      "multilayer_perceptron ran in: 0.002032756805419922 sec\n",
      "multilayer_perceptron ran in: 0.0015957355499267578 sec\n",
      "multilayer_perceptron ran in: 0.0016634464263916016 sec\n",
      "multilayer_perceptron ran in: 0.0016326904296875 sec\n",
      "multilayer_perceptron ran in: 0.002781391143798828 sec\n",
      "multilayer_perceptron ran in: 0.001622915267944336 sec\n",
      "multilayer_perceptron ran in: 0.001645803451538086 sec\n",
      "multilayer_perceptron ran in: 0.013961315155029297 sec\n",
      "multilayer_perceptron ran in: 0.0016031265258789062 sec\n",
      "multilayer_perceptron ran in: 0.0017113685607910156 sec\n",
      "multilayer_perceptron ran in: 0.0020029544830322266 sec\n",
      "multilayer_perceptron ran in: 0.002064943313598633 sec\n",
      "multilayer_perceptron ran in: 0.002069711685180664 sec\n",
      "multilayer_perceptron ran in: 0.002058267593383789 sec\n",
      "multilayer_perceptron ran in: 0.002065420150756836 sec\n",
      "multilayer_perceptron ran in: 0.0017948150634765625 sec\n",
      "multilayer_perceptron ran in: 0.0019316673278808594 sec\n",
      "multilayer_perceptron ran in: 0.0018041133880615234 sec\n",
      "multilayer_perceptron ran in: 0.0018658638000488281 sec\n",
      "multilayer_perceptron ran in: 0.0018200874328613281 sec\n",
      "multilayer_perceptron ran in: 0.0017685890197753906 sec\n",
      "multilayer_perceptron ran in: 0.0020608901977539062 sec\n",
      "multilayer_perceptron ran in: 0.0017728805541992188 sec\n",
      "multilayer_perceptron ran in: 0.0016109943389892578 sec\n",
      "multilayer_perceptron ran in: 0.0016245841979980469 sec\n",
      "multilayer_perceptron ran in: 0.0019116401672363281 sec\n",
      "multilayer_perceptron ran in: 0.0016269683837890625 sec\n",
      "multilayer_perceptron ran in: 0.0016448497772216797 sec\n",
      "multilayer_perceptron ran in: 0.0016214847564697266 sec\n",
      "multilayer_perceptron ran in: 0.0015876293182373047 sec\n",
      "multilayer_perceptron ran in: 0.0016829967498779297 sec\n",
      "multilayer_perceptron ran in: 0.0015709400177001953 sec\n",
      "multilayer_perceptron ran in: 0.001583099365234375 sec\n",
      "multilayer_perceptron ran in: 0.0015997886657714844 sec\n",
      "multilayer_perceptron ran in: 0.026288747787475586 sec\n",
      "multilayer_perceptron ran in: 0.0017578601837158203 sec\n",
      "multilayer_perceptron ran in: 0.0017991065979003906 sec\n",
      "multilayer_perceptron ran in: 0.0023849010467529297 sec\n",
      "multilayer_perceptron ran in: 0.0017533302307128906 sec\n",
      "multilayer_perceptron ran in: 0.001814126968383789 sec\n",
      "multilayer_perceptron ran in: 0.0017290115356445312 sec\n",
      "multilayer_perceptron ran in: 0.0017392635345458984 sec\n",
      "multilayer_perceptron ran in: 0.0017542839050292969 sec\n",
      "multilayer_perceptron ran in: 0.001748800277709961 sec\n",
      "multilayer_perceptron ran in: 0.0017590522766113281 sec\n",
      "multilayer_perceptron ran in: 0.0017812252044677734 sec\n",
      "multilayer_perceptron ran in: 0.0019519329071044922 sec\n",
      "multilayer_perceptron ran in: 0.0017514228820800781 sec\n",
      "multilayer_perceptron ran in: 0.0018420219421386719 sec\n",
      "multilayer_perceptron ran in: 0.002300262451171875 sec\n",
      "multilayer_perceptron ran in: 0.0017969608306884766 sec\n",
      "multilayer_perceptron ran in: 0.0018122196197509766 sec\n",
      "multilayer_perceptron ran in: 0.0017545223236083984 sec\n",
      "multilayer_perceptron ran in: 0.0017650127410888672 sec\n",
      "multilayer_perceptron ran in: 0.001971006393432617 sec\n",
      "multilayer_perceptron ran in: 0.0017676353454589844 sec\n",
      "multilayer_perceptron ran in: 0.001741170883178711 sec\n",
      "multilayer_perceptron ran in: 0.001737833023071289 sec\n",
      "multilayer_perceptron ran in: 0.0017197132110595703 sec\n",
      "multilayer_perceptron ran in: 0.0018649101257324219 sec\n",
      "multilayer_perceptron ran in: 0.0017189979553222656 sec\n",
      "multilayer_perceptron ran in: 0.0018177032470703125 sec\n",
      "multilayer_perceptron ran in: 0.0017919540405273438 sec\n",
      "multilayer_perceptron ran in: 0.0017714500427246094 sec\n",
      "multilayer_perceptron ran in: 0.058838605880737305 sec\n",
      "multilayer_perceptron ran in: 0.0018877983093261719 sec\n",
      "multilayer_perceptron ran in: 0.0020873546600341797 sec\n",
      "multilayer_perceptron ran in: 0.0017294883728027344 sec\n",
      "multilayer_perceptron ran in: 0.0017657279968261719 sec\n",
      "multilayer_perceptron ran in: 0.0017757415771484375 sec\n",
      "multilayer_perceptron ran in: 0.0018467903137207031 sec\n",
      "multilayer_perceptron ran in: 0.0017361640930175781 sec\n",
      "multilayer_perceptron ran in: 0.001772165298461914 sec\n",
      "multilayer_perceptron ran in: 0.0017955303192138672 sec\n",
      "multilayer_perceptron ran in: 0.0018229484558105469 sec\n",
      "multilayer_perceptron ran in: 0.001766204833984375 sec\n",
      "multilayer_perceptron ran in: 0.001603841781616211 sec\n",
      "multilayer_perceptron ran in: 0.0020062923431396484 sec\n",
      "multilayer_perceptron ran in: 0.0015609264373779297 sec\n",
      "multilayer_perceptron ran in: 0.0016324520111083984 sec\n",
      "multilayer_perceptron ran in: 0.0015816688537597656 sec\n",
      "multilayer_perceptron ran in: 0.0016150474548339844 sec\n",
      "multilayer_perceptron ran in: 0.0016360282897949219 sec\n",
      "multilayer_perceptron ran in: 0.001577138900756836 sec\n",
      "multilayer_perceptron ran in: 0.0015616416931152344 sec\n",
      "multilayer_perceptron ran in: 0.0019588470458984375 sec\n",
      "multilayer_perceptron ran in: 0.0015881061553955078 sec\n",
      "multilayer_perceptron ran in: 0.0015940666198730469 sec\n",
      "multilayer_perceptron ran in: 0.0015559196472167969 sec\n",
      "multilayer_perceptron ran in: 0.001524209976196289 sec\n",
      "multilayer_perceptron ran in: 0.0026671886444091797 sec\n",
      "multilayer_perceptron ran in: 0.0015485286712646484 sec\n",
      "multilayer_perceptron ran in: 0.0015516281127929688 sec\n",
      "multilayer_perceptron ran in: 0.0015690326690673828 sec\n",
      "multilayer_perceptron ran in: 0.001528024673461914 sec\n",
      "multilayer_perceptron ran in: 0.0020492076873779297 sec\n",
      "multilayer_perceptron ran in: 0.0015497207641601562 sec\n",
      "multilayer_perceptron ran in: 0.0015869140625 sec\n",
      "multilayer_perceptron ran in: 0.0015518665313720703 sec\n",
      "multilayer_perceptron ran in: 0.001585245132446289 sec\n",
      "multilayer_perceptron ran in: 0.0016293525695800781 sec\n",
      "multilayer_perceptron ran in: 0.0016450881958007812 sec\n",
      "multilayer_perceptron ran in: 0.0016062259674072266 sec\n",
      "multilayer_perceptron ran in: 0.0015575885772705078 sec\n",
      "multilayer_perceptron ran in: 0.0015947818756103516 sec\n",
      "multilayer_perceptron ran in: 0.0015940666198730469 sec\n",
      "multilayer_perceptron ran in: 0.0015869140625 sec\n",
      "multilayer_perceptron ran in: 0.0015950202941894531 sec\n",
      "multilayer_perceptron ran in: 0.0015459060668945312 sec\n",
      "multilayer_perceptron ran in: 0.0015461444854736328 sec\n",
      "multilayer_perceptron ran in: 0.00274658203125 sec\n",
      "multilayer_perceptron ran in: 0.0015382766723632812 sec\n",
      "multilayer_perceptron ran in: 0.0015397071838378906 sec\n",
      "multilayer_perceptron ran in: 0.0015647411346435547 sec\n",
      "multilayer_perceptron ran in: 0.0019388198852539062 sec\n",
      "multilayer_perceptron ran in: 0.0023336410522460938 sec\n",
      "multilayer_perceptron ran in: 0.0015788078308105469 sec\n",
      "multilayer_perceptron ran in: 0.0016658306121826172 sec\n",
      "multilayer_perceptron ran in: 0.0015988349914550781 sec\n",
      "multilayer_perceptron ran in: 0.0015838146209716797 sec\n",
      "multilayer_perceptron ran in: 0.0020418167114257812 sec\n",
      "multilayer_perceptron ran in: 0.0015523433685302734 sec\n",
      "multilayer_perceptron ran in: 0.001605987548828125 sec\n",
      "multilayer_perceptron ran in: 0.0015532970428466797 sec\n",
      "multilayer_perceptron ran in: 0.001577138900756836 sec\n",
      "multilayer_perceptron ran in: 0.0019028186798095703 sec\n",
      "multilayer_perceptron ran in: 0.0015339851379394531 sec\n",
      "multilayer_perceptron ran in: 0.0015816688537597656 sec\n",
      "multilayer_perceptron ran in: 0.001544952392578125 sec\n",
      "multilayer_perceptron ran in: 0.0015749931335449219 sec\n",
      "multilayer_perceptron ran in: 0.0019872188568115234 sec\n",
      "multilayer_perceptron ran in: 0.002275705337524414 sec\n",
      "multilayer_perceptron ran in: 0.0017201900482177734 sec\n",
      "multilayer_perceptron ran in: 0.0017511844635009766 sec\n",
      "multilayer_perceptron ran in: 0.001798391342163086 sec\n",
      "multilayer_perceptron ran in: 0.0018084049224853516 sec\n",
      "multilayer_perceptron ran in: 0.0017170906066894531 sec\n",
      "multilayer_perceptron ran in: 0.0017557144165039062 sec\n",
      "multilayer_perceptron ran in: 0.0017287731170654297 sec\n",
      "multilayer_perceptron ran in: 0.001712799072265625 sec\n",
      "multilayer_perceptron ran in: 0.00170135498046875 sec\n",
      "multilayer_perceptron ran in: 0.0017991065979003906 sec\n",
      "multilayer_perceptron ran in: 0.0017321109771728516 sec\n",
      "multilayer_perceptron ran in: 0.0020682811737060547 sec\n",
      "multilayer_perceptron ran in: 0.0017032623291015625 sec\n",
      "multilayer_perceptron ran in: 0.0017082691192626953 sec\n",
      "multilayer_perceptron ran in: 0.0025954246520996094 sec\n",
      "multilayer_perceptron ran in: 0.0016906261444091797 sec\n",
      "multilayer_perceptron ran in: 0.0016682147979736328 sec\n",
      "multilayer_perceptron ran in: 0.0016846656799316406 sec\n",
      "multilayer_perceptron ran in: 0.001695394515991211 sec\n",
      "multilayer_perceptron ran in: 0.001800537109375 sec\n",
      "multilayer_perceptron ran in: 0.0017025470733642578 sec\n",
      "multilayer_perceptron ran in: 0.0016689300537109375 sec\n",
      "multilayer_perceptron ran in: 0.002639293670654297 sec\n",
      "multilayer_perceptron ran in: 0.0016748905181884766 sec\n",
      "multilayer_perceptron ran in: 0.001653432846069336 sec\n",
      "multilayer_perceptron ran in: 0.0018911361694335938 sec\n",
      "multilayer_perceptron ran in: 0.0017333030700683594 sec\n",
      "multilayer_perceptron ran in: 0.0024726390838623047 sec\n",
      "multilayer_perceptron ran in: 0.001707315444946289 sec\n",
      "multilayer_perceptron ran in: 0.0016818046569824219 sec\n",
      "multilayer_perceptron ran in: 0.0018036365509033203 sec\n",
      "multilayer_perceptron ran in: 0.0018074512481689453 sec\n",
      "multilayer_perceptron ran in: 0.0015523433685302734 sec\n",
      "multilayer_perceptron ran in: 0.001527547836303711 sec\n",
      "multilayer_perceptron ran in: 0.0015301704406738281 sec\n",
      "multilayer_perceptron ran in: 0.001873016357421875 sec\n",
      "multilayer_perceptron ran in: 0.0015168190002441406 sec\n",
      "multilayer_perceptron ran in: 0.0014994144439697266 sec\n",
      "multilayer_perceptron ran in: 0.0015282630920410156 sec\n",
      "multilayer_perceptron ran in: 0.0015311241149902344 sec\n",
      "multilayer_perceptron ran in: 0.0019576549530029297 sec\n",
      "multilayer_perceptron ran in: 0.0015175342559814453 sec\n",
      "multilayer_perceptron ran in: 0.0015294551849365234 sec\n",
      "multilayer_perceptron ran in: 0.0015320777893066406 sec\n",
      "multilayer_perceptron ran in: 0.0015361309051513672 sec\n",
      "multilayer_perceptron ran in: 0.0015187263488769531 sec\n",
      "multilayer_perceptron ran in: 0.0015277862548828125 sec\n",
      "multilayer_perceptron ran in: 0.0018029212951660156 sec\n",
      "multilayer_perceptron ran in: 0.0015451908111572266 sec\n",
      "multilayer_perceptron ran in: 0.0015499591827392578 sec\n",
      "multilayer_perceptron ran in: 0.001544952392578125 sec\n",
      "multilayer_perceptron ran in: 0.0024170875549316406 sec\n",
      "multilayer_perceptron ran in: 0.0015683174133300781 sec\n",
      "multilayer_perceptron ran in: 0.0015330314636230469 sec\n",
      "multilayer_perceptron ran in: 0.0015721321105957031 sec\n",
      "multilayer_perceptron ran in: 0.002002239227294922 sec\n",
      "multilayer_perceptron ran in: 0.0015208721160888672 sec\n",
      "multilayer_perceptron ran in: 0.00152587890625 sec\n",
      "multilayer_perceptron ran in: 0.0015096664428710938 sec\n",
      "multilayer_perceptron ran in: 0.001531839370727539 sec\n",
      "multilayer_perceptron ran in: 0.0015096664428710938 sec\n",
      "multilayer_perceptron ran in: 0.0015468597412109375 sec\n",
      "multilayer_perceptron ran in: 0.0018913745880126953 sec\n",
      "multilayer_perceptron ran in: 0.0015635490417480469 sec\n",
      "multilayer_perceptron ran in: 0.0015671253204345703 sec\n",
      "multilayer_perceptron ran in: 0.0015320777893066406 sec\n",
      "multilayer_perceptron ran in: 0.0015859603881835938 sec\n",
      "multilayer_perceptron ran in: 0.0016326904296875 sec\n",
      "multilayer_perceptron ran in: 0.0015492439270019531 sec\n",
      "multilayer_perceptron ran in: 0.0015950202941894531 sec\n",
      "multilayer_perceptron ran in: 0.0015769004821777344 sec\n",
      "multilayer_perceptron ran in: 0.0016503334045410156 sec\n",
      "multilayer_perceptron ran in: 0.0015785694122314453 sec\n",
      "multilayer_perceptron ran in: 0.001577615737915039 sec\n",
      "multilayer_perceptron ran in: 0.001566171646118164 sec\n",
      "multilayer_perceptron ran in: 0.0015740394592285156 sec\n",
      "multilayer_perceptron ran in: 0.0022492408752441406 sec\n",
      "multilayer_perceptron ran in: 0.0015437602996826172 sec\n",
      "multilayer_perceptron ran in: 0.0015811920166015625 sec\n",
      "multilayer_perceptron ran in: 0.0015730857849121094 sec\n",
      "multilayer_perceptron ran in: 0.0015692710876464844 sec\n",
      "multilayer_perceptron ran in: 0.0015926361083984375 sec\n",
      "multilayer_perceptron ran in: 0.0015647411346435547 sec\n",
      "multilayer_perceptron ran in: 0.0015947818756103516 sec\n",
      "multilayer_perceptron ran in: 0.0015399456024169922 sec\n",
      "multilayer_perceptron ran in: 0.001585245132446289 sec\n",
      "multilayer_perceptron ran in: 0.0017635822296142578 sec\n",
      "multilayer_perceptron ran in: 0.0015909671783447266 sec\n",
      "multilayer_perceptron ran in: 0.0015692710876464844 sec\n",
      "multilayer_perceptron ran in: 0.0015478134155273438 sec\n",
      "multilayer_perceptron ran in: 0.001689910888671875 sec\n",
      "multilayer_perceptron ran in: 0.0015773773193359375 sec\n",
      "multilayer_perceptron ran in: 0.0015683174133300781 sec\n",
      "multilayer_perceptron ran in: 0.00156402587890625 sec\n",
      "multilayer_perceptron ran in: 0.0022687911987304688 sec\n",
      "multilayer_perceptron ran in: 0.0015413761138916016 sec\n",
      "multilayer_perceptron ran in: 0.0015835762023925781 sec\n",
      "multilayer_perceptron ran in: 0.0015327930450439453 sec\n",
      "multilayer_perceptron ran in: 0.0015408992767333984 sec\n",
      "multilayer_perceptron ran in: 0.0019369125366210938 sec\n",
      "multilayer_perceptron ran in: 0.001567840576171875 sec\n",
      "multilayer_perceptron ran in: 0.0015783309936523438 sec\n",
      "multilayer_perceptron ran in: 0.0015540122985839844 sec\n",
      "multilayer_perceptron ran in: 0.0015859603881835938 sec\n",
      "multilayer_perceptron ran in: 0.0020074844360351562 sec\n",
      "multilayer_perceptron ran in: 0.0016016960144042969 sec\n",
      "multilayer_perceptron ran in: 0.001636505126953125 sec\n",
      "multilayer_perceptron ran in: 0.001577615737915039 sec\n",
      "multilayer_perceptron ran in: 0.0016176700592041016 sec\n",
      "multilayer_perceptron ran in: 0.0015571117401123047 sec\n",
      "multilayer_perceptron ran in: 0.0015778541564941406 sec\n",
      "multilayer_perceptron ran in: 0.0016503334045410156 sec\n",
      "multilayer_perceptron ran in: 0.0015211105346679688 sec\n",
      "multilayer_perceptron ran in: 0.0015516281127929688 sec\n",
      "multilayer_perceptron ran in: 0.002081155776977539 sec\n",
      "multilayer_perceptron ran in: 0.0015544891357421875 sec\n",
      "multilayer_perceptron ran in: 0.0016388893127441406 sec\n",
      "multilayer_perceptron ran in: 0.001527547836303711 sec\n",
      "multilayer_perceptron ran in: 0.0015501976013183594 sec\n",
      "multilayer_perceptron ran in: 0.0020189285278320312 sec\n",
      "multilayer_perceptron ran in: 0.001560211181640625 sec\n",
      "multilayer_perceptron ran in: 0.0015387535095214844 sec\n",
      "multilayer_perceptron ran in: 0.001543283462524414 sec\n",
      "multilayer_perceptron ran in: 0.0015974044799804688 sec\n",
      "multilayer_perceptron ran in: 0.0015459060668945312 sec\n",
      "multilayer_perceptron ran in: 0.0015285015106201172 sec\n",
      "multilayer_perceptron ran in: 0.0026314258575439453 sec\n",
      "multilayer_perceptron ran in: 0.001558065414428711 sec\n",
      "multilayer_perceptron ran in: 0.0015740394592285156 sec\n",
      "multilayer_perceptron ran in: 0.00153350830078125 sec\n",
      "multilayer_perceptron ran in: 0.0015797615051269531 sec\n",
      "multilayer_perceptron ran in: 0.0020656585693359375 sec\n",
      "multilayer_perceptron ran in: 0.0015482902526855469 sec\n",
      "multilayer_perceptron ran in: 0.0015828609466552734 sec\n",
      "multilayer_perceptron ran in: 0.0015697479248046875 sec\n",
      "multilayer_perceptron ran in: 0.0015366077423095703 sec\n",
      "multilayer_perceptron ran in: 0.0020024776458740234 sec\n",
      "multilayer_perceptron ran in: 0.0015916824340820312 sec\n",
      "multilayer_perceptron ran in: 0.001596212387084961 sec\n",
      "multilayer_perceptron ran in: 0.0015439987182617188 sec\n",
      "multilayer_perceptron ran in: 0.001596689224243164 sec\n",
      "multilayer_perceptron ran in: 0.001590728759765625 sec\n",
      "multilayer_perceptron ran in: 0.0016093254089355469 sec\n",
      "multilayer_perceptron ran in: 0.0019676685333251953 sec\n",
      "multilayer_perceptron ran in: 0.0017361640930175781 sec\n",
      "multilayer_perceptron ran in: 0.0016334056854248047 sec\n",
      "multilayer_perceptron ran in: 0.0024542808532714844 sec\n",
      "multilayer_perceptron ran in: 0.0016100406646728516 sec\n",
      "multilayer_perceptron ran in: 0.0016107559204101562 sec\n",
      "multilayer_perceptron ran in: 0.0015687942504882812 sec\n",
      "multilayer_perceptron ran in: 0.0015845298767089844 sec\n",
      "multilayer_perceptron ran in: 0.0020532608032226562 sec\n",
      "multilayer_perceptron ran in: 0.0015590190887451172 sec\n",
      "multilayer_perceptron ran in: 0.0016105175018310547 sec\n",
      "multilayer_perceptron ran in: 0.0015757083892822266 sec\n",
      "multilayer_perceptron ran in: 0.0016033649444580078 sec\n",
      "multilayer_perceptron ran in: 0.0021364688873291016 sec\n",
      "multilayer_perceptron ran in: 0.0015995502471923828 sec\n",
      "multilayer_perceptron ran in: 0.001573324203491211 sec\n",
      "multilayer_perceptron ran in: 0.001547098159790039 sec\n",
      "multilayer_perceptron ran in: 0.0015878677368164062 sec\n",
      "multilayer_perceptron ran in: 0.001552581787109375 sec\n",
      "multilayer_perceptron ran in: 0.0015301704406738281 sec\n",
      "multilayer_perceptron ran in: 0.001580953598022461 sec\n",
      "multilayer_perceptron ran in: 0.0016772747039794922 sec\n",
      "multilayer_perceptron ran in: 0.0015401840209960938 sec\n",
      "multilayer_perceptron ran in: 0.01370382308959961 sec\n",
      "multilayer_perceptron ran in: 0.0015461444854736328 sec\n",
      "multilayer_perceptron ran in: 0.001611948013305664 sec\n",
      "multilayer_perceptron ran in: 0.0015208721160888672 sec\n",
      "multilayer_perceptron ran in: 0.0015437602996826172 sec\n",
      "multilayer_perceptron ran in: 0.0015666484832763672 sec\n",
      "multilayer_perceptron ran in: 0.0015211105346679688 sec\n",
      "multilayer_perceptron ran in: 0.002015352249145508 sec\n",
      "multilayer_perceptron ran in: 0.0015940666198730469 sec\n",
      "multilayer_perceptron ran in: 0.0015468597412109375 sec\n",
      "multilayer_perceptron ran in: 0.0015423297882080078 sec\n",
      "multilayer_perceptron ran in: 0.0016319751739501953 sec\n",
      "multilayer_perceptron ran in: 0.0019800662994384766 sec\n",
      "multilayer_perceptron ran in: 0.0015304088592529297 sec\n",
      "multilayer_perceptron ran in: 0.00153350830078125 sec\n",
      "multilayer_perceptron ran in: 0.002688169479370117 sec\n",
      "multilayer_perceptron ran in: 0.0015282630920410156 sec\n",
      "multilayer_perceptron ran in: 0.001560211181640625 sec\n",
      "multilayer_perceptron ran in: 0.0015473365783691406 sec\n",
      "multilayer_perceptron ran in: 0.0015063285827636719 sec\n",
      "multilayer_perceptron ran in: 0.001538991928100586 sec\n",
      "multilayer_perceptron ran in: 0.0015285015106201172 sec\n",
      "multilayer_perceptron ran in: 0.0015196800231933594 sec\n",
      "multilayer_perceptron ran in: 0.0015184879302978516 sec\n",
      "multilayer_perceptron ran in: 0.0015485286712646484 sec\n",
      "multilayer_perceptron ran in: 0.009627103805541992 sec\n",
      "multilayer_perceptron ran in: 0.0015523433685302734 sec\n",
      "multilayer_perceptron ran in: 0.001592874526977539 sec\n",
      "multilayer_perceptron ran in: 0.0015645027160644531 sec\n",
      "multilayer_perceptron ran in: 0.0015850067138671875 sec\n",
      "multilayer_perceptron ran in: 0.001481771469116211 sec\n",
      "multilayer_perceptron ran in: 0.0014529228210449219 sec\n",
      "multilayer_perceptron ran in: 0.0015501976013183594 sec\n",
      "multilayer_perceptron ran in: 0.0015838146209716797 sec\n",
      "multilayer_perceptron ran in: 0.0015637874603271484 sec\n",
      "multilayer_perceptron ran in: 0.0019545555114746094 sec\n",
      "multilayer_perceptron ran in: 0.001550912857055664 sec\n",
      "multilayer_perceptron ran in: 0.001567840576171875 sec\n",
      "multilayer_perceptron ran in: 0.0015435218811035156 sec\n",
      "multilayer_perceptron ran in: 0.0015583038330078125 sec\n",
      "multilayer_perceptron ran in: 0.013841629028320312 sec\n",
      "multilayer_perceptron ran in: 0.002210378646850586 sec\n",
      "multilayer_perceptron ran in: 0.0032880306243896484 sec\n",
      "multilayer_perceptron ran in: 0.009634256362915039 sec\n",
      "multilayer_perceptron ran in: 0.0016942024230957031 sec\n",
      "multilayer_perceptron ran in: 0.00186920166015625 sec\n",
      "multilayer_perceptron ran in: 0.001583099365234375 sec\n",
      "multilayer_perceptron ran in: 0.0015578269958496094 sec\n",
      "multilayer_perceptron ran in: 0.0019845962524414062 sec\n",
      "multilayer_perceptron ran in: 0.0015614032745361328 sec\n",
      "multilayer_perceptron ran in: 0.0015671253204345703 sec\n",
      "multilayer_perceptron ran in: 0.0015625953674316406 sec\n",
      "multilayer_perceptron ran in: 0.001538991928100586 sec\n",
      "multilayer_perceptron ran in: 0.00201416015625 sec\n",
      "multilayer_perceptron ran in: 0.0015625953674316406 sec\n",
      "multilayer_perceptron ran in: 0.0015316009521484375 sec\n",
      "multilayer_perceptron ran in: 0.0015196800231933594 sec\n",
      "multilayer_perceptron ran in: 0.0015499591827392578 sec\n",
      "multilayer_perceptron ran in: 0.0015227794647216797 sec\n",
      "multilayer_perceptron ran in: 0.0015642642974853516 sec\n",
      "multilayer_perceptron ran in: 0.0019669532775878906 sec\n",
      "multilayer_perceptron ran in: 0.0015261173248291016 sec\n",
      "multilayer_perceptron ran in: 0.00153350830078125 sec\n",
      "multilayer_perceptron ran in: 0.01793837547302246 sec\n",
      "multilayer_perceptron ran in: 0.0015554428100585938 sec\n",
      "multilayer_perceptron ran in: 0.0015902519226074219 sec\n",
      "multilayer_perceptron ran in: 0.0015850067138671875 sec\n",
      "multilayer_perceptron ran in: 0.0015566349029541016 sec\n",
      "multilayer_perceptron ran in: 0.0015265941619873047 sec\n",
      "multilayer_perceptron ran in: 0.0015354156494140625 sec\n",
      "multilayer_perceptron ran in: 0.0016052722930908203 sec\n",
      "multilayer_perceptron ran in: 0.0015790462493896484 sec\n",
      "multilayer_perceptron ran in: 0.0015971660614013672 sec\n",
      "multilayer_perceptron ran in: 0.0015897750854492188 sec\n",
      "multilayer_perceptron ran in: 0.0015835762023925781 sec\n",
      "multilayer_perceptron ran in: 0.0018584728240966797 sec\n",
      "multilayer_perceptron ran in: 0.0015625953674316406 sec\n",
      "multilayer_perceptron ran in: 0.0015828609466552734 sec\n",
      "multilayer_perceptron ran in: 0.0015709400177001953 sec\n",
      "multilayer_perceptron ran in: 0.0015993118286132812 sec\n",
      "multilayer_perceptron ran in: 0.0022628307342529297 sec\n",
      "multilayer_perceptron ran in: 0.0015888214111328125 sec\n",
      "multilayer_perceptron ran in: 0.0015811920166015625 sec\n",
      "multilayer_perceptron ran in: 0.0019237995147705078 sec\n",
      "multilayer_perceptron ran in: 0.0020530223846435547 sec\n",
      "multilayer_perceptron ran in: 0.0015795230865478516 sec\n",
      "multilayer_perceptron ran in: 0.0015265941619873047 sec\n",
      "multilayer_perceptron ran in: 0.001567840576171875 sec\n",
      "multilayer_perceptron ran in: 0.0015723705291748047 sec\n",
      "multilayer_perceptron ran in: 0.0016241073608398438 sec\n",
      "multilayer_perceptron ran in: 0.0022504329681396484 sec\n",
      "multilayer_perceptron ran in: 0.0024650096893310547 sec\n",
      "multilayer_perceptron ran in: 0.0015621185302734375 sec\n",
      "multilayer_perceptron ran in: 0.001589059829711914 sec\n",
      "multilayer_perceptron ran in: 0.0023190975189208984 sec\n",
      "multilayer_perceptron ran in: 0.0015578269958496094 sec\n",
      "multilayer_perceptron ran in: 0.0016567707061767578 sec\n",
      "multilayer_perceptron ran in: 0.014076471328735352 sec\n",
      "multilayer_perceptron ran in: 0.0015680789947509766 sec\n",
      "multilayer_perceptron ran in: 0.0015575885772705078 sec\n",
      "multilayer_perceptron ran in: 0.001542806625366211 sec\n",
      "multilayer_perceptron ran in: 0.0016298294067382812 sec\n",
      "multilayer_perceptron ran in: 0.0015532970428466797 sec\n",
      "multilayer_perceptron ran in: 0.001584768295288086 sec\n",
      "multilayer_perceptron ran in: 0.001569509506225586 sec\n",
      "multilayer_perceptron ran in: 0.0015377998352050781 sec\n",
      "multilayer_perceptron ran in: 0.0016341209411621094 sec\n",
      "multilayer_perceptron ran in: 0.0016222000122070312 sec\n",
      "multilayer_perceptron ran in: 0.001552581787109375 sec\n",
      "multilayer_perceptron ran in: 0.0015628337860107422 sec\n",
      "multilayer_perceptron ran in: 0.0015635490417480469 sec\n",
      "multilayer_perceptron ran in: 0.0019328594207763672 sec\n",
      "multilayer_perceptron ran in: 0.0015599727630615234 sec\n",
      "multilayer_perceptron ran in: 0.004975557327270508 sec\n",
      "multilayer_perceptron ran in: 0.0015285015106201172 sec\n",
      "multilayer_perceptron ran in: 0.0016057491302490234 sec\n",
      "multilayer_perceptron ran in: 0.0015535354614257812 sec\n",
      "multilayer_perceptron ran in: 0.0015876293182373047 sec\n",
      "multilayer_perceptron ran in: 0.0015952587127685547 sec\n",
      "multilayer_perceptron ran in: 0.0016036033630371094 sec\n",
      "multilayer_perceptron ran in: 0.0016474723815917969 sec\n",
      "multilayer_perceptron ran in: 0.0015900135040283203 sec\n",
      "multilayer_perceptron ran in: 0.0020627975463867188 sec\n",
      "multilayer_perceptron ran in: 0.0015988349914550781 sec\n",
      "multilayer_perceptron ran in: 0.0016210079193115234 sec\n",
      "multilayer_perceptron ran in: 0.01398777961730957 sec\n",
      "multilayer_perceptron ran in: 0.0015599727630615234 sec\n",
      "multilayer_perceptron ran in: 0.0017693042755126953 sec\n",
      "multilayer_perceptron ran in: 0.0015850067138671875 sec\n",
      "multilayer_perceptron ran in: 0.0016398429870605469 sec\n",
      "multilayer_perceptron ran in: 0.0016007423400878906 sec\n",
      "multilayer_perceptron ran in: 0.0015957355499267578 sec\n",
      "multilayer_perceptron ran in: 0.00200653076171875 sec\n",
      "multilayer_perceptron ran in: 0.0015788078308105469 sec\n",
      "multilayer_perceptron ran in: 0.0016021728515625 sec\n",
      "multilayer_perceptron ran in: 0.001558542251586914 sec\n",
      "multilayer_perceptron ran in: 0.001598358154296875 sec\n",
      "multilayer_perceptron ran in: 0.0016052722930908203 sec\n",
      "multilayer_perceptron ran in: 0.0015718936920166016 sec\n",
      "multilayer_perceptron ran in: 0.0015687942504882812 sec\n",
      "multilayer_perceptron ran in: 0.0021064281463623047 sec\n",
      "multilayer_perceptron ran in: 0.001585245132446289 sec\n",
      "multilayer_perceptron ran in: 0.0015811920166015625 sec\n",
      "multilayer_perceptron ran in: 0.0016105175018310547 sec\n",
      "multilayer_perceptron ran in: 0.001544952392578125 sec\n",
      "multilayer_perceptron ran in: 0.0019855499267578125 sec\n",
      "multilayer_perceptron ran in: 0.0015413761138916016 sec\n",
      "multilayer_perceptron ran in: 0.0015151500701904297 sec\n",
      "multilayer_perceptron ran in: 0.0015604496002197266 sec\n",
      "multilayer_perceptron ran in: 0.0015006065368652344 sec\n",
      "multilayer_perceptron ran in: 0.0015196800231933594 sec\n",
      "multilayer_perceptron ran in: 0.0015163421630859375 sec\n",
      "multilayer_perceptron ran in: 0.0019850730895996094 sec\n",
      "multilayer_perceptron ran in: 0.0015141963958740234 sec\n",
      "Epoch: 1 Cost=10954.5908\n",
      "Modellierung ist beendet: 1 Epochs of Training\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "\n",
    "# Eingabe vom Benutzer: Prozentsatz der Daten, die verarbeitet werden sollen\n",
    "percentage_to_process = float(input(\"Geben Sie den Prozentsatz der Daten ein, der verarbeitet werden soll (0-100): \"))\n",
    "\n",
    "# Berechnen Sie die Anzahl der Datensätze, die verarbeitet werden sollen\n",
    "n_samples_to_process = int(n_samples * (percentage_to_process / 100))\n",
    "\n",
    "# Training Epochs\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.0\n",
    "\n",
    "    total_batch = int(n_samples_to_process / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        # Den nächsten Batch an Trainingsdaten und -labels nehmen\n",
    "\n",
    "        batch_x = tf.cast(x_train[i * batch_size: (i + 1) * batch_size], tf.float32)\n",
    "        batch_y = tf.cast(y_train[i * batch_size: (i + 1) * batch_size], tf.float32)\n",
    "\n",
    "        # Führen Sie die Optimierung und Cost-Berechnung durch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            pred = multilayer_perceptron(batch_x, weights, biases)\n",
    "\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=batch_y, logits=pred))\n",
    "\n",
    "        gradients = tape.gradient(loss, list(weights.values()) + list(biases.values()))\n",
    "        optimizer.apply_gradients(zip(gradients, list(weights.values()) + list(biases.values())))\n",
    "\n",
    "        avg_cost += loss / total_batch\n",
    "\n",
    "    print(\"Epoch: {} Cost={:.4f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Modellierung ist beendet: {} Epochs of Training\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell Auswertung\n",
    "\n",
    "Tensorflow bietet einige eingebaute Funktionen, die uns bei der Auswertung helfen. Dazu gehören `tf.equal` und `tf.reduce_mean`.\n",
    "\n",
    "\n",
    "### tf.equal\n",
    "\n",
    "Dies ist im Grunde genommen nur eine Kontrolle, ob die Vorhersagen mit den Labels übereinstimmen. Da wir in unserem Fall wissen, dass die Labels eine 1 in einem Array von Nullen sind, können wir `argmax()` verwenden, um die Position zu vergleichen. Denke daran, dass y immer noch der Platzhalter ist, den wir anfangs erstellt haben. Wir werden eine Reihe an Operationen durchführen, um einen Tensor zu erhalten, in den wir die Testdaten einlesen können, um es auszuwerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste das Modell\n",
    "correct_predictions = tf.math.reduce_all(tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.bool, name=None), name='tf.math.reduce_all/All:0', description=\"created by layer 'tf.math.reduce_all'\")\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um numerische Werte für unsere Vorhersagen zu erhalten müssen wir `tf.cast` verwenden, um den Tensor mit Booleans zurückzuführen in einen Tensor mit Floats. Dann können wir den Durchschnitt nehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.cast(correct_predictions, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.cast/Cast:0', description=\"created by layer 'tf.cast'\")\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir `tf.reduce_mean` verwenden, um den Durchschnitt der Elemente im Tensor zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.keras_tensor.KerasTensor"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das wirkt evtl. etwas merkwürdig, aber diese Genauigkeit ist immer noch ein Tensor Objekt. Denke daran, dass wir immer noch die tatsächlichen Testdaten übergeben müssen. Jetzt können wir die MNIST Testlabels und Bilder aufrufen und die Genauigkeit auswerten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 8, ..., 9, 7, 2], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `eval()` Methode erlaubt es uns direkt in der Session den Tensor auszuwerten ohne `tf.sess():mm` aufrufen zu müssen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keine GPU gefunden. Verwende die CPU.\n",
      "375/375 [==============================] - 23s 58ms/step - loss: 0.2899 - sparse_categorical_accuracy: 0.9176\n",
      "375/375 [==============================] - 3s 6ms/step - loss: 0.1556 - sparse_categorical_accuracy: 0.9520\n",
      "Test Loss: 0.15559233725070953\n",
      "Test Accuracy: 0.9520000219345093\n"
     ]
    }
   ],
   "source": [
    "# Überprüfen, ob eine GPU verfügbar ist\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"GPU gefunden. Aktiviere GPU-Unterstützung.\")\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"Keine GPU gefunden. Verwende die CPU.\")\n",
    "\n",
    "# Erstellen des Modells\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(n_input,)),  \n",
    "    tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(n_hidden_2, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(n_classes, activation=None)\n",
    "])\n",
    "\n",
    "# Kompilieren des Modells mit der Genauigkeitsmetrik\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Trainieren des Modells auf den Trainingsdaten in Batches mit einer erhöhten Batch-Größe\n",
    "x_train_flattened = x_train.reshape(-1, n_input)\n",
    "batch_size = 128  # Erhöhen Sie die Batch-Größe, um die GPU besser auszulasten\n",
    "model.fit(x_train_flattened, y_train, batch_size=batch_size, epochs=training_epochs)\n",
    "\n",
    "# Evaluieren der Genauigkeit auf den Testdaten (nur am Ende)\n",
    "x_test_flattened = x_test.reshape(-1, n_input)\n",
    "test_loss, test_accuracy = model.evaluate(x_test_flattened, y_test)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisieren des Unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier startet der Unittest, zu anfang werden die Datenausgelesen und anschließend die Train Accuracy und Test Accurracy ausgegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus Kommentiertet Test für Optimierung\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X = x_train  # Hier die x_train-Matrix verwenden\n",
    "    y = y_train  # Hier die y_train-Matrix verwenden\n",
    "    print('MNIST:', X.shape, y.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n",
    "    np.random.seed(31337)\n",
    "    ta = TheAlgorithm(X_train, y_train, X_test, y_test)\n",
    "    train_accuracy = ta.fit()\n",
    "    print()\n",
    "    print('Train Accuracy:', train_accuracy, '\\n')\n",
    "    print(\"Train confusion matrix:\\n%s\\n\" % ta.train_confusion_matrix)\n",
    "    test_accuracy = ta.predict()\n",
    "    print()\n",
    "    print('Test Accuracy:', test_accuracy, '\\n')\n",
    "    print(\"Test confusion matrix:\\n%s\\n\" % ta.test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST: (48000, 784) (48000,)\n",
      "__init__ ran in: 1.0251998901367188e-05 sec\n",
      "fit ran in: 17.03990077972412 sec\n",
      "Train Accuracy: 72.53645833333333\n",
      "Classification Report for the classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.81       963\n",
      "           1       0.75      0.95      0.84      1099\n",
      "           2       0.77      0.61      0.68       923\n",
      "           3       0.67      0.76      0.71      1022\n",
      "           4       0.72      0.78      0.75       961\n",
      "           5       0.62      0.44      0.51       844\n",
      "           6       0.73      0.85      0.79       948\n",
      "           7       0.75      0.79      0.77       978\n",
      "           8       0.80      0.54      0.65       922\n",
      "           9       0.77      0.59      0.67       940\n",
      "\n",
      "    accuracy                           0.73      9600\n",
      "   macro avg       0.73      0.72      0.72      9600\n",
      "weighted avg       0.73      0.73      0.72      9600\n",
      "\n",
      "\n",
      "predict ran in: 0.9231042861938477 sec\n",
      "Test Accuracy: 73.19791666666666\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    X = x_train  # Hier die x_train-Matrix verwenden\n",
    "    y = y_train  # Hier die y_train-Matrix verwenden\n",
    "    print('MNIST:', X.shape, y.shape)\n",
    "    \n",
    "    # Überprüfe, ob X und y die gleiche Anzahl von Zeilen haben\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"X und y haben unterschiedliche Anzahlen von Zeilen.\")\n",
    "    \n",
    "    # Split und Modell initialisieren\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n",
    "    np.random.seed(31337)\n",
    "    ta = TheAlgorithm(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Modell trainieren\n",
    "    train_accuracy = ta.fit()\n",
    "    print('Train Accuracy:', train_accuracy)\n",
    "    \n",
    "    # Modellvorhersage auf Testdaten\n",
    "    test_accuracy = ta.predict()\n",
    "    print('Test Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Durchführung der Unittests ausgeführt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test des Ausfalls der Laufzeit von test_fit mit 0,01% der representativen Zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUp\n",
      "__init__ ran in: 2.7418136596679688e-05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ran in: 15.488598585128784 sec\n",
      "setUp\n",
      "__init__ ran in: 1.7404556274414062e-05 sec\n",
      "fit ran in: 15.766567468643188 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for the classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.81       963\n",
      "           1       0.75      0.95      0.84      1099\n",
      "           2       0.77      0.61      0.68       923\n",
      "           3       0.67      0.76      0.71      1022\n",
      "           4       0.72      0.78      0.75       961\n",
      "           5       0.62      0.44      0.51       844\n",
      "           6       0.73      0.85      0.79       948\n",
      "           7       0.75      0.79      0.77       978\n",
      "           8       0.80      0.54      0.65       922\n",
      "           9       0.77      0.59      0.67       940\n",
      "\n",
      "    accuracy                           0.73      9600\n",
      "   macro avg       0.73      0.72      0.72      9600\n",
      "weighted avg       0.73      0.73      0.72      9600\n",
      "\n",
      "\n",
      "predict ran in: 0.9788899421691895 sec\n",
      "setUp\n",
      "__init__ ran in: 6.9141387939453125e-06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_runtime_fit (__main__.TestInput)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3851/1620406533.py\", line 56, in test_runtime_fit\n",
      "    self.assertLessEqual(elapsed_time, max_allowed_runtime)\n",
      "AssertionError: 15.414827585220337 not less than or equal to 0.0015414827585220339\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 48.291s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ran in: 15.414130926132202 sec\n"
     ]
    }
   ],
   "source": [
    "class TestInput(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        pass\n",
    "\n",
    "    def setUp(self):\n",
    "        print('setUp')\n",
    "        # Verwende die Werte und Objekte aus Code 34\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        # Setzen Sie die erwarteten Genauigkeiten basierend auf Ihren tatsächlichen Daten und Modellparametern\n",
    "        self.train_accuracy = train_accuracy  # Beispielwert, aktualisieren mit tatsächlichen Trainingsgenauigkeitswert\n",
    "        self.test_accuracy = test_accuracy  # Beispielwert, aktualisieren mit tatsächlichen Testgenauigkeitswert\n",
    "        # Ersetzen der ta_train_accuracy und ta_test_accuracy durch die tatsächlichen Genauigkeitswerte\n",
    "        self.train_confusion_matrix = ta.train_confusion_matrix\n",
    "        self.test_confusion_matrix = ta.test_confusion_matrix\n",
    "\n",
    "    def tearDown(self):\n",
    "        pass\n",
    "\n",
    "    def test_fit(self):\n",
    "        np.random.seed(31337)\n",
    "        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "        self.assertEqual(self.ta.fit(), self.train_accuracy)\n",
    "        self.assertTrue(np.array_equal(self.ta.train_confusion_matrix, self.train_confusion_matrix))\n",
    "\n",
    "    def test_predict(self):\n",
    "        np.random.seed(31337)\n",
    "        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "        self.ta.fit()\n",
    "        self.assertEqual(self.ta.predict(), self.test_accuracy)\n",
    "        self.assertTrue(np.array_equal(self.ta.test_confusion_matrix, self.test_confusion_matrix))\n",
    "        \n",
    "    def test_runtime_fit(self):\n",
    "        np.random.seed(31337)\n",
    "        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "\n",
    "        # Messen der Laufzeit der fit-Funktion\n",
    "        start_time = time.time()\n",
    "        self.ta.fit()\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Legen Sie den Grenzwert fest, z.B. 120% der repräsentativen Laufzeit\n",
    "        representative_runtime = elapsed_time  # Aktualisieren Sie diesen Wert entsprechend\n",
    "        max_allowed_runtime = 0.0001 * representative_runtime\n",
    "\n",
    "        # Überprüfen, ob die Laufzeit innerhalb des zulässigen Bereichs liegt\n",
    "        self.assertLessEqual(elapsed_time, max_allowed_runtime)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test des Durchlaufs der Laufzeit von test_fit mit 120% der representativen Zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUp\n",
      "__init__ ran in: 1.7404556274414062e-05 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ran in: 15.302992343902588 sec\n",
      "setUp\n",
      "__init__ ran in: 1.5974044799804688e-05 sec\n",
      "fit ran in: 15.319247961044312 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for the classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.81       963\n",
      "           1       0.75      0.95      0.84      1099\n",
      "           2       0.77      0.61      0.68       923\n",
      "           3       0.67      0.76      0.71      1022\n",
      "           4       0.72      0.78      0.75       961\n",
      "           5       0.62      0.44      0.51       844\n",
      "           6       0.73      0.85      0.79       948\n",
      "           7       0.75      0.79      0.77       978\n",
      "           8       0.80      0.54      0.65       922\n",
      "           9       0.77      0.59      0.67       940\n",
      "\n",
      "    accuracy                           0.73      9600\n",
      "   macro avg       0.73      0.72      0.72      9600\n",
      "weighted avg       0.73      0.73      0.72      9600\n",
      "\n",
      "\n",
      "predict ran in: 0.9180817604064941 sec\n",
      "setUp\n",
      "__init__ ran in: 9.5367431640625e-06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 47.588s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ran in: 15.199236631393433 sec\n"
     ]
    }
   ],
   "source": [
    "class TestInput(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        pass\n",
    "\n",
    "    def setUp(self):\n",
    "        print('setUp')\n",
    "        # Verwende die Werte und Objekte aus Code 34\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - splitRatio, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        # Setzen Sie die erwarteten Genauigkeiten basierend auf Ihren tatsächlichen Daten und Modellparametern\n",
    "        self.train_accuracy = train_accuracy  # Beispielwert, aktualisieren mit tatsächlichen Trainingsgenauigkeitswert\n",
    "        self.test_accuracy = test_accuracy  # Beispielwert, aktualisieren mit tatsächlichen Testgenauigkeitswert\n",
    "        # Ersetzen der ta_train_accuracy und ta_test_accuracy durch die tatsächlichen Genauigkeitswerte\n",
    "        self.train_confusion_matrix = ta.train_confusion_matrix\n",
    "        self.test_confusion_matrix = ta.test_confusion_matrix\n",
    "\n",
    "    def tearDown(self):\n",
    "        pass\n",
    "\n",
    "    def test_fit(self):\n",
    "        np.random.seed(31337)\n",
    "        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "        self.assertEqual(self.ta.fit(), self.train_accuracy)\n",
    "        self.assertTrue(np.array_equal(self.ta.train_confusion_matrix, self.train_confusion_matrix))\n",
    "\n",
    "    def test_predict(self):\n",
    "        np.random.seed(31337)\n",
    "        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "        self.ta.fit()\n",
    "        self.assertEqual(self.ta.predict(), self.test_accuracy)\n",
    "        self.assertTrue(np.array_equal(self.ta.test_confusion_matrix, self.test_confusion_matrix))\n",
    "        \n",
    "    def test_runtime_fit(self):\n",
    "        np.random.seed(31337)\n",
    "        self.ta = TheAlgorithm(self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "\n",
    "        # Messen der Laufzeit der fit-Funktion\n",
    "        start_time = time.time()\n",
    "        self.ta.fit()\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Legen Sie den Grenzwert fest, z.B. 120% der repräsentativen Laufzeit\n",
    "        representative_runtime = elapsed_time  # Aktualisieren Sie diesen Wert entsprechend\n",
    "        max_allowed_runtime = 1.2 * representative_runtime\n",
    "\n",
    "        # Überprüfen, ob die Laufzeit innerhalb des zulässigen Bereichs liegt\n",
    "        self.assertLessEqual(elapsed_time, max_allowed_runtime)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximale benötigte RAM-Menge: 38485.74 MB\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Ihr Code hier\n",
    "\n",
    "    max_ram = max_ram_usage()\n",
    "\n",
    "    print(f\"Maximale benötigte RAM-Menge: {max_ram:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gut gemacht!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
